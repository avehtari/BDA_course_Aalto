\documentclass[a4paper,11pt,english]{article}

\usepackage{babel}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{newtxtext} % times
\usepackage{amsmath}
\usepackage[varqu,varl]{inconsolata} % typewriter
\usepackage{microtype}
\usepackage{url}
\urlstyle{same}
\usepackage{xcolor}

\usepackage[pdftex,colorlinks=true,citecolor=blue,
            pagecolor=blue,linkcolor=blue,menucolor=blue,
            urlcolor=blue]{hyperref}
\hypersetup{%
  bookmarksopen=true,
  bookmarksnumbered=true,
  pdftitle={Bayesian data analysis},
  pdfsubject={Reading instructions},
  pdfauthor={Aki Vehtari},
  pdfkeywords={Bayesian probability theory, Bayesian inference, Bayesian data analysis},
  pdfstartview={FitH -32768}
}


% if not draft, smaller printable area makes the paper more readable
\topmargin -4mm
\oddsidemargin 0mm
\textheight 225mm
\textwidth 160mm

%\parskip=\baselineskip
\def\eff{\mathrm{rep}}

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\Sd}{Sd}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\Invchi2}{Inv-\chi^2}
\DeclareMathOperator{\NInvchi2}{N-Inv-\chi^2}
\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\tr}{tr}
%\DeclareMathOperator{\Pr}{Pr}
\DeclareMathOperator{\trace}{trace}
\DeclareMathOperator{\rep}{\mathrm{rep}}

\pagestyle{empty}

\begin{document}
\thispagestyle{empty}

\section*{Bayesian data analysis -- reading instructions 7} 
\smallskip
{\bf Aki Vehtari}

\smallskip

\section*{Chapter 7}

\subsection*{Outline}
\begin{list}{$\bullet$}{\parsep=0pt\itemsep=2pt}
\item 7.1 Measures of predictive accuracy
\item {\color{gray}7.2 Information criteria and cross-validation} (read instead the article mentioned below)
\item {\color{gray}7.3 Model comparison based on predictive performance}  (read instead the article mentioned below)
\item {\color{gray}7.4 Model comparison using Bayes factors (not used in the course, but useful to reaf if you have heard about Bayes factors)}
\item 7.5 Continuous model expansion / sensitivity analysis
\item {\color{gray}7.6 Example (may be skipped)}
\end{list}

\noindent
Instead of Sections 7.2 and 7.3 it's better to read
\begin{list}{$\bullet$}{\parsep=0pt\itemsep=2pt}
\item Aki Vehtari, Andrew Gelman and Jonah Gabry (2017). Practical
  Bayesian model evaluation using leave-one-out cross-validation and
  WAIC. \textit{Statistics and Computing}, \textbf{27}(5):1413-1432,
  doi:10.1007/s11222-016-9696-4. \href{http://arxiv.org/abs/1507.04544}{arXiv preprint arXiv:1507.04544}.
\item \href{https://mc-stan.org/loo/reference/loo-glossary.html}{LOO
    package glossary} summarises many important terms used in the
  assignments.
\item If during the project you have questions about cross-validation, see \href{https://avehtari.github.io/modelselection/CV-FAQ.html}{CV-FAQ}
\end{list}

\noindent
In Sections 7.2 and 7.3 of BDA, for historical reasons there is a multiplier $-2$ used. After the book was published, we have concluded that it causes too much confusion and recommend not to multiply by $-2$. The above paper is not using $-2$ anymore.\\

\subsection*{Extra material}

\noindent
The following article provides excellent discussion about ``How should I evaluate my modelling choices?'' from a scientific perspective.
\begin{itemize}
\item 
  Danielle J. Navarro (2019). Between the devil and the deep blue
  sea: Tensions between scientific judgement and statistical model
  selection. \textit{Computational Brain \& Behavior}
  \textbf{2}:28--34. \href{https://doi.org/10.1007/s42113-018-0019-z}{Online}.
\end{itemize}

\noindent
There is extra material at \url{https://avehtari.github.io/modelselection/}
\begin{list}{$\bullet$}{\parsep=0pt\itemsep=2pt}
\item Videos, slides, notebooks, references
% \item The most relevant for the course is the first part of the
%   talk ``Model assessment, comparison and selection at Master
%   class in Bayesian statistics, CIRM, Marseille''
\item Sections 1 and 5 (less than 3 pages) of
  ``\href{https://arxiv.org/abs/2008.10296}{Uncertainty in Bayesian
    Leave-One-Out Cross-Validation Based Model Comparison}'' clarify
  how to interpret standard error in model comparison
\item Cross-validation FAQ
  \url{https://avehtari.github.io/modelselection/CV-FAQ.html} answers
  many frequently asked questions.
\end{list}

\subsection*{Important terms}

\noindent
Find all the terms and symbols listed below. When reading the chapter
and the above mentioned article, write down questions related to
things unclear for you or things you think might be unclear for
others.
\begin{list}{$\bullet$}{\parsep=0pt\itemsep=2pt}
\item predictive accuracy/fit/error
\item external validation
\item cross-validation
\item information criteria
\item overfitting
\item measures of predictive accuracy
\item point prediction
\item scoring function
\item mean squared error
\item probabilistic prediction
\item scoring rule
\item logarithmic score
\item log-predictive density
\item out-of-sample predictive fit
\item elpd, elppd, lppd
\item deviance
\item within-sample predictive accuracy
\item adjusted within-sample predictive accuracy
\item AIC, DIC, WAIC (less important)
\item effective number of parameters
\item singular model
\item BIC (less important)
\item leave-one-out cross-validation
\item evaluating predictive error comparisons
\item bias induced by model selection
\item Bayes factors
\item continuous model expansion
\item sensitivity analysis
\end{list}
%fourth edition batching p. 287

\subsection*{Additional reading}

More theoretical details can be found in
\begin{itemize}
\item Aki Vehtari and Janne Ojanen (2012). A survey of Bayesian
  predictive methods for model assessment, selection and
  comparison. In Statistics Surveys,
  6:142-228. \url{http://dx.doi.org/10.1214/12-SS102}
\end{itemize}

\noindent
See more experimental comparisons in 
\begin{itemize}
\item   Juho Piironen and Aki Vehtari (2017). Comparison of Bayesian predictive methods for model selection. Statistics and Computing, 27(3):711-735. doi:10.1007/s11222-016-9649-y. \url{http://link.springer.com/article/10.1007/s11222-016-9649-y}
\end{itemize}

\subsection*{Posterior probability of the model vs. predictive performance}

Gelman: ``To take a historical example, I don't find it useful, from a
statistical perspective, to say that in 1850, say, our posterior
probability that Newton's laws were true was 99\%, then in 1900 it was
50\%, then by 1920, it was 0.01\% or whatever. I'd rather say that
Newton's laws were a good fit to the available data and prior
information back in 1850, but then as more data and a clearer
understanding became available, people focused on areas of lack of fit
in order to improve the model.''

Newton's laws are still sufficient for prediction in specific contexts
(non-relative speeds and differences in gravity, non-significant
effects of air resistance or other friction). See more in the course
video 1.1 Introduction to uncertainty and modelling
\url{https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=d841f429-9c3d-4d24-8228-a9f400efda7b}.

\end{document}

%%% Local Variables: 
%%% TeX-PDF-mode: t
%%% TeX-master: t
%%% End: 
