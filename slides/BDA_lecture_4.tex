\documentclass[finnish,english,t]{beamer}
%\documentclass[finnish,english,handout]{beamer}

% Uncomment if want to show notes
% \setbeameroption{show notes}

\mode<presentation>
{
  % \usetheme{Warsaw}
}
\setbeamertemplate{itemize items}[circle]
\setbeamercolor{frametitle}{bg=white,fg=navyblue}


% \usepackage[pdftex]{graphicx}
%\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{microtype}
\usepackage{epic,epsfig}
\usepackage{subfigure,float}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{inputenc}
\usepackage{babel}
\usepackage{afterpage}
\usepackage{url}
\urlstyle{same}
\usepackage{eufrak}
\usepackage{amsbsy}
\usepackage{eucal}
\usepackage{rotating}
\usepackage{listings}
\usepackage{lstbayes}
\usepackage[all,poly,ps,color]{xy}

\usepackage{natbib}
\bibliographystyle{apalike}

% \definecolor{hutblue}{rgb}{0,0.2549,0.6784}
% \definecolor{midnightblue}{rgb}{0.0977,0.0977,0.4375}
% \definecolor{hutsilver}{rgb}{0.4863,0.4784,0.4784}
% \definecolor{lightgray}{rgb}{0.95,0.95,0.95}
% \definecolor{section}{rgb}{0,0.2549,0.6784}
% \definecolor{list1}{rgb}{0,0.2549,0.6784}
 \definecolor{navyblue}{rgb}{0,0,0.5}
\renewcommand{\emph}[1]{\textcolor{navyblue}{#1}}
\definecolor{darkgreen}{rgb}{0,0.3922,0}

\graphicspath{{./figs/}}

\pdfinfo{
  /Title      (Bayesian data analysis, Ch 10)
  /Author     (Aki Vehtari) %
  /Keywords   (Bayesian probability theory, Bayesian inference, Bayesian data analysis)
}


\parindent=0pt
\parskip=8pt
\tolerance=9000
\abovedisplayshortskip=2pt

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{headline}[default]{}
\setbeamertemplate{headline}[text line]{\insertsection}
\setbeamertemplate{footline}[frame number]


\def\o{{\mathbf o}}
\def\t{{\mathbf \theta}}
\def\w{{\mathbf w}}
\def\x{{\mathbf x}}
\def\y{{\mathbf y}}
\def\z{{\mathbf z}}

\def\eff{\mathrm{eff}}

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\Sd}{Sd}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\Gammad}{Gamma}
\DeclareMathOperator{\Invgamma}{Inv-gamma}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Negbin}{Neg-bin}
\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\BF}{BF}
\DeclareMathOperator{\Invchi2}{Inv-\chi^2}
\DeclareMathOperator{\NInvchi2}{N-Inv-\chi^2}
\DeclareMathOperator{\InvWishart}{Inv-Wishart}
\DeclareMathOperator{\tr}{tr}
% \DeclareMathOperator{\Pr}{Pr}
\def\euro{{\footnotesize \EUR\, }}
\DeclareMathOperator{\rep}{\mathrm{rep}}

\title[]{Bayesian data analysis}
\subtitle{}

\author{Aki Vehtari}

\institute[Aalto]{}

\begin{document}

\begin{frame}{Chapter 10}

  \begin{itemize}
\item 10.1 Numerical integration (overview)
\item 10.2 Distributional approximations (overview, more in Chapter 4 and 13)
\item 10.3 Direct simulation and rejection sampling (overview)
\item 10.4 \textbf{Importance sampling} (used in PSIS-LOO discussed later)
\item 10.5 \textbf{How many simulation draws are needed?} (Ex 10.1 and 10.2)
  \begin{itemize}
  \item see chapter notes and extra slides for how many significant
    digits to report
  \end{itemize}
\item 10.6 Software (can be skipped)
\item 10.7 Debugging (can be skipped)
   \end{itemize}
\end{frame}

\begin{frame}{Notation}

  \begin{itemize}
  \item In this chapter, generic $p(\theta)$ is used instead of
    $p(\theta|y)$
  \item Unnormalized distribution is denoted by $q(\cdot)$
    \begin{itemize}
    \item $\int q(\theta) d\theta \neq 1$, but finite
    \item $q(\cdot) \propto p(\cdot)$
    \end{itemize}
  \item Proposal distribution is denoted by $g(\cdot)$
  \end{itemize}

\end{frame}

\begin{frame}{Numerical accuracy -- floating point}

  \begin{itemize}
  \item Floating point presentation of numbers. e.g. with 64bits
    \begin{itemize}
    \item closest value to zero is $\approx 2.2\cdot 10^{-308}$
      \begin{itemize}
      \item generate sample of 600 from normal distribution:\\
        {\color{navyblue} qr=rnorm(600)}
      \item calculate joint density given normal:\\
        {\color{navyblue} prod(dnorm(qr)) $\rightarrow$} {\color{red} 0 (underflow)}
      \item<2-> see log densities in the next slide
      \end{itemize}
    \item<3-> closest value to 1 is $\approx 1 \pm  2.2\cdot 10^{-16}$
      \begin{itemize}
      \item<3-> Laplace and ratio of girl and boy babies 
      \item<3-> {\color{navyblue}  pbeta(0.5, 241945, 251527) $\rightarrow$} {\color{red}  1 (rounding)}
      \item<4-> {\color{navyblue} pbeta(0.5, 241945, 251527, lower.tail=FALSE) $\approx -1.2\cdot 10^{-42}$}\\
        there is more accuracy near 0
      \end{itemize}
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Numerical accuracy -- log scale}

  \begin{itemize}
  \item Log densities
    \begin{itemize}
    \item use log densities to avoid over- and underflows in floating
      point presentation
      \begin{itemize}
      \item {\color{navyblue} prod(dnorm(qr)) $\rightarrow$} {\color{red} 0 (underflow)}
      \item {\color{navyblue} sum(dnorm(qr, log=TRUE)) $\rightarrow$} {\color{darkgreen} -847.3}
      \item<2-> how many observations we can now handle? % $\sim 10^308$
    \end{itemize}
    \item<3-> compute exp as late as possible
      \begin{itemize}
      \item<4-> e.g. for $a>b$, compute $\log(\exp(a)+\exp(b)) = a + \log(1+\exp(b-a))$\\
        \uncover<5->{
          e.g. {\color{navyblue} $\log(\exp(800)+\exp(800)) \rightarrow$} {\color{red} Inf}}\\
        \uncover<6->{but {\color{navyblue} $800 + \log(1 + \exp(800-800)) \approx$} {\color{darkgreen} $800.69$}}
      \item<7-> e.g. in Metropolis-algorithm (Assignment 5) compute the log of ratio of densities using the identity\\
        $\log(a/b)=\log(a)-\log(b)$
    \end{itemize}
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}

  {\Large\color{navyblue} It's all about expectations}

  \vspace{-1.5\baselineskip}
   \begin{align*}
     E_{\color{blue} p(\theta|y)}[f(\theta)] & = \int f(\theta) {\color{blue} p(\theta|y)} d\theta,\\
     \text{where} \quad
     {\color{blue} p(\theta|y)} & = \frac{\color{darkgreen}p(y|\theta)p(\theta)}{\color{red} \int p(y|\theta)p(\theta) d\theta}
   \end{align*}
     \uncover<2->{We can easily evaluate ${\color{darkgreen} p(y|\theta)p(\theta)}$ for any $\theta$, but the integral ${\color{red} \int p(y|\theta)p(\theta) d\theta}$ is usually difficult.}

     \uncover<3->{We can use the unnormalized posterior\\
       ${\color{darkgreen} q(\theta|y)
     = p(y|\theta)p(\theta)} \propto {\color{blue}p(\theta|y)}$, for example, in}
 \begin{itemize}
   \vspace{-0.5\baselineskip}
    \item<4-> Grid (equal spacing) evaluation with self-normalization
      \begin{align*}
        E_{\color{blue} p(\theta|y)}[f(\theta)] \approx
        \frac{\sum_{s=1}^S \left[f(\theta^{(s)}){\color{darkgreen}q(\theta^{(s)}|y)}\right]}{\sum_{s=1}^S{\color{darkgreen}q(\theta^{(s)}|y)}}
      \end{align*}
    \item<5-> Monte Carlo methods which can sample from
      ${\color{blue}p(\theta^{(s)}|y)}$ using only
      ${\color{darkgreen}q(\theta^{(s)}|y)}$ {\color{gray}(each draw has weight 1/S)}
         \vspace{-0.5\baselineskip}
      \begin{align*}
        E_{\color{blue} p(\theta|y)}[f(\theta)] \approx \frac{1}{S} \sum_{s=1}^S f(\theta^{(s)})
      \end{align*}
    \end{itemize}
   
\end{frame}

 \begin{frame}
   
  {\Large\color{navyblue} It's all about expectations}

   \begin{align*}
   E_{\theta}[f(\theta)] = \int f(\theta) p(\theta|y) d\theta
   \end{align*}

  \begin{itemize}
  \item Conjugate priors and analytic solutions (Ch 1-5)
  \item Grid integration and other quadrature rules (Ch 3, 10)
  \item Independent Monte Carlo, rejection and importance sampling (Ch 10)
  \item Markov Chain Monte Carlo (Ch 11-12)
  \item {\color{gray}Distributional approximations (Laplace, VB, EP) (Ch 4, 13)}
  \end{itemize}
   

 \end{frame}


\begin{frame}
   
  {\Large\color{navyblue} Quadrature integration}

  \begin{itemize}
  \item The simplest quadrature integration is grid integration\\
    \hspace{0cm}\begin{minipage}{3.5cm}
    \begin{align*}
      \E[\theta] \approx \sum_{t=1}^{T} \theta^{(t)}w^{(t)} ,
    \end{align*}
  \end{minipage}
  \begin{minipage}{6cm}
  %\includegraphics[width=6cm]{Integration_rectangle.png}
  \includegraphics[width=6cm]{norm1d_3c.pdf}
\end{minipage}\\
where $w^{(t)}$ is the normalized probability of a grid cell $t$, and $\alpha^{(t)}$ and $\beta^{(t)}$ are center locations of grid cells
\item<2-> In 1D further variations with better accuracy, e.g. trapezoid
  \begin{center}
    \includegraphics[width=6cm]{norm1d_3d.pdf}
  \end{center}
  \vspace{-0.7\baselineskip}
\item<3-> Adaptive quadrature methods add evaluation points where needed{\color{gray}, e.g., R function \texttt{integrate()}}
\item<4-> In 2D and higher
  \begin{itemize}
  \item nested quadrature
  \item product rules
  \end{itemize}
  \end{itemize}
  

\end{frame}


\begin{frame}{Monte Carlo - history}

  \begin{itemize}
  \item Used already before computers
    \begin{itemize}
      \item Buffon (18th century; needles)
      \item De Forest, Darwin, Galton (19th century)
      \item Pearson (19th century; roulette)
      \item Gosset (Student, 1908; hat)
    \end{itemize}
    \pause
  \item "Monte Carlo method" term was proposed by Metropolis, von Neumann
    or Ulam in the end of 1940s
     \begin{itemize}
     \item they worked together in atomic bomb project
     \item Metropolis and Ulam, "The Monte Carlo Method", 1949
     \end{itemize}
    \pause
   \item Bayesians started to have enough cheap computation time in 1990s
     \begin{itemize}
     \item BUGS project started 1989 (last OpenBUGS release 2014)
     \item Gelfand \& Smith, 1990
     \item Stan initial release 2012
     \end{itemize}
     % \begin{itemize}
     %   \item tätä ennen käyttö vähäistä, vaikka bayesilaisiakin
     %     osallistui teorian ja menetelmien kehittämiseen
     % \end{itemize}
  \end{itemize}

\end{frame}

% \note{Buffon tiputteli neuloja lautalattialle
% % http://noppa5.pc.helsinki.fi/koe/flash/prob/buf-en.html

% Pearson laski kuinka monta ruletinpyöritystä tarvitaan,
% tätä tietoa joku muu hyödynsikin

% Gosset käytti paperilappuja

% atomipommin kehittäjillä oli laskentatehoa

% }

\begin{frame}{Monte Carlo}

  \begin{itemize}
  \item Simulate draws from the target distribution
    \begin{itemize}
    \item these draws can be treated as any observations
    \item a collection of draws is sample
    \end{itemize}
  \item Use these draws, for example,
    \begin{itemize}
    \item to compute means, deviations, quantiles
    \item to draw histograms
    \item to marginalize
    \item etc.
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}
{Monte Carlo vs. deterministic}

  \begin{itemize}
  \item Monte Carlo = simulation methods
    \begin{itemize}
    \item evaluation points are selected stochastically (randomly)
    \end{itemize}
  \item Deterministic methods (e.g. grid)
    \begin{itemize}
    \item evaluation points are selected by some deterministic rule
    \item good deterministic methods converge faster (need less function evaluations)
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}
{How many simulation draws are needed?}

  \begin{itemize}
  \item How many draws or how big sample size?
  \item If draws are independent
    \begin{itemize}
    \item usual methods to estimate the uncertainty due to a finite
      number of observations (finite sample size)
    \end{itemize}
  \item Markov chain Monte Carlo produces dependent draws
    \begin{itemize}
    \item requires additional work to estimate the \emph{effective
        sample size}
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{How many simulation draws are needed?}

  \begin{itemize}
  \item Expectation of unknown quantity
    \begin{equation*}
      \E(\theta)\approx \frac{1}{S}\sum_{s=1}^S \theta^{(s)}
    \end{equation*}
    if $S$ is big and $\theta^{(s)}$ are independent, way may assume
    that the distribution of the expectation approaches normal
    distribution (see BDA3 Ch 4) with variance $\sigma^2_\theta/S$
    (asymptotic normality)
    \begin{itemize}
    \item this variance is independent on dimensionality of $\theta$
      \pause
    % \item total variance is sum of the epistemic uncertainty in the
    %   posterior and the uncertainty due to using finite number of
    %   Monte Carlo draws
    %   \begin{equation*}
    %     \sigma^2_\theta+\sigma^2_\theta/S \pause= \sigma^2_\theta(1+1/S)
    %   \end{equation*}
    %   \pause
    %   \vspace{-5mm}
    % \item e.g. if $S=100$, deviation increases by $\sqrt{1+1/S}=1.005$\\
    %   i.e. Monte Carlo error is very small (for the expectation)
    %   \pause
    \item See BDA3 Ch 4 for counter-examples for asymptotic normality
      \pause
    \item $\sigma_\theta/\sqrt{S}$ is called Monte Carlo standard
      error (MCSE)
    \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Example: Kilpisjärvi summer temperature}

  Average temperature in June, July, and August at Kilpisjärvi,
  Finland in 1952--2013

  \begin{center}
    \only<1>{\includegraphics[width=8cm]{kilpis_data.pdf}}
    \only<2>{\includegraphics[width=8cm]{kilpis_pfit.pdf}}
  \end{center}

\end{frame}

\begin{frame}{Example: Kilpisjärvi summer temperature}

  \begin{center}
    \only<1>{\includegraphics[width=8cm]{kilpis_phist.pdf}}
    \only<2>{\includegraphics[width=8cm]{kilpis_phist100.pdf}}
    \only<3>{\includegraphics[width=8cm]{kilpis_phist100_mcse1a.pdf}}
    \only<4>{\includegraphics[width=8cm]{kilpis_phist100_mcse1b.pdf}}
    \only<3>{$\sigma_\theta\approx 0.83,\, \text{MCSE} = \sigma_\theta/\sqrt{S} \approx 0.083,$\\ in repeated sampling we may expect mean estimate to vary within (1.8, 2.1) (90\% interval)}
    \only<4>{$\sigma_\theta\approx 0.83,\, \text{MCSE} \approx 0.026,$\\ in repeated sampling we may expect mean estimate to vary within (1.9, 2.0) (90\% interval)}
%     \uncover<3-4>{\vspace{0.5\baselineskip}\color{gray} $\text{total deviation}^2 = \sigma_\theta^2 + \text{MCSE}^2$}
  \end{center}

\end{frame}

\begin{frame}{Example: Kilpisjärvi summer temperature}

  \vspace{0.5\baselineskip}
  \makebox[12cm][t]{
    \hspace{-0.9cm}
  \begin{minipage}[b][12.2cm][t]{12.2cm}
    \only<1->{\includegraphics[width=4cm]{kilpis_phist100_mcse1a.pdf}}
    \only<2->{\includegraphics[width=4cm]{kilpis_phist100_mcse2a.pdf}}
    \only<3->{\includegraphics[width=4cm]{kilpis_phist100_mcse3a.pdf}}\\
    \only<1->{\includegraphics[width=4cm]{kilpis_phist100_mcse1b.pdf}}
    \only<2->{\includegraphics[width=4cm]{kilpis_phist100_mcse2b.pdf}}
    \only<3->{\includegraphics[width=4cm]{kilpis_phist100_mcse3b.pdf}}\\
    \begin{center}
      \vspace{-1.2\baselineskip}
      \only<4->{Tail quantiles are more difficult to estimate\\
      {\scriptsize See \href{https://doi.org/10.1214/20-BA1221}{Vehtari, Gelman, Simpson, Carpenter, \& Bürkner (2021)} for quantile MCSE computation.}}
\end{center}
  \end{minipage}
  }

\end{frame}

\begin{frame}{How many simulation draws are needed?}

  \begin{itemize}
  \item Posterior probability
    \begin{equation*}
      p(\theta \in A)\approx \frac{1}{S}\sum_l I(\theta^{(s)} \in A)
    \end{equation*}
    where $I(\theta^{(s)} \in A)=1$ if $\theta^{(s)} \in A$
    \begin{itemize}
    \item $I(\cdot)$ is binomially distributed as $p(\theta \in A)$
        \begin{itemize}
        \item use beta CDF, or normal approximation
        \item[$\rightarrow$] $\var(I(\cdot)) =  p(1-p)$  (Appendix A, p. 579)
        \item[$\rightarrow$] standard deviation of $p$ is $\sqrt{p(1-p)/S}$
        \end{itemize}
        \pause
      \item if $S=100$ and we observe $\frac{1}{S}\sum_l I(\theta^{(s)} \in A)=0.05$,\\ then $\sqrt{p(1-p)/S} \approx 0.02$\\
        i.e. accuracy is about $4\%$ units\\
        or from quantiles of beta distribution the range is $(0.02,0.11)$
        \pause
      \item $S=2000$ draws needed for $1\%$ unit accuracy
    \end{itemize}
    \pause
  \item To  estimate small probabilities, a large number of draws is needed
    \begin{itemize}
    \item to be able to estimate small $p$, need to get draws with
      $\theta^{(l)} \in A$, which in expectation requires $S \gg 1/p$
    \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Example: Kilpisjärvi summer temperature}

  \begin{center}
    \only<1>{\includegraphics[width=10cm]{kilpis_ppneg_mcse1a.pdf}}
    \only<2>{\includegraphics[width=10cm]{kilpis_ppneg_mcse1b.pdf}}
    \only<3>{\includegraphics[width=10cm]{kilpis_ppneg_mcse1c.pdf}}
    \only<4>{\includegraphics[width=10cm]{kilpis_ppneg_mcse1000.pdf}}
    \only<5>{\includegraphics[width=10cm]{kilpis_ppneg_mcse4000.pdf}}
  \end{center}

\end{frame}

\begin{frame}{From probabilities to quantiles}

  \begin{itemize}
  \item Probability: $p(\theta < A)\approx \frac{1}{S}\sum_l I(\theta^{(s)} < A)$
  \item 5\%-quantile: Find $A$ so that $p(\theta < A)=0.05$
  \item<2-> If $S=1000$ and uncertainty interval for 5\% probability
    is $(0.04,0.06)$ (see earlier slide), we can find uncertainty
    interval $(A^-,A^+)$, so that $p(\theta < A^-)=0.04$, and
    $p(\theta < A^+)=0.06$
    \begin{itemize}
    \item<3-> we can summarise this interval by transforming it to MCSE
    \item<3-> see examples in \url{https://avehtari.github.io/casestudies/Digits/digits.html}
    \item<3-> {\color{gray}if interested, see algorithm details in \href{https://doi.org/10.1214/20-BA1221}{Vehtari, Gelman, Simpson, Carpenter, \& Bürkner (2021), doi.org/10.1214/20-BA1221}.}
    \end{itemize}
  \end{itemize}
  
\end{frame}

\begin{frame}{How many digits to show in reports?}

\vspace{-0.5\baselineskip}  
  \begin{itemize}
  \item Too many digits make reading of the results slower and give
    false impression of the accuracy
  \item<2-> Don't show digits which are just random noise
    \begin{itemize}
    \item check what is the Monte Carlo standard error
    \end{itemize}
  \item<3-> Show meaningful digits given the posterior uncertainty
  \item<4-> Example: The mean and 90\% central posterior interval for temperature
     increase C$^\circ$/century based on posterior draws
     \begin{itemize}
     \item<5-> {\color{red} 2.050774 and $[$0.7472868 3.3017524$]$} (NO!)
     \item<6-> {\color{darkgreen} 2.1 and $[$0.7 3.3$]$}
    \item<7-> {\color{navyblue} 2 and $[$1 3$]$} (depends on the context)
     \end{itemize}
   \item<8-> Example: The probability that temp increase is
     positive
     \begin{itemize}
     \item<9-> {\color{red} 0.9960000} (NO!)
     \item<10-> {\color{navyblue} 1.00} (depends on the context)
     \item<11-> With 4000 draws MCSE $\approx$ 0.002. We could report
       that probability is {\color{darkgreen} very likely larger than 0.99}, or sample
       more to justify reporting three digits
     \item<12-> For probabilities close to 0 or 1, consider also when
       the model assumption justify certain accuracy
     \end{itemize}
   % \item<12-> For your reports: Don't be lazy and settle for the
   %   default number of digits. Think for each reported value how
   %   many digits is sensible.
  \end{itemize}

\uncover<13>{See also \url{https://avehtari.github.io/casestudies/Digits/digits.html}}
  
\end{frame}

\begin{frame}{More data}

  \begin{itemize}
  \item<+-> The analysis I just showed used data from 1952--2013
  \item<+-> With data data from 1952--2022
    \begin{itemize}
    \item The probability that temp increase is positive:
      $0.9995 \pm 0.0006$ (90\% interval),\\ which can be reported as more than
      $99.9\%$ probability
    \item With data from other locations we would be even more certain
    \end{itemize}
  \item<+-> Summer 2023 was the hottest in the recorded history
  \end{itemize}

\end{frame}


\begin{frame}{How many simulation draws are needed?}

  \begin{itemize}
  \item Less draws needed with
    \begin{itemize}
    \item deterministic methods
    \item marginalization (Rao-Blackwellization)
    \item variance reduction methods, such, control variates
    \end{itemize}
  \item<2-> Number of independent draws needed doesn't depend on the
    number of dimensions
    \begin{itemize}
    \item but it may be difficult to obtain independent draws in high
      dimensional case
    \end{itemize}
  \item<3-> Some algorithms are less efficient
    \begin{itemize}
    \item Compute MCSE using \textit{effective sample size (ESS)}
      instead of the number of draws $S$
    \item Usually ESS$<S$
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Direct simulation}

  \begin{itemize}
  \item Produces independent draws
    \begin{itemize}
    \item Using analytic transformations of uniform random numbers
      (e.g. appendix A)
    \item factorization
    \item numerical inverse-CDF
    \end{itemize}
  \item Problem: restricted to limited set of models
  \end{itemize}

\end{frame}

\begin{frame}

  {\Large\color{navyblue} Random number generators}

  \begin{itemize}
  \item Good pseudo random number generators are sufficient for
    Bayesian inference
    \begin{itemize}
    \item pseudo random generator uses deterministic algorithm to
      produce a sequence which is difficult to make difference from
      truly random sequence
    \item modern software used for statistical analysis have good
      pseudo RNGs
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}
{Direct simulation: Example}

  \begin{itemize}
  \item Box-Muller -method:\\ If $U_1$ and $U_2$ are independent
    draws from distribution $\U(0,1)$, and
    \begin{align*}
      X_1 & = \sqrt{-2\log(U_1)}\cos(2\pi U_2) \\
      X_2 & = \sqrt{-2\log(U_1)}\sin(2\pi U_2)
    \end{align*}
    then $X_1$ and $X_2$ are independent draws from the distribution
    $\N(0,1)$
    \pause
    \begin{itemize}
      \item not the fastest method due to trigonometric computations
      \item for normal distribution more than ten different methods
      \item e.g. R uses inverse-CDF
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Grid sampling and curse of dimensionality}

  \begin{itemize}
      \item 10 parameters
      \item if we don't know beforehand where the posterior mass is
        \begin{itemize}
          \item need to choose wide box for the grid
          \item need to have enough grid points to get some of them
            where essential mass is
        \end{itemize}
      \item e.g. 50 or 1000 grid points per dimension
        \begin{itemize}
        \item[$\rightarrow$] 50$^{10} \approx$ 1e17 grid points
        \item[$\rightarrow$] 1000$^{10} \approx$ 1e30 grid points
        \end{itemize}
      \item R and my current laptop can compute density of normal
        distribution about 50 million times per second
        \begin{itemize}
        \item[$\rightarrow$] evaluation in 1e17 grid points would take
           60 years %triljoona vuotta
        \item[$\rightarrow$] evaluation in 1e30 grid points would take
           600 billion years %triljoona vuotta
        \end{itemize}
 \end{itemize}

\end{frame}

\begin{frame}{Indirect sampling}
  
  \begin{itemize}
  \item Rejection sampling
    % \begin{itemize}
    % \item draw directly from a proposal distribution, reject some
    %   draws, remaining draws are independent draws from the target
    %   distribution
    % \end{itemize}
    % \pause
  \item Importance sampling
    % \begin{itemize}
    % \item draw directly from a proposal distribution, weight the draws
    % \end{itemize}
    % \pause
  \item Markov chain Monte Carlo (next week)
    % \begin{itemize}
    % \item draw directly from a transition distribution forming a
    %   Markov chain, draws are dependent draws from the target
    %   distribution
    % \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Rejection sampling}

    \vspace{-.3\baselineskip}
  \begin{itemize}
  \item[-] Proposal forms envelope over the target distribution ${q(\theta|y)}/{M g(\theta)} \leq 1$
  \item[-] Draw from the proposal and accept with probability ${q(\theta|y)}/{M g(\theta)}$
  \item<3>[-] Common for truncated distributions
  \end{itemize}
  \begin{center}
    \vspace{-1.6\baselineskip}
    \only<1>{\includegraphics[width=10cm]{rejection1.pdf}}
    \only<2>{\includegraphics[width=10cm]{rejection2.pdf}}
    \only<3>{\includegraphics[width=10cm]{rejection3.pdf}}
  \end{center}

\end{frame}

\begin{frame}{Rejection sampling}

  \begin{itemize}
  \item The effective sample size (ESS) is the number of accepted draws
    \begin{itemize}
    \item with bad proposal distribution may require a lot of trials
    \item selection of good proposal gets very difficult when
      the number of dimensions increase
    \item reliable diagnostics and thus can be a useful part
    \end{itemize}
  \end{itemize}

\end{frame}


\begin{frame}{Importance sampling}

  \begin{itemize}
     \vspace{-.5\baselineskip}
   \item[-] Proposal does not need to have a higher value everywhere
   \end{itemize}
   \begin{center}
     \vspace{-1\baselineskip}
     \only<1-2>{\includegraphics[width=9cm]{importancesamp1.pdf}}
     \only<3>{\includegraphics[width=9cm]{importancesamp2.pdf}}
     \vspace{-1\baselineskip}
     \only<2->{
   \begin{eqnarray*}
      \E[f(\theta)] \approx \frac{\sum_s w_s f(\theta^{(s)})}{\sum_s
      w_s}, \qquad \text{where} \quad 
      w_s =  \frac{q(\theta^{(s)})}{g(\theta^{(s)})} \qquad
   \end{eqnarray*}
   }
   \end{center}

\end{frame}

\begin{frame}{Some uses of importance sampling}

  In general selection of good proposal gets more difficult when the
  number of dimensions increase, but there are many special use case
  which scale well (e.g. I've used IS up to 10k dimensions)

  \uncover<2->{
  \begin{itemize}
  \item Fast leave-one-out cross-validation
  \item Fast bootstrapping
  \item Fast prior and likelihood sensitivity analysis
  \item Conformal Bayesian computation
  \item Particle filtering
  \item Improving distributional approximations (e.g Laplace, VI)
  \end{itemize}
}

\end{frame}

\begin{frame}{IS finite variance and central limit theorem}

  \begin{itemize}
  \item If $h(\theta){\color{darkgreen} w}$ and
    ${\color{darkgreen} w}$ have finite variance $\rightarrow$ CLT
    \begin{itemize}
    \item variance goes down as $1/S$
    \item Effective sample size (ESS) takes into account the variability in the weights
    \end{itemize}
  \item<2-> We would like to have finite variance and CLT
    \begin{itemize}
    \item sometimes these can be guaranteed by construction, e.g., by
      choosing $g(\theta)$ so that ${\color{darkgreen} w(\theta)}$ is bounded
    \item generally not trivial
    \end{itemize}
  % \item<3-> If variance is infinite, but mean is finite\\
  %   $\rightarrow$
  %   \textit{generalized CLT and asymptotic consistency} 
  \item<4-> Pre-asymptotic and asymptotic behavior can be really different!
  \end{itemize}
  
\end{frame}

\begin{frame}{Importance re-sampling}

  \begin{itemize}
  \item<+-> Using the weighted draws is good
    \begin{align*}
      \E[f(\theta)] \approx \frac{\sum_s w_s f(\theta^{(s)})}{\sum_s w_s}
    \end{align*}
  \item<+-> But it can be convenient to obtain draws with equal weights
    \begin{itemize}
    \item resample the draws according to the weights
    \item some original draws may be included more than once
    \item loses some information, but now the weights are equal
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Example: Importance sampling in Bioassay}

   \vspace{-.5\baselineskip}
   \makebox[12cm][t]{
     \hspace{-0.9cm}
     \begin{minipage}[t][12cm][t]{12cm}
       \begin{center}
        \makebox[0cm][t]{\hspace{-0.5cm}\rotatebox{90}{\hspace{1cm}Grid}}
        \includegraphics[width=3.4cm]{bioassayis1d.pdf}
      \includegraphics[width=3.4cm]{bioassayis1s.pdf}
      \includegraphics[width=3.4cm]{bioassayis1h.pdf}\\
      \only<2->{
        \makebox[0cm][t]{\hspace{-0.5cm}\rotatebox{90}{\hspace{1cm}Normal}}
      \includegraphics[width=3.4cm]{bioassayis2d.pdf}
      \includegraphics[width=3.4cm]{bioassayis2s.pdf}
      \includegraphics[width=3.4cm]{bioassayis2h.pdf}\\}
    \only<2-3>{Normal approximation is discussed more in BDA3 Ch 4\\}
    \only<3>{But the normal approximation is not that good here:\\ Grid sd(LD50) $\approx$ 0.1, Normal sd(LD50) $\approx$ .75!}
      \only<4->{
        \makebox[0cm][t]{\hspace{-0.5cm}\rotatebox{90}{\hspace{1cm}IR}}
      \includegraphics[width=3.4cm]{bioassayis3d.pdf}
      \includegraphics[width=3.4cm]{bioassayis3s.pdf}
      \includegraphics[width=3.4cm]{bioassayis3h.pdf}\\}
    \only<5->{Grid sd(LD50) $\approx$ 0.1, IR sd(LD50) $\approx$ 0.1}
    \end{center}
     \end{minipage}  
   }
  
\end{frame}

\begin{frame}{Example: Importance sampling in Bioassay}


   \makebox[12cm][t]{
     \hspace{-0.9cm}
     \begin{minipage}[t][6cm][t]{6cm}
       \begin{center}
       Grid\\
       \includegraphics[width=6cm]{bioassayis1s.pdf}
     \end{center}
     \end{minipage}
     \begin{minipage}[t][6cm][t]{6cm}
       \begin{center}
       IR\\
       \includegraphics[width=6cm]{bioassayis3s.pdf}
  \end{center}
  \end{minipage}
}

\end{frame}

\begin{frame}{Example: Importance sampling in Bioassay}

       \begin{center}
       IR\\
       \includegraphics[width=10cm]{bioassayis3s.pdf}
  \end{center}

\end{frame}

\begin{frame}{Example: Importance sampling in Bioassay}

       \begin{center}
       \only<1>{\includegraphics[width=10cm]{bioassayisw1.pdf}}
       \only<2>{\includegraphics[width=10cm]{bioassayisw2.pdf}}
       \only<3>{\includegraphics[width=10cm]{bioassayisw3.pdf}}
  \end{center}

\end{frame}

\begin{frame}{Example: Importance sampling in Bioassay}

       \begin{center}
         \vspace{-\baselineskip}
       \includegraphics[width=8cm]{bioassayisw2.pdf}\\
         \vspace{-2\baselineskip}
         \begin{align*}
           \text{ESS} & = \frac{1}{\sum_{s=1}^S (\tilde{w}(\theta^s))^2}, \quad \text{where } \tilde{w}(\theta^s)=w(\theta^s)/\sum_{s'=1}^Sw(\theta^{s'})\\
           \only<3->{\text{ESS} & \approx 396, \quad (\text{ESS}< S=1000)}
         \end{align*}
           \only<2>{{\color{red}
               \makebox[0cm][c]{\parbox{9.5cm}{\vspace{-2.5\baselineskip} \footnotesize BDA3 1st (2013) and 2nd (2014) printing have an error for $\tilde{w}(\theta^s)$. The equation should not have the multiplier S (the normalized weights should sum to one). Online version is correct. Errata for the book\\ \url{http://www.stat.columbia.edu/~gelman/book/errata_bda3.txt}\vspace{-2\baselineskip}}}}}
           \only<3>{\phantom{{\color{red}
            \makebox[0cm][c]{\parbox{9.5cm}{\vspace{-2.5\baselineskip} \footnotesize BDA3 1st (2013) and 2nd (2014) printing have an error for $\tilde{w}(\theta^s)$. The normalized weights equation should not have the multiplier S (the normalized weights should sum to one). Errata for the book\\ \url{http://www.stat.columbia.edu/~gelman/book/errata_bda3.txt}\vspace{-2\baselineskip}}}}}}
  \end{center}

\end{frame}

\begin{frame}{Example: Importance sampling in Bioassay}

       \begin{center}
         \vspace{-\baselineskip}
       \includegraphics[width=8cm]{bioassayisw2.pdf}\\
         \vspace{-2\baselineskip}
         \begin{align*}
           \text{ESS} & = \frac{1}{\sum_{s=1}^S (\tilde{w}(\theta^s))^2}, \quad \text{is based on variance of } \tilde{w}(\theta^s) \\
           \text{ESS} & \approx 396 \\
         \end{align*}
       \end{center}
       
         \vspace{-2\baselineskip}
         \uncover<2->{If all $\tilde{w}(\theta^s)=1/S$, then $\text{ESS}=1/(SS^{-2})=S$\\}
         \uncover<3->{If one $\tilde{w}(\theta^s)=1$, and others $0$, then $\text{ESS}=1/1=1$}

\end{frame}

\begin{frame}{Example: Importance sampling in Bioassay}

       \begin{center}
         \vspace{-\baselineskip}
       \includegraphics[width=8cm]{bioassayisw2.pdf}\\
         \vspace{-2\baselineskip}
         \begin{align*}
           \text{ESS} & = \frac{1}{\sum_{s=1}^S (\tilde{w}(\theta^s))^2}, \quad \text{is based on variance of } \tilde{w}(\theta^s) \\
           \text{ESS} & \approx 396 \\ \uncover<1->{&\text{Pareto-$k$ diagnostic preferably $<$ 0.7: }}\uncover<2->{ \hat{k} \approx 0.65}
         \end{align*}
  \end{center}

\end{frame}

\begin{frame}{Pareto-$\hat{k}$ diagnostic}

  \begin{itemize}
  \item Based on extreme value analysis and generalized central limit theorem
    \begin{itemize}
    \item<2-> we can estimate tail of a distribution with a Pareto distribution
    \item<3-> shape parameter $k$ tells the number of \textit{fractional moments} as $1/k$
    \item<3-> estimate $\hat{k}$ from finite data
    \item<4-> the statistical behavior of distribution of mean can be
      predicted by generalized CLT
      \begin{itemize}
      \item minimum sample size and convergence rate given $\hat{k}$
      \end{itemize}
    \end{itemize}
  \end{itemize}

  \vspace{5\baselineskip}
  {\small
  See more in Vehtari, Simpson, Gelman, Yao, and Gabry (2022). Pareto
  smoothed importance sampling. arXiv:1507.02646.}
  
\end{frame}

\begin{frame}{Pareto-$\hat{k}$ diagnostic}

%  \begin{itemize}
   Pickands (1975): many distributions have tail ($x > u$) that
    is well approximated with Generalized Pareto distribution (GPD)
%  \end{itemize}

    {
      \vspace{-0.5\baselineskip}
\only<1>{\includegraphics[width=9.5cm]{bulktail.pdf}}\only<2>{\includegraphics[width=9.5cm]{tail1.pdf}}\only<3>{\includegraphics[width=9.5cm]{tail2.pdf}}\only<4>{\includegraphics[width=9.5cm]{tail3.pdf}}
}

\end{frame}

\begin{frame}{Pareto-$\hat{k}$ diagnostic}

 GPD has a shape parameter $k$,\\and $1/k$ finite fractional
    moments

    {
      \vspace{-0.5\baselineskip}
  \includegraphics[width=9.5cm]{bulktail.pdf}
}

\end{frame}

\begin{frame}{Example: Importance sampling in Bioassay}

       \begin{center}
         \vspace{-\baselineskip}
       \includegraphics[width=8cm]{bioassayisw2.pdf}\\
         \vspace{-2\baselineskip}
         \begin{align*}
           \text{ESS} & = \frac{1}{\sum_{s=1}^S (\tilde{w}(\theta^s))^2}, \quad \text{is based on variance of } \tilde{w}(\theta^s) \\
           \text{ESS} & \approx 396 \\ \uncover<1->{&\text{Pareto-$k$ diagnostic preferably $<$ 0.7: }}\uncover<1->{ \hat{k} \approx 0.65}
         \end{align*}
  \end{center}

\end{frame}

\begin{frame}{Pareto-$\hat{k}$ and convergence rate}

  \begin{itemize}
  \item CLT says that to half the MCSE, need 4 times bigger S
  \item<2-> If Pareto-$\hat{k} \approx 0.7$, to half the MCSE, need 10 times bigger S
  \item<3-> If Pareto-$\hat{k}>1$, to half the MCSE, nothing helps
  \end{itemize}
  
\end{frame}

\begin{frame}{Pareto smoothed importance sampling (PSIS)}

  \begin{itemize}
  \item Replace the largest observed ratios with expected ordered
    statistics of the fitted Pareto distribution
    \begin{itemize}
    \item corresponds to modeling of the tail, and as usual, modeling
      reduces the noise
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Estimating Pareto-$\hat{k}$}

  \begin{itemize}
  \item Fast empirical profile Bayes quadrature estimate by Zhang and
    Stephens (2009)
    \begin{itemize}
    \item excellent accuracy compared to exact Bayesian inference
    \item see more in \href{https://arxiv.org/abs/1507.02646}{Vehtari, Simpson, Gelman, Yao \& Gabry (2022)}
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Pareto-$\hat{k}$ diagnostic use cases}

  \vspace{-.5\baselineskip}
  \begin{itemize}
  \item Importance sampling
    \begin{itemize}
    \item leave-one-out cross-validation (Vehtari et al., 2016, 2017; Bürkner at al, 2020)
    \item Bayesian stacking (Yao et al., 2018, 2021, 2022)
    \item leave-future-out cross-validation (Bürkner et al., 2020)
    \item Bayesian bootstrap (Paananen et al, 2021, online appendix)
    \item prior and likelihood sensitivity analysis (Kallioinen et al., 2021)
    \item improving distributional approximations (Yao et al., 2018; Zhang et al., 2021; Dhaka et al., 2021)
    \item implicitly adaptive importance sampling (Paananen et al., 2021)
    \end{itemize}
  \item Stochastic optimization (Dhaka et al., 2020)
  \item Divergences and gradients in VI (Dhaka et al., 2021)
  \item MCMC (Paananen et al., 2021)
  \end{itemize}

\end{frame}

\begin{frame}{Importance sampling leave-one-out cross-validation}

  \begin{itemize}
  \item Later in the course you will learn how $p(\theta|y)$ can be
    used as a proposal distribution for $p(\theta|y_{-i})$
    \begin{itemize}
    \item which allows fast computation of leave-one-out cross-validation
      \begin{align*}
        p(y_i|y_{-i})=\int p(y_i|\theta) p(\theta|y_{-i}) d\theta
      \end{align*}
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Curse of dimensionality}

  \begin{itemize}
  \item Number of grid points increases exponentially
  \item Concentration of the measure, i.e., where is the most of the
    mass?
  \end{itemize}

\end{frame}

\begin{frame}{Markov chain Monte Carlo (MCMC)}

  \begin{itemize}
  \item Pros
    \begin{itemize}
    \item Markov chain goes where most of the posterior mass is
    \item Certain MCMC methods scale well to high dimensions
    \end{itemize}
  \item Cons
    \begin{itemize}
    \item Draws are dependent (affects how many draws are needed)
    \item Convergence in practical time is not guaranteed
    \end{itemize}
  \item MCMC methods in this course
    \begin{itemize}
    \item Gibbs: ``iterative conditional sampling''
    \item Metropolis: ``random walk in joint distribution''
    \item Dynamic Hamiltonian Monte Carlo: ``state-of-the-art'' used in Stan
    \end{itemize}
  \end{itemize}

\end{frame}

\end{document}

%%% Local Variables: 
%%% TeX-PDF-mode: t
%%% TeX-master: t
%%% End: 
