\documentclass[english,t]{beamer}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{newtxtext} % times
%\usepackage[scaled=.95]{cabin} % sans serif
\usepackage{amsmath}
\usepackage[varqu,varl]{inconsolata} % typewriter
\usepackage[varg]{newtxmath}
\usefonttheme[onlymath]{serif} % beamer font theme
\usepackage{microtype}
\usepackage{url}
\urlstyle{same}
\usepackage{graphicx}
\graphicspath{{./figs/}}
\usepackage{alltt}
\usepackage{subfigure}
\usepackage{enumerate}
\usepackage{listings}

\mode<presentation>
{
  \setbeamercovered{invisible}
  \setbeamertemplate{itemize items}[circle]
  \setbeamercolor{frametitle}{bg=white,fg=navyblue}
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{headline}[default]{}
  \setbeamertemplate{footline}[split]
  % \setbeamertemplate{headline}[text line]{\insertsection}
  % \setbeamertemplate{footline}[frame number]
}

\pdfinfo{            
  /Title      (BDA, Lecture 1, Introduction) 
  /Author     (Aki Vehtari) % 
  /Keywords   (Bayesian data analysis)
}

% Use tikz
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

\newenvironment{list1}{
  \begin{list}{$\color{list1}\bullet$}{\itemsep=6pt}}{
  \end{list}}
\newenvironment{list2}{
  \begin{list}{-}{\baselineskip=15pt}}{
  \end{list}}
\newenvironment{list3}{
  \begin{list}{$\cdot$}{\baselineskip=15pt}}{
  \end{list}}

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\Sd}{Sd}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\Gammad}{Gamma}
\DeclareMathOperator{\Invgamma}{Inv-gamma}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Negbin}{Neg-bin}
\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\logit}{logit}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\normal}{normal}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\BF}{BF}
\DeclareMathOperator{\Invchi2}{Inv-\chi^2}
% \DeclareMathOperator{\Pr}{Pr}
\def\euro{{\footnotesize \EUR\, }}
\DeclareMathOperator{\rep}{\mathrm{rep}}

\definecolor{navyblue}{rgb}{0,0,0.5}
\definecolor{darkgreen}{rgb}{0,0.3922,0}
\definecolor{matblue}{rgb}{0,0.447,0.741}    
\definecolor{matred}{rgb}{0.85,0.325,0.098}
\definecolor{matyellow}{rgb}{0.929,0.694,0.125}
\definecolor{matpurple}{rgb}{0.494,0.184,0.556}
\definecolor{matgreen}{rgb}{0.466,0.674,0.188}
\definecolor{matcyan}{rgb}{0.301,0.745,0.933}
\definecolor{matdarkred}{rgb}{0.635,0.078,0.184}

\newcommand{\nred}{{\color{matred}\#red}}
\newcommand{\nyellow}{{\color{matyellow}\#yellow}}
\newcommand{\red}{{\color{matred}\#red}}
\newcommand{\yellow}{{\color{matyellow}\#yellow}}

\hypersetup{pdfstartview={FitV}}
\date{}

\title[]{Bayesian data analysis}
\subtitle{Introduction}

\author{Aki Vehtari}

\institute[Aalto University]{}
  
\begin{document}

\begin{frame}
  \frametitle{Bayesian data analysis (Aalto fall 2023)}  %
  \framesubtitle{}
  
  \begin{itemize}
  \item Book: Gelman, Carlin, Stern, Dunson, Vehtari \& Rubin: Bayesian Data
    Analysis, Third Edition. {\footnotesize (online pdf available)}
  \item The course website has more detailed information\\
    {\small\url{https://avehtari.github.io/BDA_course_Aalto/Aalto2023.html}}
  \item Timetable: see the course website
  \item TAs: David Kohns, Noa Kallioinen, Andrew Johnson, Leevi
    Lindgren, Anna Riha, Niko Siccha, Maksim Sinelnikov, Teemu
    SÃ¤ilynoja
    \end{itemize}
    \vspace{-0.25\baselineskip}
 \begin{center}
   \includegraphics[width=2.6cm]{figs/BDA3.jpg}
 \end{center}

\end{frame}

\begin{frame}{Uncertainty and decision making}

  \begin{itemize}
  \item Predicting concrete quality
  \end{itemize}
  
  \vspace{2\baselineskip}
  \only<1>{
      \hspace{-6mm}
    \begin{minipage}[c]{0.475\linewidth}
    \includegraphics[height=3.7cm,trim=200 0 254 0,clip]{45396382385_3aa7f81768_k_ccby.jpg}
    \end{minipage}
    \begin{minipage}[c]{0.8cm}
      \centering
      \hspace{3mm}$\rightarrow$\hspace{-1.5mm}
    \end{minipage}
    \begin{minipage}[c]{0.475\linewidth}
    \includegraphics[height=3.7cm]{ConcreteBridge.jpg}
    \end{minipage}
    }
  % \uncover<2->{
  %   \includegraphics[width=5cm]{45396382385_3aa7f81768_k.jpg}\\
  %   \vspace{-0.5mm}
  %   $\downarrow$\\
  %   \vspace{1mm}
  %   \includegraphics[width=5cm]{ConcreteBridge.jpg}
  %   }
  % \includegraphics[width=5cm]{slump1b-eps-converted-to.pdf}
%     %https://www.flickr.com/photos/tareqsalahuddin/7273434238
%   \uncover<3->{\includegraphics[width=6cm]{7273434238_a40e5fc127_c.jpg}}}\\
% \vspace{0.5\baselineskip}
% \uncover<4->{\hspace{3.8cm}\includegraphics[width=7cm]{data_population_simple.pdf}}

\end{frame}

\begin{frame}{Uncertainty and decision making\footnote{\color{gray}with E. Siivola, Aalto and S. Weber, Novartis Pharma}}

  \vspace{-0.5\baselineskip}
\begin{itemize}
\item Everolimus is immunosuppressant to prevent rejection of organ
  transplants
\item Pharmacokinetic model of drug and body, optimal dosage depends on weight\\
  \begin{minipage}[t]{\textwidth}
    \vspace{-.2\baselineskip}
  \hspace{-1.2cm}\includegraphics[width=6cm]{2compartment_graph.png}
  \includegraphics[width=6cm]{data_population_simple.pdf}
\end{minipage}
    \vspace{.2\baselineskip}
\item<2-> Model fitted with 500 adults, extrapolation to children?
\item<3-> Maturation effect, 17 observations from children
\end{itemize}

\end{frame}

\begin{frame}{Uncertainty in modeling}

  % fake data
  % mean fit
  % mean fit + gaussian
  % samples
  % samples + gaussians
  % 1D data
  % gaussian fit
  % gaussian samples
  % posterior draws
  % exact contours
  % marginal with jitter?
  % marginal outside of the plot?
  % marginal with jitter in other direction
  % marginal outside of the in other direction
  
  \only<1>{\includegraphics[width=10cm]{fakel_data.pdf}}
  \only<2>{\includegraphics[width=10cm]{fakel_postmean.pdf}}
  \only<3>{\includegraphics[width=10cm]{fakel_postmeanpred.pdf}}
  \only<4>{\includegraphics[width=10cm]{fakel_postdraws.pdf}}
  \only<5>{\includegraphics[width=10cm]{fakel_postdrawspred.pdf}}
%  \only<6>{\includegraphics[width=10cm]{fakel_postdrawspred2.pdf}}

\end{frame}

% \begin{frame}{Uncertainty and decision making}

%   \vspace{\baselineskip}
%   \centering
%   \uncover<1->{\includegraphics[width=6.5cm]{7273434238_a40e5fc127_c.jpg}}\\
% \vspace{0.5\baselineskip}
% \uncover<2->{  \centering
% \includegraphics[width=6.5cm]{data_population_simple.pdf}}

% \end{frame}

\begin{frame}{Bayesian probability theory}

  \vspace{\baselineskip}
  
  expert information \hfill \uncover<3->{data}

  \hspace{0.8cm}\uncover<2->{$\searrow$} \hfill \uncover<4->{$\swarrow$}\hspace{0.8cm}

  \uncover<2->{
    \begin{minipage}[t]{0.45\linewidth}\center
mathematical model \\ + \\ uncertainty with probabilities
\end{minipage}
} \hfill \uncover<4->{
  \begin{minipage}[t]{0.05\linewidth}
    ~\\
+\\
\end{minipage}
\hfill
  \begin{minipage}[t]{0.45\linewidth}
    Bayesian probability theory\\
    \vspace{-\baselineskip}
{\color{gray}\footnotesize
  \begin{align*}
     p(\theta|y) & = \frac{p(y|\theta)p(\theta)}{p(y)} \\
    p(\tilde{y}|y) & = \int p(\tilde{y}|\theta) p(\theta|y)d\theta
  \end{align*}}
\end{minipage}
}

   \vspace{-2\baselineskip}

  \center 

  \uncover<5->{$\downarrow$
  
    updated uncertainty\\
    \uncover<6->{+\\ understandable trusted models}}

\end{frame}

\begin{frame}{Bayesian inference with computers}

  \vspace{\baselineskip}
\hspace{1cm}  \begin{minipage}[t]{0.8\linewidth}
  mathematical model to computer
      \vspace{\baselineskip}

probabilistic programming
      \vspace{\baselineskip}

computation, automatic inference algorithms
      \vspace{\baselineskip}

limitations of computers
\end{minipage}

\end{frame}

\begin{frame}{Yes, but did it work?}

  \vspace{\baselineskip}
\hspace{1cm}  \begin{minipage}[t]{0.8\linewidth}
computation + inference diagnostics
      \vspace{\baselineskip}

model diagnostics
      \vspace{\baselineskip}

limitations of mathematical models
      \vspace{\baselineskip}

improve and iterate
\end{minipage}

\end{frame}

\begin{frame}{Probabilistic programming and Stan}

  Stan is a probabilistic programming framework and ecosystem

  40+ developers, 100+ contributors, 100K+ users

  \center
  \vspace{\baselineskip}
  \includegraphics[width=2.5cm]{stan_logo_wide.png}\\
  mc-stan.org
  
\end{frame}

\begin{frame}{Bayesian Data Analysis course}

  \begin{itemize}
  \item Probability distributions as model building blocks
    \begin{itemize}
    \item need to understand the math part (prereq.)
    \item continuous vs discrete (prereq.)
    \item observation model, likelihood, prior
    \item constructing bigger models
    \end{itemize}
  \item<2-> Computation
    \begin{itemize}
    \item 
  We need to be able to compute expectations% with respect to posterior
  %distribution ${\color{blue}p(\theta|y)}$
  \begin{align*}
    \E_{\color{blue}{\theta|y}}\left[ g(\theta) \right] = \int {\color{blue}p(\theta|y)}g(\theta) d\theta
  \end{align*}
    \item when analytic solutions are not available, computational
      approximations with finite number of function evaluations
    \item grid, importance sampling, Monte Carlo, Markov chain Monte Carlo
    \end{itemize}
  \item<3-> Workflow
    \begin{itemize}
    \item steps of model building, inference, and diagnostics
    \end{itemize}
  \end{itemize}
  
\end{frame}

\begin{frame}{Impact on society}

  \vspace{\baselineskip}
    Better modelling and quantification of uncertainty\\
  \hspace{0.5cm}  \begin{minipage}[t]{0.8\linewidth}
  \vspace{0.5\baselineskip}
    
  \begin{itemize}
  \item[$\rightarrow$] better science
  \vspace{\baselineskip}
  
\item[$\rightarrow$] better informed decision making\\ in companies, government, and NGOs
\end{itemize}
\end{minipage}

\end{frame}

\begin{frame}{Bayesian probability theory}

  \begin{itemize}
  \item Based on Bayesian probability theory
    \begin{itemize}
    \item uncertainty is presented with probabilities
    \item probabilities are updated based on new information
    \end{itemize}
    \pause
    \item Thomas Bayes (170?--1761)
    \begin{itemize}
    \item English nonconformist, Presbyterian minister,
      mathematician
    \item considered the problem of {\it inverse probability}
      \begin{itemize}
        \item significant part of the Bayesian theory
      \end{itemize}
  \end{itemize}
  \pause
  \item Bayes did not invent all, but was first to solve problem of
    inverse probability in special case
  \item Modern Bayesian theory with rigorous proofs developed in
    20th century
    \pause
  \item A nice book about history: Sharon Bertsch McGrayne,
    \textit{The Theory That Would Not Die}, 2012.
\end{itemize}
\end{frame}

\begin{frame}{Term Bayesian used first time in mid 20th century}

  \begin{itemize}
  \item Earlier there was just "probability theory"
    \begin{itemize}
    \item concept of the probability was not strictly defined,
      although it was close to modern Bayesian interpretation
    \item in the end of 19th century there were increasing demand
      for more strict definition of probability (mathematical and
      philosophical problem)
    \end{itemize}
    \pause
    \item In the beginning of 20th century frequentist view gained popularity
    \begin{itemize}
    \item accepts definition of probabilities only through frequencies
    \item does not accept inverse probability or use of prior
    \item gained popularity due to apparent objectivity and "cook
      book" like reference books
    \end{itemize}
    \pause
    \item R. A. Fisher used in 1950 first
      time term "Bayesian" to emphasize the difference to general
      term "probability theory"
      \begin{itemize}
      \item term became quickly popular, because alternative
        descriptions were longer
    \end{itemize}
    \pause
    \item The probabilistic programming revolution started in early 1990's
\end{itemize}
\end{frame}

\begin{frame}{Uncertainty and probabilistic modeling}

  \begin{itemize}
  \item Two types of uncertainty: aleatoric and epistemic
    \vspace{\baselineskip}
  \item Representing uncertainty with probabilities
    \vspace{\baselineskip}
  \item Updating uncertainty
   \end{itemize}
\end{frame}

\begin{frame}{Two types of uncertainty}

  \begin{itemize}
  \item Aleatoric uncertainty due to randomness
    \begin{itemize}
    \item<2-> we are not able to obtain observations which could reduce
      this uncertainty
    \end{itemize}
    \vspace{\baselineskip}
  \item Epistemic uncertainty due to lack of knowledge
    \begin{itemize}
    \item<3-> we are able to obtain observations which can reduce
      this uncertainty
    \item<3-> two observers may have different epistemic uncertainty
    %\item<3-> epistemic uncertainty changes when the information chances
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Updating uncertainty}

  \begin{itemize}
  \item<2-> Probability of red $\frac{\mathrm{\nred}}{\mathrm{\nred+\nyellow}}=\theta$ 
    \vspace{\baselineskip}
  \item<3-> $p(y=\mathrm{\red}|\theta)=\theta \quad$ aleatoric uncertainty
    \vspace{\baselineskip}
  \item<4-> $p(\theta) \quad$ epistemic uncertainty
    \vspace{\baselineskip}
  \item<5-> Picking many chips updates our uncertainty about the proportion
    \vspace{\baselineskip}
  \item<5-> $p(\theta|\mathrm{y=\red,\yellow,\red,\red,\ldots})=?$
    \vspace{\baselineskip}
  \item<6-> Bayes rule
      $p(\theta|y)=\frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta) d\theta}$
  \end{itemize}
\end{frame}

\begin{frame}{Model vs. likelihood}

  \begin{itemize}
  \item Bayes rule
      ${\color{blue}p(\theta|y)}\propto {\color{darkgreen}p(y|\theta)}{\color{darkgreen}p(\theta)}$
    \vspace{\baselineskip}
  \item Model: {\color{darkgreen}$p(\mathbf{y}|\theta)$} as a function of $\mathbf{y}$ given fixed $\theta$
    describes the aleatoric uncertainty \vspace{\baselineskip}
  \item Likelihood: {\color{darkgreen}$p(y|\boldsymbol\theta)$} %=L(\theta|y)$ 
    as a function of $\boldsymbol\theta$
    given fixed $y$ provides information about epistemic uncertainty,
    but is not a probability distribution
    \vspace{\baselineskip}
  \item<2-> Bayes rule combines the likelihood with prior uncertainty
    $p(\theta)$ and transforms them to updated posterior uncertainty
  \end{itemize}
\end{frame}

\begin{frame}{The art of probabilistic modeling}

  \begin{itemize}
  \item The art of probabilistic modeling is to describe in a
    mathematical form (model and prior distributions) what we already
    know and what we don't know 
\vspace{\baselineskip}
  \item<2-> ``Easy'' part is to use Bayes rule to update the uncertainties
    \begin{itemize}
    \item computational challenges
    \end{itemize}
\vspace{\baselineskip}
  \item<3-> Other parts of the art of probabilistic modeling are, for example,
    \begin{itemize}
    \item model checking: is data in conflict with our prior knowledge?
    \item presentation: presenting the model and the results to the application experts
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}{Modeling nature}

  \begin{itemize}
  \item Drop a ball from different heights and measure time
    \pause
    \begin{itemize}
    \item Newton
    \item air resistance, air pressure, shape and surface structure of the ball
    \item relativity
    \end{itemize}
    \pause
  \item Taking into account the accuracy of the measurements, how
    accurate model is needed?
    \pause
    \begin{itemize}
    \item often simple models are adequate and useful
    \item \emph{All models are wrong, but some of them are useful},
      George P. Box
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}{Reminder: Uncertainty and probabilistic modeling}

  \begin{itemize}
  \item Two types of uncertainty: aleatoric and epistemic
    \vspace{\baselineskip}
  \item Representing uncertainty with probabilities
    \vspace{\baselineskip}
  \item Updating uncertainty
    \vspace{\baselineskip}
   \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Chapter 1}  %
  \framesubtitle{Reading instructions}
  \begin{itemize}
\item 1.1-1.3 important terms
\item 1.4 a useful example
\item 1.5 foundations
\item 1.6 \& 1.7 examples (can be skipped, but may be useful to read)
\item 1.8 \& 1.9 background material, good to read before doing the exercises
\item 1.10 a point of view for using Bayesian inference
  \end{itemize}

\end{frame}

\begin{frame}{Part of the assignment 1}

  Refresh your memory on these concepts!
  \begin{itemize}
  \item[-] probability
  \item[-] probability density
  \item[-] probability mass
  \item[-] probability density function (pdf)
  \item[-] probability mass function (pmf)
  \item[-] probability distribution
  \item[-] discrete probability distribution
  \item[-] continuous probability distribution
  \item[-] cumulative distribution function (cdf)
  \item[-] likelihood
  \end{itemize}
  
\end{frame}

\begin{frame}{Ambiguous notation in statistics}

  \vspace{-\baselineskip}
  {\small Find this in the Chapter 1 reading instructions on the course web page!}
  
  \begin{itemize}
  \item[] In $p(y \mid \theta)$
    % \pause
  \begin{itemize}
  \item[-] $y$ can be variable or value
    \begin{itemize}
    \item[] we could clarify by using $p(Y \mid \theta)$ or $p(y \mid \theta)$
    \end{itemize}
    % \pause
  \item[-] $\theta$ can be variable or value
    \begin{itemize}
    \item[] we could clarify by using $p(y \mid \Theta)$ or $p(y \mid \theta)$
    \end{itemize}
    % \pause
  \item[-] $p$ can be a discrete or continuous function of $y$ or $\theta$
    \begin{itemize}
    \item[] we could clarify by using $P_Y$, $P_\Theta$, $p_Y$ or $p_\Theta$
    \end{itemize}
    % \pause
\item[-]
  $P_Y(Y \mid \Theta=\theta)$ is a probability mass function, sampling distribution, observation model
    % \pause
\item[-]
$P(Y=y \mid \Theta=\theta)$ is a probability
    % \pause
\item[-]
$P_\Theta(Y=y \mid \Theta)$ is a likelihood function (can be discrete or continuous)
    % \pause
\item[-] $p_Y(Y \mid \Theta=\theta)$ is a probability density function, sampling distrbution, observation model
    % \pause
\item[-] $p(Y=y \mid \Theta=\theta)$ is a density
    % \pause
\item[-] $p_\Theta(Y=y \mid \Theta)$ is a likelihood function (can be discrete or continuous)
    % \pause
  \item[-] $y$ and $\theta$ can also be mix of continuous and discrete
    % \pause
    \item[-] due to the sloppines sometimes likelihood is used to refer
$P_{Y,\theta}(Y \mid \Theta)$, $p_{Y,\theta}(Y \mid \Theta)$

  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Questions}

  \begin{itemize}
  \item<1-> Pick a number between 1--5
    \begin{itemize}
    \item<2-> raise as many fingers 
    \item<3-> is the number of fingers raised random (by you or by others)?
    \end{itemize}
    \vspace{\baselineskip}
  \item<4-> If we build a robot with very fast vision which can observe
    the rotating coin accurately, is the throw random for the robot?
    \vspace{\baselineskip}
  % \item<5-> Is the quantum uncertainty aleatoric or epistemic?
  %   \vspace{\baselineskip}
  \item<5-> What is your own example with both aleatoric and epistemic
    uncertainty?
  \end{itemize}
\end{frame}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
