<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-17">

<title>Bayesian Data Analysis course - BDA3 notes – Bayesian Data Analysis course</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dc55a5b9e770e841cd82e46aadbfb9b0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-1713c1db1ad41a796903378fc9ac7b2b.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Bayesian Data Analysis course</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Material</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./Aalto2025.html"> 
<span class="menu-text">Aalto 2025</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./BDA3_notes.html" aria-current="page"> 
<span class="menu-text">BDA3 notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./assignments.html"> 
<span class="menu-text">Assignments</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./project.html"> 
<span class="menu-text">Project</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./demos.html"> 
<span class="menu-text">Demos</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./FAQ.html"> 
<span class="menu-text">FAQ</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#ch1" id="toc-ch1" class="nav-link active" data-scroll-target="#ch1">Chapter 1 Probability and inference</a>
  <ul class="collapse">
  <li><a href="#outline" id="toc-outline" class="nav-link" data-scroll-target="#outline">Outline</a></li>
  <li><a href="#the-most-important-terms" id="toc-the-most-important-terms" class="nav-link" data-scroll-target="#the-most-important-terms">The most important terms</a></li>
  <li><a href="#recommended-exercises" id="toc-recommended-exercises" class="nav-link" data-scroll-target="#recommended-exercises">Recommended exercises</a></li>
  <li><a href="#distributed-as-sim" id="toc-distributed-as-sim" class="nav-link" data-scroll-target="#distributed-as-sim">Distributed as, <span class="math inline">\(\sim\)</span></a></li>
  <li><a href="#proportional-to-propto" id="toc-proportional-to-propto" class="nav-link" data-scroll-target="#proportional-to-propto">Proportional to, <span class="math inline">\(\propto\)</span></a></li>
  <li><a href="#model-and-likelihood" id="toc-model-and-likelihood" class="nav-link" data-scroll-target="#model-and-likelihood">Model and likelihood</a></li>
  <li><a href="#two-types-of-uncertainty" id="toc-two-types-of-uncertainty" class="nav-link" data-scroll-target="#two-types-of-uncertainty">Two types of uncertainty</a></li>
  <li><a href="#transformation-of-variables" id="toc-transformation-of-variables" class="nav-link" data-scroll-target="#transformation-of-variables">Transformation of variables</a></li>
  <li><a href="#ambiguous-notation-in-statistics" id="toc-ambiguous-notation-in-statistics" class="nav-link" data-scroll-target="#ambiguous-notation-in-statistics">Ambiguous notation in statistics</a></li>
  <li><a href="#exchangeability" id="toc-exchangeability" class="nav-link" data-scroll-target="#exchangeability">Exchangeability</a></li>
  <li><a href="#number-of-digits" id="toc-number-of-digits" class="nav-link" data-scroll-target="#number-of-digits">Number of digits</a></li>
  </ul></li>
  <li><a href="#ch2" id="toc-ch2" class="nav-link" data-scroll-target="#ch2">Chapter 2 Single-parameter models</a>
  <ul class="collapse">
  <li><a href="#outline-1" id="toc-outline-1" class="nav-link" data-scroll-target="#outline-1">Outline</a></li>
  <li><a href="#the-most-important-terms-1" id="toc-the-most-important-terms-1" class="nav-link" data-scroll-target="#the-most-important-terms-1">The most important terms</a></li>
  <li><a href="#r-and-python-demos" id="toc-r-and-python-demos" class="nav-link" data-scroll-target="#r-and-python-demos">R and Python demos</a></li>
  <li><a href="#recommended-exercises-1" id="toc-recommended-exercises-1" class="nav-link" data-scroll-target="#recommended-exercises-1">Recommended exercises</a></li>
  <li><a href="#posterior-draws-and-sample" id="toc-posterior-draws-and-sample" class="nav-link" data-scroll-target="#posterior-draws-and-sample">Posterior draws and sample</a></li>
  <li><a href="#posterior-credible-and-confidence-intervals" id="toc-posterior-credible-and-confidence-intervals" class="nav-link" data-scroll-target="#posterior-credible-and-confidence-intervals">Posterior, credible, and confidence intervals</a></li>
  <li><a href="#integration-over-beta-distribution" id="toc-integration-over-beta-distribution" class="nav-link" data-scroll-target="#integration-over-beta-distribution">Integration over Beta distribution</a></li>
  <li><a href="#numerical-accuracy" id="toc-numerical-accuracy" class="nav-link" data-scroll-target="#numerical-accuracy">Numerical accuracy</a></li>
  <li><a href="#highest-posterior-density-interval" id="toc-highest-posterior-density-interval" class="nav-link" data-scroll-target="#highest-posterior-density-interval">Highest Posterior Density interval</a></li>
  <li><a href="#gaussian-distribution-in-more-complex-models-and-methods" id="toc-gaussian-distribution-in-more-complex-models-and-methods" class="nav-link" data-scroll-target="#gaussian-distribution-in-more-complex-models-and-methods">Gaussian distribution in more complex models and methods</a></li>
  <li><a href="#predictive-distribution" id="toc-predictive-distribution" class="nav-link" data-scroll-target="#predictive-distribution">Predictive distribution</a></li>
  <li><a href="#non-informative-and-weakly-informative-priors" id="toc-non-informative-and-weakly-informative-priors" class="nav-link" data-scroll-target="#non-informative-and-weakly-informative-priors">Non-informative and weakly informative priors</a></li>
  <li><a href="#should-we-worry-about-rigged-priors" id="toc-should-we-worry-about-rigged-priors" class="nav-link" data-scroll-target="#should-we-worry-about-rigged-priors">Should we worry about rigged priors?</a></li>
  <li><a href="#prior-knowledge-elicitation" id="toc-prior-knowledge-elicitation" class="nav-link" data-scroll-target="#prior-knowledge-elicitation">Prior knowledge elicitation</a></li>
  <li><a href="#exchangeability-1" id="toc-exchangeability-1" class="nav-link" data-scroll-target="#exchangeability-1">Exchangeability</a></li>
  <li><a href="#the-number-of-left-handed-students-in-the-class" id="toc-the-number-of-left-handed-students-in-the-class" class="nav-link" data-scroll-target="#the-number-of-left-handed-students-in-the-class">The number of left-handed students in the class</a></li>
  </ul></li>
  <li><a href="#ch3" id="toc-ch3" class="nav-link" data-scroll-target="#ch3">Chapter 3 Introduction to multiparameter models</a>
  <ul class="collapse">
  <li><a href="#outline-2" id="toc-outline-2" class="nav-link" data-scroll-target="#outline-2">Outline</a></li>
  <li><a href="#the-most-important-terms-2" id="toc-the-most-important-terms-2" class="nav-link" data-scroll-target="#the-most-important-terms-2">The most important terms</a></li>
  <li><a href="#r-and-python-demos-1" id="toc-r-and-python-demos-1" class="nav-link" data-scroll-target="#r-and-python-demos-1">R and Python demos</a></li>
  <li><a href="#recommended-exercises-2" id="toc-recommended-exercises-2" class="nav-link" data-scroll-target="#recommended-exercises-2">Recommended exercises</a></li>
  <li><a href="#conjugate-prior-for-normal-distribution" id="toc-conjugate-prior-for-normal-distribution" class="nav-link" data-scroll-target="#conjugate-prior-for-normal-distribution">Conjugate prior for normal distribution</a></li>
  <li><a href="#trace-of-square-matrix" id="toc-trace-of-square-matrix" class="nav-link" data-scroll-target="#trace-of-square-matrix">Trace of square matrix</a></li>
  <li><a href="#history-and-naming-of-distributions" id="toc-history-and-naming-of-distributions" class="nav-link" data-scroll-target="#history-and-naming-of-distributions">History and naming of distributions</a></li>
  <li><a href="#using-monte-carlo-to-obtain-draws-from-the-posterior-of-generated-quantities" id="toc-using-monte-carlo-to-obtain-draws-from-the-posterior-of-generated-quantities" class="nav-link" data-scroll-target="#using-monte-carlo-to-obtain-draws-from-the-posterior-of-generated-quantities">Using Monte Carlo to obtain draws from the posterior of generated quantities</a></li>
  <li><a href="#the-number-of-required-monte-carlo-draws" id="toc-the-number-of-required-monte-carlo-draws" class="nav-link" data-scroll-target="#the-number-of-required-monte-carlo-draws">The number of required Monte Carlo draws</a></li>
  <li><a href="#bioassay" id="toc-bioassay" class="nav-link" data-scroll-target="#bioassay">Bioassay</a></li>
  <li><a href="#bayesian-vs.-frequentist-statements-in-two-group-comparisons" id="toc-bayesian-vs.-frequentist-statements-in-two-group-comparisons" class="nav-link" data-scroll-target="#bayesian-vs.-frequentist-statements-in-two-group-comparisons">Bayesian vs.&nbsp;frequentist statements in two group comparisons</a></li>
  <li><a href="#unimodal-and-multimodal" id="toc-unimodal-and-multimodal" class="nav-link" data-scroll-target="#unimodal-and-multimodal">Unimodal and multimodal</a></li>
  </ul></li>
  <li><a href="#ch4" id="toc-ch4" class="nav-link" data-scroll-target="#ch4">Chapter 4 Asymptotics and connections to non-Bayesian approaches</a>
  <ul class="collapse">
  <li><a href="#outline-3" id="toc-outline-3" class="nav-link" data-scroll-target="#outline-3">Outline</a></li>
  <li><a href="#the-most-important-terms-3" id="toc-the-most-important-terms-3" class="nav-link" data-scroll-target="#the-most-important-terms-3">The most important terms</a></li>
  <li><a href="#r-and-python-demos-2" id="toc-r-and-python-demos-2" class="nav-link" data-scroll-target="#r-and-python-demos-2">R and Python demos</a></li>
  <li><a href="#normal-approximation" id="toc-normal-approximation" class="nav-link" data-scroll-target="#normal-approximation">Normal approximation</a></li>
  <li><a href="#observed-information" id="toc-observed-information" class="nav-link" data-scroll-target="#observed-information">Observed information</a></li>
  <li><a href="#aliasing" id="toc-aliasing" class="nav-link" data-scroll-target="#aliasing">Aliasing</a></li>
  <li><a href="#frequency-property-vs.-frequentist" id="toc-frequency-property-vs.-frequentist" class="nav-link" data-scroll-target="#frequency-property-vs.-frequentist">Frequency property vs.&nbsp;frequentist</a></li>
  <li><a href="#transformation-of-variables-1" id="toc-transformation-of-variables-1" class="nav-link" data-scroll-target="#transformation-of-variables-1">Transformation of variables</a></li>
  <li><a href="#on-derivation" id="toc-on-derivation" class="nav-link" data-scroll-target="#on-derivation">On derivation</a></li>
  </ul></li>
  <li><a href="#ch5" id="toc-ch5" class="nav-link" data-scroll-target="#ch5">Chapter 5 Hierarchical models</a>
  <ul class="collapse">
  <li><a href="#outline-4" id="toc-outline-4" class="nav-link" data-scroll-target="#outline-4">Outline</a></li>
  <li><a href="#the-most-important-terms-4" id="toc-the-most-important-terms-4" class="nav-link" data-scroll-target="#the-most-important-terms-4">The most important terms</a></li>
  <li><a href="#r-and-python-demos-3" id="toc-r-and-python-demos-3" class="nav-link" data-scroll-target="#r-and-python-demos-3">R and Python demos</a></li>
  <li><a href="#recommended-exercises-3" id="toc-recommended-exercises-3" class="nav-link" data-scroll-target="#recommended-exercises-3">Recommended exercises</a></li>
  <li><a href="#computation" id="toc-computation" class="nav-link" data-scroll-target="#computation">Computation</a></li>
  <li><a href="#exchangeability-vs.-independence" id="toc-exchangeability-vs.-independence" class="nav-link" data-scroll-target="#exchangeability-vs.-independence">Exchangeability vs.&nbsp;independence</a></li>
  <li><a href="#what-if-observations-are-not-exchangeable" id="toc-what-if-observations-are-not-exchangeable" class="nav-link" data-scroll-target="#what-if-observations-are-not-exchangeable">What if observations are not exchangeable</a></li>
  <li><a href="#weakly-informative-priors-for-hierarchical-variance-parameters" id="toc-weakly-informative-priors-for-hierarchical-variance-parameters" class="nav-link" data-scroll-target="#weakly-informative-priors-for-hierarchical-variance-parameters">Weakly informative priors for hierarchical variance parameters</a></li>
  </ul></li>
  <li><a href="#ch6" id="toc-ch6" class="nav-link" data-scroll-target="#ch6">Chapter 6 Model checking</a>
  <ul class="collapse">
  <li><a href="#outline-5" id="toc-outline-5" class="nav-link" data-scroll-target="#outline-5">Outline</a></li>
  <li><a href="#the-most-important-terms-5" id="toc-the-most-important-terms-5" class="nav-link" data-scroll-target="#the-most-important-terms-5">The most important terms</a></li>
  <li><a href="#r-and-python-demos-4" id="toc-r-and-python-demos-4" class="nav-link" data-scroll-target="#r-and-python-demos-4">R and Python demos</a></li>
  <li><a href="#recommended-exercises-4" id="toc-recommended-exercises-4" class="nav-link" data-scroll-target="#recommended-exercises-4">Recommended exercises</a></li>
  <li><a href="#replicates-vs.-future-observation" id="toc-replicates-vs.-future-observation" class="nav-link" data-scroll-target="#replicates-vs.-future-observation">Replicates vs.&nbsp;future observation</a></li>
  <li><a href="#posterior-predictive-p-values" id="toc-posterior-predictive-p-values" class="nav-link" data-scroll-target="#posterior-predictive-p-values">Posterior predictive <span class="math inline">\(p\)</span>-values</a></li>
  <li><a href="#prior-predictive-checking" id="toc-prior-predictive-checking" class="nav-link" data-scroll-target="#prior-predictive-checking">Prior predictive checking</a></li>
  <li><a href="#additional-reading" id="toc-additional-reading" class="nav-link" data-scroll-target="#additional-reading">Additional reading</a></li>
  </ul></li>
  <li><a href="#ch7" id="toc-ch7" class="nav-link" data-scroll-target="#ch7">Chapter 7 Evaluating, comparing, and expanding models</a>
  <ul class="collapse">
  <li><a href="#outline-6" id="toc-outline-6" class="nav-link" data-scroll-target="#outline-6">Outline</a></li>
  <li><a href="#extra-material" id="toc-extra-material" class="nav-link" data-scroll-target="#extra-material">Extra material</a></li>
  <li><a href="#the-most-important-terms-6" id="toc-the-most-important-terms-6" class="nav-link" data-scroll-target="#the-most-important-terms-6">The most important terms</a></li>
  <li><a href="#additional-reading-1" id="toc-additional-reading-1" class="nav-link" data-scroll-target="#additional-reading-1">Additional reading</a></li>
  <li><a href="#posterior-probability-of-the-model-vs.-predictive-performance" id="toc-posterior-probability-of-the-model-vs.-predictive-performance" class="nav-link" data-scroll-target="#posterior-probability-of-the-model-vs.-predictive-performance">Posterior probability of the model vs.&nbsp;predictive performance</a></li>
  </ul></li>
  <li><a href="#ch8" id="toc-ch8" class="nav-link" data-scroll-target="#ch8">Chapter 8 Modeling accounting for data collection</a>
  <ul class="collapse">
  <li><a href="#outline-7" id="toc-outline-7" class="nav-link" data-scroll-target="#outline-7">Outline</a></li>
  <li><a href="#the-most-important-terms-7" id="toc-the-most-important-terms-7" class="nav-link" data-scroll-target="#the-most-important-terms-7">The most important terms</a></li>
  </ul></li>
  <li><a href="#ch9" id="toc-ch9" class="nav-link" data-scroll-target="#ch9">Chapter 9 Decision analysis</a>
  <ul class="collapse">
  <li><a href="#outline-8" id="toc-outline-8" class="nav-link" data-scroll-target="#outline-8">Outline</a></li>
  <li><a href="#the-most-important-terms-8" id="toc-the-most-important-terms-8" class="nav-link" data-scroll-target="#the-most-important-terms-8">The most important terms</a></li>
  <li><a href="#simpler-examples" id="toc-simpler-examples" class="nav-link" data-scroll-target="#simpler-examples">Simpler examples</a></li>
  <li><a href="#model-selection-as-a-decision-problem" id="toc-model-selection-as-a-decision-problem" class="nav-link" data-scroll-target="#model-selection-as-a-decision-problem">Model selection as a decision problem</a></li>
  </ul></li>
  <li><a href="#ch10" id="toc-ch10" class="nav-link" data-scroll-target="#ch10">Chapter 10 Introduction to Bayesian computation</a>
  <ul class="collapse">
  <li><a href="#outline-9" id="toc-outline-9" class="nav-link" data-scroll-target="#outline-9">Outline</a></li>
  <li><a href="#the-most-important-terms-9" id="toc-the-most-important-terms-9" class="nav-link" data-scroll-target="#the-most-important-terms-9">The most important terms</a></li>
  <li><a href="#r-and-python-demos-5" id="toc-r-and-python-demos-5" class="nav-link" data-scroll-target="#r-and-python-demos-5">R and Python demos</a></li>
  <li><a href="#recommended-exercises-5" id="toc-recommended-exercises-5" class="nav-link" data-scroll-target="#recommended-exercises-5">Recommended exercises</a></li>
  <li><a href="#numerical-accuracy-of-computer-arithmetic" id="toc-numerical-accuracy-of-computer-arithmetic" class="nav-link" data-scroll-target="#numerical-accuracy-of-computer-arithmetic">Numerical accuracy of computer arithmetic</a></li>
  <li><a href="#terms-draw-draws-and-sample" id="toc-terms-draw-draws-and-sample" class="nav-link" data-scroll-target="#terms-draw-draws-and-sample">Terms draw, draws and sample</a></li>
  <li><a href="#monte-carlo-standard-error" id="toc-monte-carlo-standard-error" class="nav-link" data-scroll-target="#monte-carlo-standard-error">Monte Carlo standard error</a></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem" class="nav-link" data-scroll-target="#central-limit-theorem">Central limit theorem</a></li>
  <li><a href="#pareto-hatk-diagnostic" id="toc-pareto-hatk-diagnostic" class="nav-link" data-scroll-target="#pareto-hatk-diagnostic">Pareto-<span class="math inline">\(\hat{k}\)</span> diagnostic</a></li>
  <li><a href="#how-many-digits-should-be-displayed" id="toc-how-many-digits-should-be-displayed" class="nav-link" data-scroll-target="#how-many-digits-should-be-displayed">How many digits should be displayed</a></li>
  <li><a href="#quadrature" id="toc-quadrature" class="nav-link" data-scroll-target="#quadrature">Quadrature</a></li>
  <li><a href="#rejection-sampling" id="toc-rejection-sampling" class="nav-link" data-scroll-target="#rejection-sampling">Rejection sampling</a></li>
  <li><a href="#importance-sampling" id="toc-importance-sampling" class="nav-link" data-scroll-target="#importance-sampling">Importance sampling</a></li>
  <li><a href="#importance-resampling-with-or-without-replacement" id="toc-importance-resampling-with-or-without-replacement" class="nav-link" data-scroll-target="#importance-resampling-with-or-without-replacement">Importance resampling with or without replacement</a></li>
  <li><a href="#importance-sampling-effective-sample-size" id="toc-importance-sampling-effective-sample-size" class="nav-link" data-scroll-target="#importance-sampling-effective-sample-size">Importance sampling effective sample size</a></li>
  <li><a href="#buffons-needles" id="toc-buffons-needles" class="nav-link" data-scroll-target="#buffons-needles">Buffon’s needles</a></li>
  </ul></li>
  <li><a href="#ch11" id="toc-ch11" class="nav-link" data-scroll-target="#ch11">Chapter 11 Basics of Markov chain simulation</a>
  <ul class="collapse">
  <li><a href="#outline-10" id="toc-outline-10" class="nav-link" data-scroll-target="#outline-10">Outline</a></li>
  <li><a href="#the-most-important-terms-10" id="toc-the-most-important-terms-10" class="nav-link" data-scroll-target="#the-most-important-terms-10">The most important terms</a></li>
  <li><a href="#r-and-python" id="toc-r-and-python" class="nav-link" data-scroll-target="#r-and-python">R and Python</a></li>
  <li><a href="#recommended-exercises-6" id="toc-recommended-exercises-6" class="nav-link" data-scroll-target="#recommended-exercises-6">Recommended exercises</a></li>
  <li><a href="#basics-of-markov-chains" id="toc-basics-of-markov-chains" class="nav-link" data-scroll-target="#basics-of-markov-chains">Basics of Markov chains</a></li>
  <li><a href="#animations" id="toc-animations" class="nav-link" data-scroll-target="#animations">Animations</a></li>
  <li><a href="#metropolis-algorithm" id="toc-metropolis-algorithm" class="nav-link" data-scroll-target="#metropolis-algorithm">Metropolis algorithm</a></li>
  <li><a href="#transition-distribution-vs.-proposal-distribution" id="toc-transition-distribution-vs.-proposal-distribution" class="nav-link" data-scroll-target="#transition-distribution-vs.-proposal-distribution">Transition distribution vs.&nbsp;proposal distribution</a></li>
  <li><a href="#convergence" id="toc-convergence" class="nav-link" data-scroll-target="#convergence">Convergence</a></li>
  <li><a href="#widehatr-effective-sample-size-ess-previously-n_mathrmeff" id="toc-widehatr-effective-sample-size-ess-previously-n_mathrmeff" class="nav-link" data-scroll-target="#widehatr-effective-sample-size-ess-previously-n_mathrmeff"><span class="math inline">\(\widehat{R}\)</span>, effective sample size (ESS, previously <span class="math inline">\(n_\mathrm{eff}\)</span>)</a></li>
  </ul></li>
  <li><a href="#ch12" id="toc-ch12" class="nav-link" data-scroll-target="#ch12">Chapter 12 Computationally efficient Markov chain simulation</a>
  <ul class="collapse">
  <li><a href="#outline-11" id="toc-outline-11" class="nav-link" data-scroll-target="#outline-11">Outline</a></li>
  <li><a href="#r-and-python-demos-6" id="toc-r-and-python-demos-6" class="nav-link" data-scroll-target="#r-and-python-demos-6">R and Python demos</a></li>
  <li><a href="#mcmc-animations" id="toc-mcmc-animations" class="nav-link" data-scroll-target="#mcmc-animations">MCMC animations</a></li>
  <li><a href="#hamiltonian-monte-carlo" id="toc-hamiltonian-monte-carlo" class="nav-link" data-scroll-target="#hamiltonian-monte-carlo">Hamiltonian Monte Carlo</a></li>
  <li><a href="#divergences-and-bfmi" id="toc-divergences-and-bfmi" class="nav-link" data-scroll-target="#divergences-and-bfmi">Divergences and BFMI</a></li>
  <li><a href="#further-information-about-stan" id="toc-further-information-about-stan" class="nav-link" data-scroll-target="#further-information-about-stan">Further information about Stan</a></li>
  <li><a href="#compiler-and-transpiler" id="toc-compiler-and-transpiler" class="nav-link" data-scroll-target="#compiler-and-transpiler">Compiler and transpiler</a></li>
  </ul></li>
  <li><a href="#chapter-13-modal-and-distributional-approximations" id="toc-chapter-13-modal-and-distributional-approximations" class="nav-link" data-scroll-target="#chapter-13-modal-and-distributional-approximations">Chapter 13 Modal and distributional approximations</a>
  <ul class="collapse">
  <li><a href="#outline-12" id="toc-outline-12" class="nav-link" data-scroll-target="#outline-12">Outline</a></li>
  </ul></li>
  <li><a href="#ch14" id="toc-ch14" class="nav-link" data-scroll-target="#ch14">Chapter 14 Introduction to regression models</a>
  <ul class="collapse">
  <li><a href="#outline-13" id="toc-outline-13" class="nav-link" data-scroll-target="#outline-13">Outline</a></li>
  </ul></li>
  <li><a href="#chapter-15-hierarchical-linear-models" id="toc-chapter-15-hierarchical-linear-models" class="nav-link" data-scroll-target="#chapter-15-hierarchical-linear-models">Chapter 15 Hierarchical linear models</a>
  <ul class="collapse">
  <li><a href="#outline-14" id="toc-outline-14" class="nav-link" data-scroll-target="#outline-14">Outline</a></li>
  </ul></li>
  <li><a href="#chapter-16-generalized-linear-models" id="toc-chapter-16-generalized-linear-models" class="nav-link" data-scroll-target="#chapter-16-generalized-linear-models">Chapter 16 Generalized linear models</a>
  <ul class="collapse">
  <li><a href="#outline-15" id="toc-outline-15" class="nav-link" data-scroll-target="#outline-15">Outline</a></li>
  </ul></li>
  <li><a href="#chapter-17-models-for-robust-inference" id="toc-chapter-17-models-for-robust-inference" class="nav-link" data-scroll-target="#chapter-17-models-for-robust-inference">Chapter 17 Models for robust inference</a>
  <ul class="collapse">
  <li><a href="#outline-16" id="toc-outline-16" class="nav-link" data-scroll-target="#outline-16">Outline</a></li>
  </ul></li>
  <li><a href="#chapter-18-models-for-missing-data" id="toc-chapter-18-models-for-missing-data" class="nav-link" data-scroll-target="#chapter-18-models-for-missing-data">Chapter 18 Models for missing data</a>
  <ul class="collapse">
  <li><a href="#outline-17" id="toc-outline-17" class="nav-link" data-scroll-target="#outline-17">Outline</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Bayesian Data Analysis course - BDA3 notes</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 17, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>These notes help you to focus on the most important parts of each chapter related o the Bayesian Data Analysis course. Before reading a chapter, you can check below which sections, pages, and terms are the most important. After reading the chapter or following the corresponding lecture, you can check here for additional clarifications. There also some notes for the chapters not included in the course.</p>
<section id="ch1" class="level2">
<h2 class="anchored" data-anchor-id="ch1">Chapter 1 Probability and inference</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=13">Chapter 1</a> is related to the pre-requisites and Lecture 1 <em>Introduction</em>.</p>
<section id="outline" class="level3">
<h3 class="anchored" data-anchor-id="outline">Outline</h3>
<ul>
<li>1.1-1.3 important terms, especially 1.3 for the notation</li>
<li>1.4 an example related to the first exercise, and another practical example</li>
<li>1.5 foundations</li>
<li>1.6 good example related to visualization exercise</li>
<li>1.7 example which can be skipped</li>
<li>1.8 background material, good to read before doing the first assignment</li>
<li>1.9 background material, good to read before doing the second assignment</li>
<li>1.10 a point of view for using Bayesian inference</li>
</ul>
</section>
<section id="the-most-important-terms" class="level3">
<h3 class="anchored" data-anchor-id="the-most-important-terms">The most important terms</h3>
<p>Find all the terms and symbols listed below. Note that some of the terms are now only briefly introduced and will be covered later in more detail. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.</p>
<ul>
<li>full probability model</li>
<li>posterior distribution</li>
<li>potentially observable quantity</li>
<li>quantities that are not directly observable</li>
<li>exchangeability</li>
<li>independently and identically distributed</li>
<li><span class="math inline">\(\theta, y, \tilde{y}, x, X, p(\cdot|\cdot), p(\cdot), \operatorname{Pr}(\cdot), \sim, H\)</span></li>
<li>sd, E, var</li>
<li>Bayes rule</li>
<li>prior distribution</li>
<li>sampling distribution, data distribution</li>
<li>joint probability distribution</li>
<li>posterior density</li>
<li>probability</li>
<li>density</li>
<li>distribution</li>
<li><span class="math inline">\(p(y|\theta)\)</span> as a function of <span class="math inline">\(y\)</span> or <span class="math inline">\(\theta\)</span></li>
<li>likelihood</li>
<li>posterior predictive distribution</li>
<li>probability as measure of uncertainty</li>
<li>subjectivity and objectivity</li>
<li>transformation of variables</li>
<li>simulation</li>
<li>inverse cumulative distribution function</li>
</ul>
</section>
<section id="recommended-exercises" class="level3">
<h3 class="anchored" data-anchor-id="recommended-exercises">Recommended exercises</h3>
<p>Optional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:</p>
<ul>
<li><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=37">1.1-1.4, 1.6-1.8</a> (<a href="http://www.stat.columbia.edu/~gelman/book/solutions3.pdf">model solutions available for 1.1-1.6</a>)</li>
</ul>
</section>
<section id="distributed-as-sim" class="level3">
<h3 class="anchored" data-anchor-id="distributed-as-sim">Distributed as, <span class="math inline">\(\sim\)</span></h3>
<p>It is common to write statistical models using a following notation: <span class="math display">\[
\begin{aligned}
y &amp; \sim \mathrm{normal}(\mu, \sigma) \\
\mu &amp; \sim \mathrm{normal}(0, 10) \\
\sigma &amp; \sim \mathrm{normal}^+(0, 1),
\end{aligned}
\]</span> where the symbol <span class="math inline">\(\sim\)</span> is called <em>tilde</em> (<code>\sim</code> in LateX). In general, we can read <span class="math inline">\(\sim\)</span> as <em>“is distributed as,”</em> and overall this notation is used as a shorthand for defining distributions, so that the above example can be written (with also as <span class="math display">\[
\begin{aligned}
   p(y| \mu, \sigma) &amp; = \mathrm{normal}(y |  \mu, \sigma)\\
   p(\mu) &amp; = \mathrm{normal}(\mu |  0, 10)\\
   p(\sigma) &amp; = \mathrm{normal}^+(\sigma |  0, 1).
\end{aligned}
\]</span></p>
<p>A collection of distribution statements define a joint distribution as the product of component distributions <span class="math display">\[
p(y,\mu,\sigma) = p(y| \mu, \sigma )p(\mu) p(\sigma).
\]</span></p>
</section>
<section id="proportional-to-propto" class="level3">
<h3 class="anchored" data-anchor-id="proportional-to-propto">Proportional to, <span class="math inline">\(\propto\)</span></h3>
<p>The symbol <span class="math inline">\(\propto\)</span> means <em>proportional to</em>, which means left hand side is equal to right hand size given a constant multiplier. For instance if <span class="math inline">\(y=2x\)</span>, then <span class="math inline">\(y \propto x\)</span>. It’s <code>\ propto</code> in LaTeX. See <a href="https://en.wikipedia.org/wiki/Proportionality_(mathematics)">Proportionality in Wikipedia</a>.</p>
<p>The Bayes rule is <span class="math display">\[
p(\theta | y) = \frac{p(y | \theta)p(\theta)}{p(y)},
\]</span> where dividing by <span class="math inline">\(p(y)\)</span> makes <span class="math inline">\(\int p(\theta | y) d\theta = 1\)</span>. <span class="math inline">\(p(y)\)</span> is often infeasible to compute, but luckily we also often don’t need to know it, and then we write the Bayes rule as <span class="math display">\[
p(\theta | y) \propto p(y | \theta)p(\theta).
\]</span></p>
</section>
<section id="model-and-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="model-and-likelihood">Model and likelihood</h3>
<p>Term <span class="math inline">\(p(y|\theta,M)\)</span> has two different names depending on the situation. Due to the short notation used, there is possibility of confusion.</p>
<ul>
<li>Term <span class="math inline">\(p(y|\theta,M)\)</span> is called a <em>model</em> (sometimes more specifically <em>observation model</em> or <em>statistical model</em>) when it is used to describe uncertainty about <span class="math inline">\(y\)</span> given <span class="math inline">\(\theta\)</span> and <span class="math inline">\(M\)</span>. Longer notation <span class="math inline">\(p_y(y|\theta,M)\)</span> shows explicitly that it is a function of <span class="math inline">\(y\)</span>.</li>
<li>In Bayes rule, the term <span class="math inline">\(p(y|\theta,M)\)</span> is called <em>likelihood function</em>. Posterior distribution describes the probability (or probability density) for different values of <span class="math inline">\(\theta\)</span> given a fixed <span class="math inline">\(y\)</span>, and thus when the posterior is computed the terms on the right hand side (in Bayes rule) are also evaluated as a function of <span class="math inline">\(\theta\)</span> given fixed <span class="math inline">\(y\)</span>. Longer notation <span class="math inline">\(p_\theta(y|\theta,M)\)</span> shows explicitly that it is a function of <span class="math inline">\(\theta\)</span>. Term has it’s own name (likelihood) to make the difference to the model. The likelihood function is unnormalized probability distribution describing uncertainty related to <span class="math inline">\(\theta\)</span> (and that’s why Bayes rule has the normalization term to get the posterior distribution).</li>
</ul>
</section>
<section id="two-types-of-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="two-types-of-uncertainty">Two types of uncertainty</h3>
<p>Epistemic and aleatory uncertainty are reviewed nicely in the article: <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.1740-9713.2004.00050.x/abstract">Tony O’Hagan, ``Dicing with the unknown’’ Significance 1(3):132-133, 2004.</a></p>
<p>In that paper, there is one typo using the word <em>aleatory</em> instead of <em>epistemic</em> (if you notice this, it’s then quite obvious).</p>
</section>
<section id="transformation-of-variables" class="level3">
<h3 class="anchored" data-anchor-id="transformation-of-variables">Transformation of variables</h3>
<ul>
<li>See <a href="https://users.aalto.fi/~ave/BDA3.pdf#page=31">BDA3 p.&nbsp;21</a></li>
</ul>
</section>
<section id="ambiguous-notation-in-statistics" class="level3">
<h3 class="anchored" data-anchor-id="ambiguous-notation-in-statistics">Ambiguous notation in statistics</h3>
<ul>
<li>In <span class="math inline">\(p(y|\theta)\)</span>
<ul>
<li><span class="math inline">\(y\)</span> can be variable or value
<ul>
<li>we could clarify by using <span class="math inline">\(p(Y|\theta)\)</span> or <span class="math inline">\(p(y|\theta)\)</span></li>
</ul></li>
<li><span class="math inline">\(\theta\)</span> can be variable or value
<ul>
<li>we could clarify by using <span class="math inline">\(p(y|\Theta)\)</span> or <span class="math inline">\(p(y|\theta)\)</span></li>
</ul></li>
<li><span class="math inline">\(p\)</span> can be a discrete or continuous function of <span class="math inline">\(y\)</span> or <span class="math inline">\(\theta\)</span>
<ul>
<li>we could clarify by using <span class="math inline">\(P_Y\)</span>, <span class="math inline">\(P_\Theta\)</span>, <span class="math inline">\(p_Y\)</span> or <span class="math inline">\(p_\Theta\)</span></li>
</ul></li>
<li><span class="math inline">\(P_Y(Y|\Theta=\theta)\)</span> is a probability mass function, sampling distribution, observation model</li>
<li><span class="math inline">\(P(Y=y|\Theta=\theta)\)</span> is a probability</li>
<li><span class="math inline">\(P_\Theta(Y=y|\Theta)\)</span> is a likelihood function (can be discrete or continuous)</li>
<li><span class="math inline">\(p_Y(Y|\Theta=\theta)\)</span> is a probability density function, sampling distribution, observation model</li>
<li><span class="math inline">\(p(Y=y|\Theta=\theta)\)</span> is a density</li>
<li><span class="math inline">\(p_\Theta(Y=y|\Theta)\)</span> is a likelihood function (can be discrete or continuous)</li>
<li><span class="math inline">\(y\)</span> and <span class="math inline">\(\theta\)</span> can also be mix of continuous and discrete</li>
<li>Due to the sloppiness sometimes likelihood is used to refer <span class="math inline">\(P_{Y,\theta}(Y|\Theta)\)</span>, <span class="math inline">\(p_{Y,\theta}(Y|\Theta)\)</span></li>
</ul></li>
</ul>
</section>
<section id="exchangeability" class="level3">
<h3 class="anchored" data-anchor-id="exchangeability">Exchangeability</h3>
<p>You don’t need to understand or use the term exchangeability before Chapter 5 and Lecture 7. At this point and until Chapter 5 and Lecture 7, it is sufficient that you know that 1) independence is stronger condition than exchangeability, 2) independence implies exchangeability, 3) exchangeability does not imply independence, 4) exchangeability is related to what information is available instead of the properties of unknown underlying data generating mechanism. If you want to know more about exchangeability right now, then read BDA Section 5.2 and <a href="#ch5">notes for Chapter 5</a>.</p>
</section>
<section id="number-of-digits" class="level3">
<h3 class="anchored" data-anchor-id="number-of-digits">Number of digits</h3>
<p>In the early assignment rounds we ask to answer numerical answers with a fixed number of decimal digits. Later in the course, we will discuss how to choose the number of significant digits based on the posterior uncertainty and Monte Carlo standard error.</p>
<ul>
<li>For example, 0.01 has two decimal digits and two significant digits</li>
<li>For example, 0.0099 has four decimal digits and two significant digits</li>
<li>See more about <a href="https://en.wikipedia.org/wiki/Significant_figures">significant digits in Wikipedia</a></li>
<li>In early assignment rounds, we have intentionally asked specified number of decimal digits</li>
</ul>
</section>
</section>
<section id="ch2" class="level2">
<h2 class="anchored" data-anchor-id="ch2">Chapter 2 Single-parameter models</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=39">Chapter 2</a> is related to the prerequisites and Lecture 2 <em>Basics of Bayesian inference</em>.</p>
<section id="outline-1" class="level3">
<h3 class="anchored" data-anchor-id="outline-1">Outline</h3>
<ul>
<li>2.1 Binomial model (e.g.&nbsp;biased coin flipping)</li>
<li>2.2 Posterior as compromise between data and prior information</li>
<li>2.3 Posterior summaries</li>
<li>2.4 Informative prior distributions (skip exponential families and sufficient statistics)</li>
<li>2.5 Gaussian model with known variance</li>
<li>2.6 Other single parameter models
<ul>
<li>in this course the normal distribution with known mean but unknown variance is the most important</li>
<li>glance through Poisson and exponential</li>
</ul></li>
<li>2.7 glance through this example, which illustrates benefits of prior information, no need to read all the details (it’s quite long example)</li>
<li>2.8 Noninformative priors</li>
<li>2.9 Weakly informative priors</li>
</ul>
<p>Laplace’s approach for approximating integrals is discussed in more detail in Chapter 4.</p>
</section>
<section id="the-most-important-terms-1" class="level3">
<h3 class="anchored" data-anchor-id="the-most-important-terms-1">The most important terms</h3>
<p>Find all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.</p>
<ul>
<li>binomial model</li>
<li>Bernoulli trial</li>
<li><span class="math inline">\(\mathop{\mathrm{Bin}}\)</span>, <span class="math inline">\(\binom{n}{y}\)</span></li>
<li>Laplace’s law of succession</li>
<li>think which expectations in eqs. 2.7-2.8</li>
<li>summarizing posterior inference</li>
<li>mode, mean, median, standard deviation, variance, quantile</li>
<li>central posterior interval</li>
<li>highest posterior density interval / region</li>
<li>uninformative / informative prior distribution</li>
<li>principle of insufficient reason</li>
<li>hyperparameter</li>
<li>conjugacy, conjugate family, conjugate prior distribution, natural conjugate prior</li>
<li>nonconjugate prior</li>
<li>normal distribution</li>
<li>conjugate prior for mean of normal distribution with known variance</li>
<li>posterior for mean of normal distribution with known variance</li>
<li>precision</li>
<li>posterior predictive distribution</li>
<li>normal model with known mean but unknown variance</li>
<li>proper and improper prior</li>
<li>unnormalized density</li>
<li>difficulties with noninformative priors</li>
<li>weakly informative priors</li>
</ul>
</section>
<section id="r-and-python-demos" class="level3">
<h3 class="anchored" data-anchor-id="r-and-python-demos">R and Python demos</h3>
<ul>
<li>2.1: Binomial model and Beta posterior. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch2/demo2_1.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch2/demo2_1.ipynb">Python</a>.</li>
<li>2.2: Comparison of posterior distributions with different parameter values for the Beta prior distribution. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch2/demo2_2.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch2/demo2_2.ipynb">Python</a>.</li>
<li>2.3: Use posterior draws to plot histogram with quantiles, and the same for a transformed variable. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch2/demo2_3.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch2/demo2_3.ipynb">Python</a>.</li>
<li>2.4: Grid sampling using inverse-cdf method. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch2/demo2_4.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch2/demo2_4.ipynb">Python</a>.</li>
</ul>
</section>
<section id="recommended-exercises-1" class="level3">
<h3 class="anchored" data-anchor-id="recommended-exercises-1">Recommended exercises</h3>
<p>Optional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:</p>
<ul>
<li><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=67">2.1-2.5, 2.8, 2.9, 2.14, 2.17, 2.22</a> (<a href="http://www.stat.columbia.edu/~gelman/book/solutions3.pdf">model solutions available for 2.1-2.5, 2.7-2.13, 2.16, 2.17, 2.20</a>, and 2.14 is in course slides)</li>
</ul>
</section>
<section id="posterior-draws-and-sample" class="level3">
<h3 class="anchored" data-anchor-id="posterior-draws-and-sample">Posterior draws and sample</h3>
<p>“Posterior draws” are sometimes also called “posterior samples”. As sample can mean also data, to improve clarity Stan developers prefer following: Individual is draw, plural is draws, a set of draws is sample, and many sets of draws are samples. The number of draws is the same as the sample size, and later we discuss also effective sample size (ESS) for a set of posterior draws.</p>
</section>
<section id="posterior-credible-and-confidence-intervals" class="level3">
<h3 class="anchored" data-anchor-id="posterior-credible-and-confidence-intervals">Posterior, credible, and confidence intervals</h3>
<ul>
<li><p><em>Confidence interval</em> is used in frequentist statistics and not used in Bayesian statistics, but we mention it here to make that fact explicit. Given a confidence level $ (95% and 99% are typical values), a <em>confidence interval</em> is a random interval which contains the parameter being estimated <span class="math inline">\(\gamma\)</span>% of the time (<a href="https://en.wikipedia.org/wiki/Confidence_interval">Wikipedia</a>).</p></li>
<li><p><em>Credible interval</em> is used in Bayesian statistics (and by choice the acronym is CI as for the confidence interval). <em>Credible interval</em> is defined such that an unobserved parameter value has a particular probability <span class="math inline">\(\alpha\)</span> to fall within it (<a href="https://en.wikipedia.org/wiki/Credible_interval">Wikipedia</a>). <em>Credible interval</em> can be used to characterize any probability distribution.</p></li>
<li><p><em>Posterior interval</em> is a credible interval specifically characterizing posterior distribution.</p></li>
</ul>
</section>
<section id="integration-over-beta-distribution" class="level3">
<h3 class="anchored" data-anchor-id="integration-over-beta-distribution">Integration over Beta distribution</h3>
<p>Chapter 2 has an example of analysing the ratio of girls born in Paris 1745–1770. Laplace used binomial model and uniform prior which produces Beta distribution as posterior distribution. Laplace wanted to calculate <span class="math inline">\(p(\theta \geq 0.5)\)</span>, which is obtained as <span class="math display">\[
\begin{aligned}
  p(\theta \geq 0.5) &amp;=&amp; \int_{0.5}^1  p(\mathbf{\theta}|y,n,M) d\theta \\
  &amp;=&amp; \frac{493473!}{241945!251527!} \int_{0.5}^1 \theta^y(1-\theta)^{n-y} d\theta\end{aligned}
\]</span> Note that <span class="math inline">\(\Gamma(n)=(n-1)!\)</span>. Integral has a form which is called <em>incomplete Beta function</em>. Bayes and Laplace had difficulties in computing this, but nowadays there are several series and continued fraction expressions. Furthermore usually the normalization term is computed by computing <span class="math inline">\(\log(\Gamma(\cdot))\)</span> directly without explicitly computing <span class="math inline">\(\Gamma(\cdot)\)</span>. Bayes was able to solve integral given small <span class="math inline">\(n\)</span> and <span class="math inline">\(y\)</span>. In case of large <span class="math inline">\(n\)</span> and <span class="math inline">\(y\)</span>, Laplace used Gaussian approximation of the posterior (more in Chapter 4). In this specific case, R <code>pbeta</code> gives the same results as Laplace’s result with at least 3 digit accuracy.</p>
</section>
<section id="numerical-accuracy" class="level3">
<h3 class="anchored" data-anchor-id="numerical-accuracy">Numerical accuracy</h3>
<p>Laplace calculated <span class="math display">\[
p(\theta \geq 0.5 | y, n, M) \approx 1.15 \times 10^{-42}.
\]</span> Correspondingly Laplace could have calculated <span class="math display">\[
p(\theta \geq 0.5 | y, n, M) = 1 - p(\theta \leq 0.5 | y, n, M),
\]</span> which in theory could be computed in R with <code>1-pbeta(0.5,y+1,n-y+1)</code>. In practice this fails, due to the limitation in the floating point representation used by the computers. In R the largest floating point number which is smaller than 1 is about 1-eps/4, where eps is about <span class="math inline">\(2.22 \times 10^{-16}\)</span> (the smallest floating point number larger than 1 is 1+eps). Thus the result from <code>pbeta(0.5,y+1,n-y+1)</code> will be rounded to 1 and <span class="math inline">\(1-1=0\neq 1.15
\times 10^{-42}\)</span>. We can compute <span class="math inline">\(p(\theta \geq 0.5 | y, n, M)\)</span> in R with <code>pbeta(0.5, y+1, n-y+1, lower.tail=FALSE)</code>.</p>
</section>
<section id="highest-posterior-density-interval" class="level3">
<h3 class="anchored" data-anchor-id="highest-posterior-density-interval">Highest Posterior Density interval</h3>
<p>HPD interval is not invariant to reparametrization. Here’s an illustrative example (using R and package <code>HDInterval</code>):</p>
<pre><code>&gt; r &lt;- exp(rnorm(1000))
&gt; quantile(log(r),c(.05, .95))
       5%       95% 
-1.532931  1.655137 
&gt; log(quantile(r,c(.05, .95)))
       5%       95% 
-1.532925  1.655139 
&gt; hdi(log(r), credMass = 0.9)
    lower     upper 
-1.449125  1.739169 
attr(,"credMass")
[1] 0.9
&gt; log(hdi(r, credMass = 0.9))
    lower     upper 
-2.607574  1.318569 
attr(,"credMass")
[1] 0.9</code></pre>
</section>
<section id="gaussian-distribution-in-more-complex-models-and-methods" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-distribution-in-more-complex-models-and-methods">Gaussian distribution in more complex models and methods</h3>
<p>Gaussian distribution is commonly used in mixture models, hierarchical models, hierarchical prior structures, scale mixture distributions, Gaussian latent variable models, Gaussian processes, Gaussian random Markov fields, Kalman filters, proposal distribution in Monte Carlo methods, etc.</p>
</section>
<section id="predictive-distribution" class="level3">
<h3 class="anchored" data-anchor-id="predictive-distribution">Predictive distribution</h3>
<p>Often the predictive distribution is more interesting than the posterior distribution. The posterior distribution describes the uncertainty in the parameters (like the proportion of red chips in the bag), but the predictive distribution describes also the uncertainty about the future event (like which color is picked next). This difference is important, for example, if we want to what could happen if some treatment is given to a patient.</p>
<p>In case of Gaussian distribution with known variance <span class="math inline">\(\sigma^2\)</span> the model is <span class="math display">\[
\begin{aligned}
  y\sim \operatorname{N}(\theta,\sigma^2),
\end{aligned}
\]</span> where <span class="math inline">\(\sigma^2\)</span> describes aleatoric uncertainty. Using uniform prior the posterior is <span class="math display">\[
\begin{aligned}
  p(\theta|y) \sim \mathop{\mathrm{N}}(\theta|\bar{y},\sigma^2/n),
\end{aligned}
\]</span> where <span class="math inline">\(\sigma^2/n\)</span> described epistemic uncertainty related to <span class="math inline">\(\theta\)</span>. Using uniform prior the posterior predictive distribution for new <span class="math inline">\(\tilde{y}\)</span> is <span class="math display">\[
\begin{aligned}
  p(\tilde{y}|y) \sim \operatorname{N}(\tilde{y}|\bar{y},\sigma^2+\sigma^2/n),
\end{aligned}
\]</span> where the uncertainty is sum of epistemic (<span class="math inline">\(\sigma^2/n\)</span>) and aleatoric uncertainty (<span class="math inline">\(\sigma^2\)</span>).</p>
</section>
<section id="non-informative-and-weakly-informative-priors" class="level3">
<h3 class="anchored" data-anchor-id="non-informative-and-weakly-informative-priors">Non-informative and weakly informative priors</h3>
<p>Our thinking has advanced since sections 2.8 and 2.9 were written. We’re even more strongly in favor weakly informative priors, and in favor of more information in the priors. Non-informative priors are likely to produce more unstable estimates (higher variance), and the lectures include also examples of how seemingly non-informative priors can actually be informative on some aspect. See further discussion and example in the <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Prior Choice Recommendations Wiki</a>. Thus Prior Choice Recommendations Wiki will see also some further updates (we’re doing research and learning more all the time).</p>
</section>
<section id="should-we-worry-about-rigged-priors" class="level3">
<h3 class="anchored" data-anchor-id="should-we-worry-about-rigged-priors">Should we worry about rigged priors?</h3>
<p><a href="http://andrewgelman.com/2017/10/04/worry-rigged-priors/">Andrew Gelman’s blog post answering worries that data analyst would choose a too optimistic prior</a>.</p>
</section>
<section id="prior-knowledge-elicitation" class="level3">
<h3 class="anchored" data-anchor-id="prior-knowledge-elicitation">Prior knowledge elicitation</h3>
<p>Prior elicitation transforms domain knowledge of various kinds into well-defined prior distributions. There are challenges in how to gather domain knowledge and hopw to transform that to mathematical form. We come back to the topic later in the course. A recent review <a href="https://doi.org/10.1214/23-BA1381"><em>“Prior Knowledge Elicitation: The Past, Present, and Future”</em> by Mikkola et al.&nbsp;(2023)</a> provides more information for those interested to go beoyond the material in thos course.</p>
</section>
<section id="exchangeability-1" class="level3">
<h3 class="anchored" data-anchor-id="exchangeability-1">Exchangeability</h3>
<p>You don’t need to understand or use the term exchangeability before Chapter 5 and Lecture 7. At this point and until Chapter 5 and Lecture 7, it is sufficient that you know that 1) independence is stronger condition than exchangeability, 2) independence implies exchangeability, 3) exchangeability does not imply independence, 4) exchangeability is related to what information is available instead of the properties of unknown underlying data generating mechanism. If you want to know more about exchangeability right now, then read BDA3 Section 5.2 and <a href="#ch5">notes for Chapter 5</a>.</p>
</section>
<section id="the-number-of-left-handed-students-in-the-class" class="level3">
<h3 class="anchored" data-anchor-id="the-number-of-left-handed-students-in-the-class">The number of left-handed students in the class</h3>
<ul>
<li>What we know and don’t know
<ul>
<li><span class="math inline">\(N=L+R\)</span> is the total number of students in the lecture hall, <span class="math inline">\(N\)</span> is known in the beginning</li>
<li><span class="math inline">\(L\)</span> and <span class="math inline">\(R\)</span> are the number of left and right handed students, not known before we start asking</li>
<li><span class="math inline">\(n=l+r\)</span> is the total number of students we have asked</li>
<li><span class="math inline">\(l\)</span> and <span class="math inline">\(r\)</span> are the numbers of left and right handed students from the students we asked</li>
<li>we also know that <span class="math inline">\(l \leq L \leq (N-r)\)</span> and <span class="math inline">\(r \leq R \leq (N-l)\)</span></li>
</ul></li>
<li>After observing <span class="math inline">\(n\)</span> students with <span class="math inline">\(l\)</span> left handed, what we know about <span class="math inline">\(L\)</span>?
<ul>
<li>We define <span class="math inline">\(L=l+\tilde{l}\)</span>, where <span class="math inline">\(\tilde{l}\)</span> is the unobserved number of left handed students among those who we did not yet ask</li>
<li>Posterior distribution for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\operatorname{Beta}(\alpha+l, \beta+r)\)</span></li>
<li>Posterior predictive distribution for <span class="math inline">\(\tilde{l}\)</span> is <span class="math inline">\(\operatorname{Beta-Binomial}(\tilde{l} | N-n, \alpha+l, \beta+r)=\int_0^1\operatorname{Bin}(\tilde{l} | N-n, \theta)\operatorname{Beta}(\theta | \alpha+l, \beta+r)d\theta\)</span></li>
</ul></li>
<li>Eventually as we have asked everyone, <span class="math inline">\(n=N\)</span>, and there is no uncertainty on the number of left-handed students present, and <span class="math inline">\(l=L\)</span> and <span class="math inline">\(\tilde{l}=0\)</span>. There is still uncertainty about <span class="math inline">\(\theta\)</span>, but that is relevant only if we would like to make predictions beyond the students in the lecture hall.</li>
</ul>
</section>
</section>
<section id="ch3" class="level2">
<h2 class="anchored" data-anchor-id="ch3">Chapter 3 Introduction to multiparameter models</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=73">Chapter 3</a> is related to the Lecture 3 <em>Multidimensional posterior</em>.</p>
<section id="outline-2" class="level3">
<h3 class="anchored" data-anchor-id="outline-2">Outline</h3>
<ul>
<li>3.1 Marginalisation</li>
<li>3.2 Normal distribution with a noninformative prior (very important)</li>
<li>3.3 Normal distribution with a conjugate prior (very important)</li>
<li>3.4 Multinomial model (can be skipped)</li>
<li>3.5 Multivariate normal with known variance (needed later)</li>
<li>3.6 Multivariate normal with unknown variance (glance through)</li>
<li>3.7 Bioassay example (very important, related to one of the exercises)</li>
<li>3.8 Summary (summary)</li>
</ul>
<p>Normal model is used a lot as a building block of the models in the later chapters, so it is important to learn it now. Bioassay example is good example used to illustrate many important concepts and it is used in several exercises over the course.</p>
</section>
<section id="the-most-important-terms-2" class="level3">
<h3 class="anchored" data-anchor-id="the-most-important-terms-2">The most important terms</h3>
<p>Find all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.</p>
<ul>
<li>marginal distribution/density</li>
<li>conditional distribution/density</li>
<li>joint distribution/density</li>
<li>nuisance parameter</li>
<li>mixture</li>
<li>normal distribution with a noninformative prior</li>
<li>normal distribution with a conjugate prior</li>
<li>sample variance</li>
<li>sufficient statistics</li>
<li><span class="math inline">\(\mu\)</span>, <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(\bar{y}\)</span>, <span class="math inline">\(s^2\)</span></li>
<li>a simple normal integral</li>
<li><span class="math inline">\(\operatorname{Inv-\chi^2}\)</span></li>
<li>factored density</li>
<li><span class="math inline">\(t_{n-1}\)</span></li>
<li>degrees of freedom</li>
<li>posterior predictive distribution</li>
<li><span class="math inline">\(\operatorname{N-Inv-\chi^2}\)</span></li>
<li>variance matrix <span class="math inline">\(\Sigma\)</span></li>
<li>nonconjugate model</li>
<li>generalized linear model</li>
<li>binomial model</li>
<li>logistic transformation</li>
<li>density at a grid</li>
</ul>
</section>
<section id="r-and-python-demos-1" class="level3">
<h3 class="anchored" data-anchor-id="r-and-python-demos-1">R and Python demos</h3>
<ul>
<li>3.1: Visualize joint density and marginal densities of posterior of normal distribution with unknown mean and variance. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch3/demo3_1.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch3/demo3_1.ipynb">Python</a>.</li>
<li>3.2: Visualize factored sampling and corresponding marginal and conditional density. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch3/demo3_2.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch3/demo3_2.ipynb">Python</a>.</li>
<li>3.3: Visualize marginal distribution of <span class="math inline">\(\mu\)</span> as a mixture of normals. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch3/demo3_3.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch3/demo3_3.ipynb">Python</a>.</li>
<li>3.4: Visualize sampling from the posterior predictive distribution. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch3/demo3_4.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch3/demo3_4.ipynb">Python</a>.</li>
<li>3.5: Visualize Newcomb’s data. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch3/demo3_5.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch3/demo3_5.ipynb">Python</a>.</li>
<li>3.6: Visualize posterior in bioassay example. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch3/demo3_6.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch3/demo3_6.ipynb">Python</a>.</li>
</ul>
</section>
<section id="recommended-exercises-2" class="level3">
<h3 class="anchored" data-anchor-id="recommended-exercises-2">Recommended exercises</h3>
<p>Optional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:</p>
<ul>
<li><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=89">3.2, 3.3, 3.9</a> (model solutions available for 3.1-3.3, 3.5, 3.9, 3.10)</li>
</ul>
</section>
<section id="conjugate-prior-for-normal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="conjugate-prior-for-normal-distribution">Conjugate prior for normal distribution</h3>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=77">BDA3 p.&nbsp;67</a> mentions that the conjugate prior for normal distribution has to have a product form <span class="math inline">\(p(\sigma^2)p(\mu|\sigma^2)\)</span>. The book refers to (3.2) and the following discussion. As additional hint is useful to think the relation of terms <span class="math inline">\((n-1)s^2\)</span> and <span class="math inline">\(n(\bar{y}-\mu)^2\)</span> in 3.2 to equations 3.3 and 3.4.</p>
</section>
<section id="trace-of-square-matrix" class="level3">
<h3 class="anchored" data-anchor-id="trace-of-square-matrix">Trace of square matrix</h3>
<p>Trace of square matrix, <span class="math inline">\(\operatorname{trace}\)</span>, <span class="math inline">\(\operatorname{tr}A\)</span>, <span class="math inline">\(\operatorname{trace}(A)\)</span>, <span class="math inline">\(\operatorname{tr}(A)\)</span>, is the sum of diagonal elements. To derive equation 3.11 the following property has been used <span class="math inline">\(\operatorname{tr}(ABC) = \operatorname{tr}(CAB) = \operatorname{tr}(BCA)\)</span>.</p>
</section>
<section id="history-and-naming-of-distributions" class="level3">
<h3 class="anchored" data-anchor-id="history-and-naming-of-distributions">History and naming of distributions</h3>
<p>See <a href="http://jeff560.tripod.com/mathword.html">Earliest Known Uses of Some of the Words of Mathematics</a>.</p>
</section>
<section id="using-monte-carlo-to-obtain-draws-from-the-posterior-of-generated-quantities" class="level3">
<h3 class="anchored" data-anchor-id="using-monte-carlo-to-obtain-draws-from-the-posterior-of-generated-quantities">Using Monte Carlo to obtain draws from the posterior of generated quantities</h3>
<p>Chapter 3 discusses closed form posteriors for binomial and normal models given conjugate priors. These are also used as part of the assignment. The assignment also requires forming a posterior for derived quantities, and these posterior don’t have closed form (so no need to try derive them). As we know how to sample from the posterior of binomial and normal models, we can use these posterior draws to get draws from the posterior of derived quantity.</p>
<p>For example, given posteriors <span class="math inline">\(p(\theta_1|y_1)\)</span> and <span class="math inline">\(p(\theta_2|y_2)\)</span> we want to find the posterior for the difference <span class="math inline">\(p(\theta_1-\theta_2|y_1,y_2)\)</span>.</p>
<ol type="1">
<li>Sample <span class="math inline">\(\theta_1^s\)</span> from <span class="math inline">\(p(\theta_1|y_1)\)</span> and <span class="math inline">\(\theta_2^s\)</span> from <span class="math inline">\(p(\theta_2|y_2)\)</span>, we can compute posterior draws for the derived quantity as <span class="math inline">\(\delta^s=\theta_1^s-\theta_2^s\)</span> (<span class="math inline">\(s=1,\ldots,S\)</span>).</li>
<li><span class="math inline">\(\delta^s\)</span> are then draws from <span class="math inline">\(p(\delta^s|y_1,y_2)\)</span>, and they can be used to illustrate the posterior <span class="math inline">\(p(\delta^s|y_1,y_2)\)</span> with histogram, and compute posterior mean, sd, and quantiles.</li>
</ol>
<p>This is one reason why Monte Carlo approaches are so commonly used.</p>
</section>
<section id="the-number-of-required-monte-carlo-draws" class="level3">
<h3 class="anchored" data-anchor-id="the-number-of-required-monte-carlo-draws">The number of required Monte Carlo draws</h3>
<p>This will discussed in Lecture 4 and Chapter 10. Meanwhile, e.g., 1000 draws is sufficient.</p>
</section>
<section id="bioassay" class="level3">
<h3 class="anchored" data-anchor-id="bioassay">Bioassay</h3>
<p>Bioassay example is is an example of very common statistical inference task typical, for example, medicine, pharmacology, health care, cognitive science, genomics, industrial processes etc.</p>
<p>The example is from Racine et al (1986) (see ref in the end of the BDA3). Swiss company makes classification of chemicals to different toxicity categories defined by authorities (like EU). Toxicity classification is based on lethal dose 50% (LD50) which tells what amount of chemical kills 50% of the subjects. Smaller the LD50 more lethal the chemical is. The original paper mentions "1983 Swiss poison Regulation" which defines following categories for chemicals orally given to rats (mg/ml)<br>
</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">Class</th>
<th style="text-align: center;">LD50</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: center;">&lt;5</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: center;">5-50</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: center;">50-500</td>
</tr>
<tr class="even">
<td style="text-align: center;">4</td>
<td style="text-align: center;">500-2000</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: center;">2000-5000</td>
</tr>
</tbody>
</table>
<p>To reduce the number of rats needed in the experiments, the company started to use Bayesian methods. The paper mentions that in those days use of just 20 rats to define the classification was very little. Book gives LD50 in log(g/ml). When the result from 3.6 is transformed to scale mg/ml, we see that the mean LD50 is about 900 and <span class="math inline">\(p(500&lt;\text{LD50}&lt;2000)\approx 0.99\)</span>. Thus, the tested chemical can be classified as category 4 toxic.</p>
<p>Note that the chemical testing is moving away from using rats and other animals to using, for example, human cells grown in chips, tissue models and human blood cells. The human-cell based approaches are also more accurate to predict the effect for humans.</p>
<p><span class="math inline">\(\operatorname{logit}\)</span> transformation can be justified information theoretically when binomial likelihood is used.</p>
<p>Code in demo 3.6 (<a href="https://avehtari.github.io/BDA_R_demos/demos_ch3/demo3_6.html">R</a>, <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch3/demo3_6.ipynb">Python</a>) can be helpful in exercises related to Bioassay example.</p>
</section>
<section id="bayesian-vs.-frequentist-statements-in-two-group-comparisons" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-vs.-frequentist-statements-in-two-group-comparisons">Bayesian vs.&nbsp;frequentist statements in two group comparisons</h3>
<p>When asking to compare groups, some students get confused as the frequentist testing is quite different. The frequentist testing is often focusing on a) differently named tests for different models and b) null hypothesis testing. In Bayesian inference a) the same Bayes rule and investigation of posterior is used for all models, b) null hypothesis testing is less common. We come later to decision making given posterior and utility/ cost function (Lecture 10.1) and more about null hypothesis testing (Lecture 12.1). Now it is assumed you will report the posterior (e.g.&nbsp;histogram), possible summaries, and report what you can infer from that. Specifically as in this assignment the group comparisons are based on continuous model parameter, the probability of 0 difference is 0 (later lecture 12.1 covers null hypothesis testing). Instead of forcing dichotomous answer (yes/no) about whether there is difference, report the whole posterior that tells also how big that difference might be. What big means depends on the application, which brings us back to the fact of importance of domain expertise. You are not experts on the application examples used in the assignment, but you can think how would you report what you have learned to a domain expert.</p>
<p><a href="https://hbiostat.org/blog/post/bayes-freq-stmts/">Frank Harrell’s recommendations how to state results in two group comparisons</a> are excellent.</p>
</section>
<section id="unimodal-and-multimodal" class="level3">
<h3 class="anchored" data-anchor-id="unimodal-and-multimodal">Unimodal and multimodal</h3>
<p>From <a href="https://en.wikipedia.org/wiki/Unimodality">Wikipedia Unimodality</a>:</p>
<ul>
<li>In statistics, a unimodal probability distribution or unimodal distribution is a probability distribution which has a single peak. The term “mode” in this context refers to any peak of the distribution.</li>
<li>If it has more modes it is “bimodal” (2), “trimodal” (3), etc., or in general, “multimodal”.</li>
</ul>
<p>BDA3 Section 2.3 discusses posterior summaries and illustrates bimodal distribution in Figure 2.2.</p>
</section>
</section>
<section id="ch4" class="level2">
<h2 class="anchored" data-anchor-id="ch4">Chapter 4 Asymptotics and connections to non-Bayesian approaches</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=93">Chapter 4</a> is related to the Lecture 11 <em>Normal approximation, frequency properties</em>.</p>
<section id="outline-3" class="level3">
<h3 class="anchored" data-anchor-id="outline-3">Outline</h3>
<ul>
<li>4.1 Normal approximation (Laplace’s method)</li>
<li>4.2 Large-sample theory</li>
<li>4.3 Counter examples</li>
<li>4.4 Frequency evaluation (not part of the course, but interesting)</li>
<li>4.5 Other statistical methods (not part of the course, but interesting)</li>
</ul>
<p>Normal approximation is used often used as part of posterior computation (more about this in <a href="#ch13">Chapter 13</a>, which is not a part of the course).</p>
</section>
<section id="the-most-important-terms-3" class="level3">
<h3 class="anchored" data-anchor-id="the-most-important-terms-3">The most important terms</h3>
<p>Find all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.</p>
<ul>
<li>sample size</li>
<li>asymptotic theory</li>
<li>normal approximation</li>
<li>quadratic function</li>
<li>Taylor series expansion</li>
<li>observed information</li>
<li>positive definite</li>
<li>why <span class="math inline">\(\log \sigma\)</span>?</li>
<li>Jacobian of the transformation</li>
<li>point estimates and standard errors- large-sample theory</li>
<li>asymptotic normality</li>
<li>consistency</li>
<li>underidentified</li>
<li>nonidentified</li>
<li>number of parameters increasing with sample size</li>
<li>aliasing</li>
<li>unbounded likelihood</li>
<li>improper posterior</li>
<li>edge of parameter space</li>
<li>tails of distribution</li>
</ul>
</section>
<section id="r-and-python-demos-2" class="level3">
<h3 class="anchored" data-anchor-id="r-and-python-demos-2">R and Python demos</h3>
<ul>
<li>4.1: Bioassay example <a href="https://avehtari.github.io/BDA_R_demos/demos_ch4/demo4_1.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch4/demo4_1.ipynb">Python</a>.</li>
</ul>
</section>
<section id="normal-approximation" class="level3">
<h3 class="anchored" data-anchor-id="normal-approximation">Normal approximation</h3>
<p>Other normal posterior approximations are discussed in <a href="ch13">Chapter 13</a>. For example, variational and expectation propagation methods improve the approximation by global fitting instead of just the curvature at the mode. The normal approximation at the mode is often also called the Laplace method, as Laplace used it first.</p>
<p>Several researchers have provided partial proofs that posterior converges towards normal distribution. In the mid 20th century Le Cam was first to provide a strict proof.</p>
</section>
<section id="observed-information" class="level3">
<h3 class="anchored" data-anchor-id="observed-information">Observed information</h3>
<p>When <span class="math inline">\(n\rightarrow\infty\)</span>, the posterior distribution approaches normal distribution. As the log density of the normal is a quadratic function, the higher derivatives of the log posterior approach zero. The curvature at the mode describes the information only in the case if asymptotic normality. In the case of the normal distribution, the curvature describes also the width of the normal. Information matrix is a <em>precision matrix</em>, which is inverse of a covariance matrix.</p>
</section>
<section id="aliasing" class="level3">
<h3 class="anchored" data-anchor-id="aliasing">Aliasing</h3>
<p>In Finnish: valetoisto.</p>
<p>Aliasing is a special case of under-identifiability, where likelihood repeats in separate points of the parameter space. That is, likelihood will get exactly same values and has same shape although possibly mirrored or otherwise projected. For example, the following mixture model <span class="math display">\[
  p(y_i|\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\lambda)=\lambda\operatorname{N}(\mu_1,\sigma_1^2)+(1-\lambda)\operatorname{N}(\mu_2,\sigma_2^2),
\]</span> has two normals with own means and variances. With a probability <span class="math inline">\(\lambda\)</span> the observation comes from <span class="math inline">\(\operatorname{N}(\mu_1,\sigma_1^2)\)</span> and a probability <span class="math inline">\(1-\lambda\)</span> from <span class="math inline">\(\operatorname{N}(\mu_2,\sigma_2^2)\)</span>. This kind of model could be used, for example, for the Newcomb’s data, so that the another normal component would model faulty measurements. Model does not state which of the components 1 or 2, would model good measurements and which would model the faulty measurements. Thus it is possible to interchange values of <span class="math inline">\((\mu_1,\mu_2)\)</span> and <span class="math inline">\((\sigma_1^2,\sigma_2^2)\)</span> and replace <span class="math inline">\(\lambda\)</span> with <span class="math inline">\((1-\lambda)\)</span> to get the equivalent model. Posterior distribution then has two modes which are mirror images of each other. When <span class="math inline">\(n\rightarrow\infty\)</span> modes will get narrower, but the posterior does not converge to a single point.</p>
<p>If we can integrate over the whole posterior, the aliasing is not a problem. However aliasing makes the approximative inference more difficult.</p>
</section>
<section id="frequency-property-vs.-frequentist" class="level3">
<h3 class="anchored" data-anchor-id="frequency-property-vs.-frequentist">Frequency property vs.&nbsp;frequentist</h3>
<p>Bayesians can evaluate frequency properties of Bayesian estimates without being frequentist. For Bayesians the starting point is the Bayes rule and decision theory. Bayesians care more about efficiency than unbiasedness. For frequentists the starting point is to find an estimator with desired frequency properties and quite often unbiasedness is chosen as the first restriction.</p>
</section>
<section id="transformation-of-variables-1" class="level3">
<h3 class="anchored" data-anchor-id="transformation-of-variables-1">Transformation of variables</h3>
<p>See <a href="https://users.aalto.fi/~ave/BDA3.pdf#page=31">BDA3 p.&nbsp;21</a> for the explanation how to derive densities for transformed variables. This explains, for example, why uniform prior <span class="math inline">\(p(\log(\sigma^2))\propto 1\)</span> for <span class="math inline">\(\log(\sigma^2)\)</span> corresponds to prior <span class="math inline">\(p(\sigma^2)=\sigma^{-2}\)</span> for <span class="math inline">\(\sigma^{2}\)</span>.</p>
</section>
<section id="on-derivation" class="level3">
<h3 class="anchored" data-anchor-id="on-derivation">On derivation</h3>
<p>Here’s a reminder how to integrate with respect to <span class="math inline">\(g(\theta)\)</span>. For example <span class="math display">\[
  \frac{d}{d\log\sigma}\sigma^{-2}=-2 \sigma^{-2}
\]</span> is easily solved by setting <span class="math inline">\(z=\log\sigma\)</span> to get <span class="math display">\[
  \frac{d}{dz}\exp(z)^{-2}=-2\exp(z)^{-3}\exp(z)=-2\exp(z)^{-2}=-2\sigma^{-2}.
\]</span></p>
</section>
</section>
<section id="ch5" class="level2">
<h2 class="anchored" data-anchor-id="ch5">Chapter 5 Hierarchical models</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=111">Chapter 5</a> is related to the Lecture 7 <em>Hierarchical models and exchangeability</em>.</p>
<section id="outline-4" class="level3">
<h3 class="anchored" data-anchor-id="outline-4">Outline</h3>
<ul>
<li>5.1 Lead-in to hierarchical models</li>
<li>5.2 Exchangeability (a useful theoretical concept)</li>
<li>5.3 Bayesian analysis of hierarchical models (discusses factorized computation which can be skipped)</li>
<li>5.4 Hierarchical normal model (discusses factorized computation which can be skipped)</li>
<li>5.5 Example: parallel experiments in eight schools (useful dicussion, skip the details of computation)</li>
<li>5.6 Meta-analysis (can be skipped in this course)</li>
<li>5.7 Weakly informative priors for hierarchical variance parameters (more recent discussion can be found in <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Prior Choice Recommendation Wiki</a>)</li>
</ul>
<p>The hierarchical models in the chapter are simple to keep computation simple. More advanced computational tools are presented in Chapters <a href="ch10">10</a>, <a href="ch11">11</a> and <a href="ch12">12</a> (part of the course), and <a href="ch13">13</a> (not part of the course).</p>
</section>
<section id="the-most-important-terms-4" class="level3">
<h3 class="anchored" data-anchor-id="the-most-important-terms-4">The most important terms</h3>
<p>Find all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.</p>
<ul>
<li>population distribution</li>
<li>hyperparameter</li>
<li>hierarchical model</li>
<li>exchangeability</li>
<li>invariant to permutations</li>
<li>independent and identically distributed</li>
<li>ignorance <!-- - de Finetti's theorem --></li>
<li>partially exchangeable</li>
<li>conditionally exchangeable</li>
<li>conditional independence</li>
<li>hyperprior</li>
<li>different posterior predictive distributions</li>
<li>the conditional probability formula</li>
</ul>
</section>
<section id="r-and-python-demos-3" class="level3">
<h3 class="anchored" data-anchor-id="r-and-python-demos-3">R and Python demos</h3>
<ul>
<li>5.1: Rats example. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch5/demo5_1.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch5/demo5_1.ipynb">Python</a>.</li>
<li>5.2: SAT example. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch5/demo5_2.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch5/demo5_2.ipynb">Python</a>.</li>
</ul>
</section>
<section id="recommended-exercises-3" class="level3">
<h3 class="anchored" data-anchor-id="recommended-exercises-3">Recommended exercises</h3>
<p>Optional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:</p>
<ul>
<li><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=144">5.1 and 5.2</a> (<a href="http://www.stat.columbia.edu/~gelman/book/solutions3.pdf">model solution available for 5.3-5.5, 5.7-5.12</a>)</li>
</ul>
</section>
<section id="computation" class="level3">
<h3 class="anchored" data-anchor-id="computation">Computation</h3>
<p>Examples in BDA3 Sections 5.3 and 5.4 continue computation with factorization and grid, but there is no need to go deep in to computational details as in the assignments you will use MCMC and Stan instead.</p>
</section>
<section id="exchangeability-vs.-independence" class="level3">
<h3 class="anchored" data-anchor-id="exchangeability-vs.-independence">Exchangeability vs.&nbsp;independence</h3>
<p>Exchangeability and independence are two separate concepts. Neither necessarily implies the other. Independent identically distributed variables/parameters are exchangeable. Exchangeability is less strict condition than independence. Often we may assume that observations or unobserved quantities are in fact dependent, but if we can’t get information about these dependencies we may assume those observations or unobserved quantities as exchangeable. ``Ignorance implies exchangeability.’’</p>
<p>In case of exchangeable observations, we may sometimes act <em>as if</em> observations were independent if the additional potential information gained from the dependencies is very small. This is related to de Finetti’s theorem (<a href="https://users.aalto.fi/~ave/BDA3.pdf#page=115">BDA3 p.&nbsp;105</a>), which applies formally only when <span class="math inline">\(J\rightarrow\infty\)</span>, but in practice difference may be small if <span class="math inline">\(J\)</span> is finite but relatively large (see examples below).</p>
<ul>
<li>If no other information than data <span class="math inline">\(y\)</span> is available to distinguish <span class="math inline">\(\theta_j\)</span> from each other and parameters can not be ordered or grouped, we may assume symmetry between parameters in their prior distribution</li>
<li>This symmetry can be represented with exchangeability</li>
<li>Parameters <span class="math inline">\(\theta_1,\ldots,\theta_J\)</span> are exchangeable in their joint distribution if <span class="math inline">\(p(\theta_1,\ldots,\theta_J)\)</span> is invariant to permutation of indexes <span class="math inline">\((1,\ldots,J)\)</span></li>
</ul>
<p>Here are some examples you may consider.</p>
<p>Ex 5.1. Exchangeability with known model parameters: For each of following three examples, answer: (i) Are observations <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> exchangeable? (ii) Are observations <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> independent? (iii) Can we act <em>as if</em> the two observations are independent?</p>
<ol type="1">
<li>A box has one black ball and one white ball. We pick a ball <span class="math inline">\(y_1\)</span> at random, put it back, and pick another ball <span class="math inline">\(y_2\)</span> at random.</li>
<li>A box has one black ball and one white ball. We pick a ball <span class="math inline">\(y_1\)</span> at random, we do not put it back, then we pick ball <span class="math inline">\(y_2\)</span>.</li>
<li>A box has a million black balls and a million white balls. We pick a ball <span class="math inline">\(y_1\)</span> at random, we do not put it back, then we pick ball <span class="math inline">\(y_2\)</span> at random.</li>
</ol>
<p>Ex 5.2. Exchangeability with unknown model parameters: For each of following three examples, answer: (i) Are observations <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> exchangeable? (ii) Are observations <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> independent? (iii) Can we act <em>as if</em> the two observations are independent?</p>
<ol type="1">
<li>A box has <span class="math inline">\(n\)</span> black and white balls but we do not know how many of each color. We pick a ball <span class="math inline">\(y_1\)</span> at random, put it back, and pick another ball <span class="math inline">\(y_2\)</span> at random.</li>
<li>A box has <span class="math inline">\(n\)</span> black and white balls but we do not know how many of each color. We pick a ball <span class="math inline">\(y_1\)</span> at random, we do not put it back, then we pick ball <span class="math inline">\(y_2\)</span> at random.</li>
<li>Same as (b) but we know that there are many balls of each color in the box.</li>
</ol>
<p>Note that for example in opinion polls, balls i.e.&nbsp;humans are not put back and there is a large but finite number of humans.</p>
<p>Following complements the divorce example in the book by discussing the effect of the additional observations</p>
<ul>
<li>Example: divorce rate per 1000 population in 8 states of the USA in 1981
<ul>
<li>without any other knowledge <span class="math inline">\(y_1,\ldots,y_8\)</span> are exchangeable</li>
<li>it is reasonable to assume a prior independence given population density <span class="math inline">\(p(y_i|\theta)\)</span></li>
</ul></li>
<li>Divorce rate in first seven are <span class="math inline">\(5.6, 6.6, 7.8, 5.6,
    7.0, 7.2, 5.4\)</span>
<ul>
<li>now we have some additional information, but still changing the indexing does not affect the joint distribution. For example, if we were told that divorce rate were not for the first seven but last seven states, it does not change the joint distribution, and thus <span class="math inline">\(y_1,\ldots,y_8\)</span> are exchangeable</li>
<li>sensible assumption is a prior independence given population density <span class="math inline">\(p(y_i|\theta)\)</span></li>
<li>if "true" <span class="math inline">\(\theta_0\)</span> were known, <span class="math inline">\(y_1,\ldots,y_8\)</span> were independent given "true" <span class="math inline">\(\theta_0\)</span></li>
<li>since <span class="math inline">\(\theta\)</span> is estimated using observations, <span class="math inline">\(y_i\)</span> are a posterior dependent, which is obvious, e.g., from the predictive density <span class="math inline">\(p(y_8|y_1,\ldots,y_7,M)\)</span>, i.e.&nbsp;e.g.&nbsp;if <span class="math inline">\(y_1,\ldots,y_7\)</span> are large then probably <span class="math inline">\(y_8\)</span> is large</li>
<li>if we were told that given rates were for the last seven states, then <span class="math inline">\(p(y_1|y_2,\ldots,y_8,M)\)</span> would be exactly same as <span class="math inline">\(p(y_8|y_1,\ldots,y_7,M)\)</span> above, i.e.&nbsp;changing the indexing does not have effect since <span class="math inline">\(y_1,\ldots,y_8\)</span> are exchangeable</li>
</ul></li>
<li>Additionally we know that <span class="math inline">\(y_8\)</span> is Nevada and rates of other states are <span class="math inline">\(5.6, 6.6, 7.8, 5.6, 7.0, 7.2, 5.4\)</span>
<ul>
<li>based on what we were told about Nevada, predictive density s <span class="math inline">\(p(y_8|y_1,\ldots,y_7,M)\)</span> should take into account that probability <span class="math inline">\(p(y_8&gt;\max(y_1,\ldots,y_7)|y_1,\ldots,y_7)\)</span> should be large</li>
<li>if we were told that, Nevada is <span class="math inline">\(y_3\)</span> (not <span class="math inline">\(y_8\)</span> as above), then new predictive density <span class="math inline">\(p(y_8|y_1,\ldots,y_7,M)\)</span> would be different, because <span class="math inline">\(y_1,\ldots,y_8\)</span> are not anymore exchangeable</li>
</ul></li>
</ul>
</section>
<section id="what-if-observations-are-not-exchangeable" class="level3">
<h3 class="anchored" data-anchor-id="what-if-observations-are-not-exchangeable">What if observations are not exchangeable</h3>
<p>Often observations are not fully exchangeable, but are partially or conditionally exchangeable. Two basic cases</p>
<ul>
<li>If observations can be grouped, we may make hierarchical model, were each group has own subpart, but the group properties are unknown. If we assume that group properties are exchangeable we may use common prior for the group properties.</li>
<li>If <span class="math inline">\(y_i\)</span> has additional information <span class="math inline">\(x_i\)</span>, then <span class="math inline">\(y_i\)</span> are not exchangeable, but <span class="math inline">\((y_i,x_i)\)</span> still are exchangeable, then we can be make joint model for <span class="math inline">\((y_i,x_i)\)</span> or conditional model <span class="math inline">\((y_i|x_i)\)</span>.</li>
</ul>
<p>Here are additional examples (Bernardo &amp; Smith, Bayesian Theory, 1994), which illustrate the above basic cases. Think of old fashioned thumb pin. This kind of pin can stay flat on it’s base or slanting so that the pin head and the edge of the base touch the table. This kind of pin represents realistic version of "unfair" coin.</p>
<ul>
<li>Let’s throw pin <span class="math inline">\(n\)</span> times and mark <span class="math inline">\(x_i=1\)</span> when pin stands on it’s base. Let’s assume, that throwing conditions stay same all the time. Most would accept throws as exchangeable.</li>
<li>Same experiment, but odd numbered throws will be made with full metal pin and even numbered throws with plastic coated pin. Most would accept exchangeability for all odd and all even throws separately, but not necessarily for both series combined. Thus we have partial exchangeability.</li>
<li>Laboratory experiments <span class="math inline">\(x_1,...,x_n\)</span>, are real valued measurements about the chemical property of some substance. If all experiments are from the same sample, in the same laboratory with same procedure, most would accept exchangeability. If experiments were made, for example, in different laboratories we could assume partial exchangeability.</li>
<li><span class="math inline">\(x_1,...,x_n\)</span> are real valued measurements about the physiological reactions to certain medicine. Different test persons get different amount of medicine. Test persons are males and females of different ages. If the attributes of the test persons were known, most would not accept results as exchangeable. In a group with certain dose, sex and age, the measurements could be assumed exchangeable. We could use grouping or if the doses and attributes are continuous we could use regression, that is, assume conditional independence.</li>
</ul>
</section>
<section id="weakly-informative-priors-for-hierarchical-variance-parameters" class="level3">
<h3 class="anchored" data-anchor-id="weakly-informative-priors-for-hierarchical-variance-parameters">Weakly informative priors for hierarchical variance parameters</h3>
<p>Our thinking has advanced since section 5.7 was written. Section 5.7 (<a href="https://users.aalto.fi/~ave/BDA3.pdf#page=138">BDA3 p.&nbsp;128–</a>) recommends use of half-Cauchy as weakly informative prior for hierarchical variance parameters. More recent recommendation is half-normal if you have substantial information on the high end values, or or half-<span class="math inline">\(t_4\)</span> if you there might be possibility of surprise. Often we don’t have so much prior information that we would be able to well define the exact tail shape of the prior, but half-normal produces usually more sensible prior predictive distributions and is thus better justified. Half-normal leads also usually to easier inference.</p>
<p>See the <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Prior Choice Wiki</a> for more recent general discussion and model specific recommendations.</p>
</section>
</section>
<section id="ch6" class="level2">
<h2 class="anchored" data-anchor-id="ch6">Chapter 6 Model checking</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=151">Chapter 6</a> is related to the Lecture 8 <em>Model checking &amp; cross-validation</em>.</p>
<section id="outline-5" class="level3">
<h3 class="anchored" data-anchor-id="outline-5">Outline</h3>
<ul>
<li>6.1 The place of model checking in applied Bayesian statistics</li>
<li>6.2 Do the inferences from the model make sense?</li>
<li>6.3 Posterior predictive checking (<span class="math inline">\(p\)</span>-values can be skipped)</li>
<li>6.4 Graphical posterior predictive checks (this can be skimmed, see instead the paper <em>Visualization in Bayesian workflow</em>)</li>
<li>6.5 Model checking for the educational testing example</li>
</ul>
</section>
<section id="the-most-important-terms-5" class="level3">
<h3 class="anchored" data-anchor-id="the-most-important-terms-5">The most important terms</h3>
<p>Find all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.</p>
<ul>
<li>model checking</li>
<li>sensitivity analysis</li>
<li>external validation</li>
<li>posterior predictive checking</li>
<li>joint posterior predictive distribution</li>
<li>marginal (posterior) predictive distribution</li>
<li>self-consistency check</li>
<li>replicated data</li>
<li><span class="math inline">\(y^{\mathop{\mathrm{\mathrm{rep}}}}\)</span>, <span class="math inline">\(\tilde{y}\)</span>, <span class="math inline">\(\tilde{x}\)</span></li>
<li>test quantities</li>
<li>discrepancy measure</li>
<li>tail-area probabilities</li>
<li>classical <span class="math inline">\(p\)</span>-value</li>
<li>posterior predictive <span class="math inline">\(p\)</span>-values</li>
<li>multiple comparisons</li>
<li>marginal predictive checks</li>
<li>cross-validation predictive distributions</li>
</ul>
</section>
<section id="r-and-python-demos-4" class="level3">
<h3 class="anchored" data-anchor-id="r-and-python-demos-4">R and Python demos</h3>
<ul>
<li>6.1: Posterior predictive checking - speed of light. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch6/demo6_1.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch6/demo6_1.ipynb">Python</a>.</li>
<li>6.2: Posterior predictive checking - sequential dependence. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch6/demo6_1.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch6/demo6_1.ipynb">Python</a>.</li>
<li>6.3: Posterior predictive checking - poor test statistic. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch6/demo6_1.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch6/demo6_1.ipynb">Python</a>.</li>
<li>6.4: Posterior predictive checking - marginal predictive <span class="math inline">\(p\)</span>-value. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch6/demo6_1.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch6/demo6_1.ipynb">Python</a>.</li>
</ul>
</section>
<section id="recommended-exercises-4" class="level3">
<h3 class="anchored" data-anchor-id="recommended-exercises-4">Recommended exercises</h3>
<p>Optional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:</p>
<ul>
<li><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=173">6.1</a> (<a href="http://www.stat.columbia.edu/~gelman/book/solutions3.pdf">model solution available for 6.1, 6.5-6.7</a>)</li>
</ul>
</section>
<section id="replicates-vs.-future-observation" class="level3">
<h3 class="anchored" data-anchor-id="replicates-vs.-future-observation">Replicates vs.&nbsp;future observation</h3>
<p>Predictive <span class="math inline">\(\tilde{y}\)</span> is the next not yet observed possible observation. <span class="math inline">\(y^{\mathrm{rep}}\)</span> refers to replicating the whole experiment (with same values of <span class="math inline">\(x\)</span>) and obtaining as many replicated observations as in the original data.</p>
</section>
<section id="posterior-predictive-p-values" class="level3">
<h3 class="anchored" data-anchor-id="posterior-predictive-p-values">Posterior predictive <span class="math inline">\(p\)</span>-values</h3>
<p>Section 6.3 discusses posterior predictive <span class="math inline">\(p\)</span>-values, which we don’t recommend any more especially in a form of hypothesis testing.</p>
</section>
<section id="prior-predictive-checking" class="level3">
<h3 class="anchored" data-anchor-id="prior-predictive-checking">Prior predictive checking</h3>
<p>Prior predictive checking using just the prior predictive distributions is very useful tool for assessing the sensibility of the model and priors even before observing any data or before doing the posterior inference. See additional reading below for examples.</p>
</section>
<section id="additional-reading" class="level3">
<h3 class="anchored" data-anchor-id="additional-reading">Additional reading</h3>
<p>The following article has some useful discussion and examples also about prior and posterior predictive checking.</p>
<ul>
<li>Gabry, Simpson, Vehtari, Betancourt, and Gelman (2018). Visualization in Bayesian workflow. <em>Journal of the Royal Statistical Society Series A</em>, , 182(2):389-402. <a href="https://doi.org/10.1111/rssa.12378">Online</a>.</li>
<li><a href="https://www.youtube.com/watch?v=E8vdXoJId8M">Video of the paper presentation</a>.</li>
</ul>
<p>And some additional useful demos</p>
<ul>
<li><a href="http://mc-stan.org/bayesplot/articles/graphical-ppcs.html">Graphical posterior predictive checks using the bayesplot package</a>.</li>
<li><a href="http://avehtari.github.io/BDA_R_demos/demos_rstan/ppc/poisson-ppc.html">Another PPC demo</a>.</li>
</ul>
</section>
</section>
<section id="ch7" class="level2">
<h2 class="anchored" data-anchor-id="ch7">Chapter 7 Evaluating, comparing, and expanding models</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=175">Chapter 7</a> is related to the Lecture 8 <em>Model checking &amp; cross-validation’’ and he Lecture 9 </em>Model comparison and selection*.</p>
<section id="outline-6" class="level3">
<h3 class="anchored" data-anchor-id="outline-6">Outline</h3>
<ul>
<li>7.1 Measures of predictive accuracy</li>
<li>7.2 Information criteria and cross-validation (read instead the article mentioned below)</li>
<li>7.3 Model comparison based on predictive performance (read instead the article mentioned below)</li>
<li>7.4 Model comparison using Bayes factors (not used in the course, but useful to read if you have heard about Bayes factors)</li>
<li>7.5 Continuous model expansion / sensitivity analysis</li>
<li>7.6 Example (may be skipped)</li>
</ul>
<p>Instead of Sections 7.2 and 7.3 it’s better to read</p>
<ul>
<li>Aki Vehtari, Andrew Gelman and Jonah Gabry (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. <em>Statistics and Computing</em>, <strong>27</strong>(5):1413-1432, doi:10.1007/s11222-016-9696-4. <a href="http://arxiv.org/abs/1507.04544">arXiv preprint arXiv:1507.04544</a>.</li>
<li><a href="https://mc-stan.org/loo/reference/loo-glossary.html">LOO package glossary</a> summarizes many important terms used in the assignments.</li>
<li><a href="https://users.aalto.fi/~ave/CV-FAQ.html">CV-FAQ</a></li>
</ul>
<p>In Sections 7.2 and 7.3 of BDA, for historical reasons there is a multiplier <span class="math inline">\(-2\)</span> used. After the book was published, we have concluded that it causes too much confusion and recommend not to multiply by <span class="math inline">\(-2\)</span>. The above paper is not using <span class="math inline">\(-2\)</span> anymore.</p>
</section>
<section id="extra-material" class="level3">
<h3 class="anchored" data-anchor-id="extra-material">Extra material</h3>
<p>The following article provides excellent discussion about “How should I evaluate my modeling choices?” from a scientific perspective.</p>
<ul>
<li><p>Danielle J. Navarro (2019). Between the devil and the deep blue sea: Tensions between scientific judgment and statistical model selection. <em>Computational Brain &amp; Behavior</em> <strong>2</strong>:28–34. <a href="https://doi.org/10.1007/s42113-018-0019-z">Online</a>.</p></li>
<li><p><a href="https://avehtari.github.io/modelselection/">Extra videos, slides, case studies, and references on model selection</a></p></li>
<li><p>Sections 1 and 5 (less than 3 pages) of “<a href="https://arxiv.org/abs/2008.10296">Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison</a>” clarify how to interpret standard error in model comparison</p></li>
</ul>
</section>
<section id="the-most-important-terms-6" class="level3">
<h3 class="anchored" data-anchor-id="the-most-important-terms-6">The most important terms</h3>
<p>Find all the terms and symbols listed below. When reading the chapter and the above mentioned article, write down questions related to things unclear for you or things you think might be unclear for others.</p>
<ul>
<li>predictive accuracy/fit/error</li>
<li>external validation</li>
<li>cross-validation</li>
<li>information criteria</li>
<li>overfitting</li>
<li>measures of predictive accuracy</li>
<li>point prediction</li>
<li>scoring function</li>
<li>mean squared error</li>
<li>scoring rule</li>
<li>logarithmic score</li>
<li>log-predictive density</li>
<li>out-of-sample predictive fit</li>
<li>elpd, elppd, lppd</li>
<li>within-sample predictive accuracy</li>
<li>adjusted within-sample predictive accuracy</li>
<li>AIC, DIC, WAIC (less important)</li>
<li>effective number of parameters</li>
<li>singular model</li>
<li>leave-one-out cross-validation</li>
<li>evaluating predictive error comparisons</li>
<li>bias induced by model selection</li>
<li>Bayes factors</li>
<li>continuous model expansion</li>
<li>sensitivity analysis</li>
</ul>
</section>
<section id="additional-reading-1" class="level3">
<h3 class="anchored" data-anchor-id="additional-reading-1">Additional reading</h3>
<p>More theoretical details can be found in</p>
<ul>
<li>Aki Vehtari and Janne Ojanen (2012). A survey of Bayesian predictive methods for model assessment, selection and comparison. In Statistics Surveys, 6:142-228. <a href="http://dx.doi.org/10.1214/12-SS102">Online</a>.</li>
</ul>
<p>More experimental comparisons can be found in</p>
<ul>
<li>Juho Piironen and Aki Vehtari (2017). Comparison of Bayesian predictive methods for model selection. Statistics and Computing, 27(3):711-735. <a href="http://link.springer.com/article/10.1007/s11222-016-9649-y">Online</a>.</li>
</ul>
</section>
<section id="posterior-probability-of-the-model-vs.-predictive-performance" class="level3">
<h3 class="anchored" data-anchor-id="posterior-probability-of-the-model-vs.-predictive-performance">Posterior probability of the model vs.&nbsp;predictive performance</h3>
<p>Gelman: “To take a historical example, I don’t find it useful, from a statistical perspective, to say that in 1850, say, our posterior probability that Newton’s laws were true was 99%, then in 1900 it was 50%, then by 1920, it was 0.01% or whatever. I’d rather say that Newton’s laws were a good fit to the available data and prior information back in 1850, but then as more data and a clearer understanding became available, people focused on areas of lack of fit in order to improve the model.”</p>
<p>Newton’s laws are still sufficient for prediction in specific contexts (non-relative speeds and differences in gravity, non-significant effects of air resistance or other friction). See more in <a href="https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=d841f429-9c3d-4d24-8228-a9f400efda7b">the course video 1.1 Introduction to uncertainty and modeling</a>.</p>
</section>
</section>
<section id="ch8" class="level2">
<h2 class="anchored" data-anchor-id="ch8">Chapter 8 Modeling accounting for data collection</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=207">Chapter 8</a> is not part of the BDA Aalto course.</p>
<p>In the earlier chapters it was assumed that the data collection is ignorable. Chapter 8 explains when data collection can be ignorable and when we need to model also the data collection. We don’t have time to go through chapter 8 in BDA course at Aalto, but it is highly recommended that you would read it in the end or after the course. Most important sections are 8.1, 8.5, pp.&nbsp;220–222 of 8.6, and 8.8, and you can get back to the other sections later.</p>
<section id="outline-7" class="level3">
<h3 class="anchored" data-anchor-id="outline-7">Outline</h3>
<ul>
<li>8.1 Bayesian inference requires a model for data collection (important)</li>
<li>8.2 Data-collection models and ignorability</li>
<li>8.3 Sample surveys</li>
<li>8.4 Designed experiments</li>
<li>8.5 Sensitivity and the role of randomization (important)</li>
<li>8.6 Observational studies (pp.&nbsp;220–222 important)</li>
<li>8.7 Censoring and truncation (important)</li>
</ul>
</section>
<section id="the-most-important-terms-7" class="level3">
<h3 class="anchored" data-anchor-id="the-most-important-terms-7">The most important terms</h3>
<ul>
<li>observed data</li>
<li>complete data</li>
<li>missing data</li>
<li>stability assumption</li>
<li>data model</li>
<li>inclusion model</li>
<li>complete data likelihood</li>
<li>observed data likelihood</li>
<li>finite-population and superpopulation inference</li>
<li>ignorability</li>
<li>ignorable designs</li>
<li>propensity score</li>
<li>sample surveys</li>
<li>random sampling of a finite population</li>
<li>stratified sampling</li>
<li>cluster sampling</li>
<li>designed experiments</li>
<li>complete randomization</li>
<li>randomized blocks and Latin squares</li>
<li>sequential designs</li>
<li>randomization given covariates</li>
<li>observational studies</li>
<li>censoring</li>
<li>truncation</li>
<li>missing completely at random</li>
</ul>
</section>
</section>
<section id="ch9" class="level2">
<h2 class="anchored" data-anchor-id="ch9">Chapter 9 Decision analysis</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=247">Chapter 9</a> is related to the Lecture 10 <em>Decision analysis</em>.</p>
<section id="outline-8" class="level3">
<h3 class="anchored" data-anchor-id="outline-8">Outline</h3>
<ul>
<li>9.1 Context and basic steps (most important part)</li>
<li>9.2 Example</li>
<li>9.3 Multistage decision analysis (you may skip this example)</li>
<li>9.4 Hierarchical decision analysis (you may skip this example)</li>
<li>9.5 Personal vs.&nbsp;institutional decision analysis (important)</li>
</ul>
</section>
<section id="the-most-important-terms-8" class="level3">
<h3 class="anchored" data-anchor-id="the-most-important-terms-8">The most important terms</h3>
<p>Find all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.</p>
<ul>
<li>decision analysis</li>
<li>steps of Bayesian decision analysis 1–4 (<a href="https://users.aalto.fi/~ave/BDA3.pdf#page=248">BDA3 p.&nbsp;238</a>)</li>
<li>decision</li>
<li>outcome</li>
<li>utility function</li>
<li>expected utility</li>
<li>decision tree</li>
<li>summarizing inference</li>
<li>model selection</li>
<li>individual decision problem</li>
<li>institutional decision problem</li>
</ul>
</section>
<section id="simpler-examples" class="level3">
<h3 class="anchored" data-anchor-id="simpler-examples">Simpler examples</h3>
<p>The lectures have simpler examples and also discuss some challenges in selecting utilities or costs.</p>
</section>
<section id="model-selection-as-a-decision-problem" class="level3">
<h3 class="anchored" data-anchor-id="model-selection-as-a-decision-problem">Model selection as a decision problem</h3>
<p><a href="ch7">Chapter 7</a> discusses how model selection con be considered as a decision problem.</p>
</section>
</section>
<section id="ch10" class="level2">
<h2 class="anchored" data-anchor-id="ch10">Chapter 10 Introduction to Bayesian computation</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=271">Chapter 10</a> is related to the Lecture 4 <em>Monte Carlo</em>.</p>
<section id="outline-9" class="level3">
<h3 class="anchored" data-anchor-id="outline-9">Outline</h3>
<ul>
<li>10.1 Numerical integration (overview)</li>
<li>10.2 Distributional approximations (overview, more in Chapters <a href="ch4">4</a> and <a href="ch13">13</a>)</li>
<li>10.3 Direct simulation and rejection sampling (overview)</li>
<li>10.4 Importance sampling (used in PSIS-LOO discussed later)</li>
<li>10.5 How many simulation draws are needed? (Important!)</li>
<li>10.6 Software (can be skipped)</li>
<li>10.7 Debugging (can be skipped)</li>
</ul>
<p>Sections 10.1–10.4 give overview of different computational methods. Some of then have been already used in the book.</p>
<p>Section 10.5 is very important and related to the exercises.</p>
</section>
<section id="the-most-important-terms-9" class="level3">
<h3 class="anchored" data-anchor-id="the-most-important-terms-9">The most important terms</h3>
<p>Find all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.</p>
<ul>
<li>unnormalized density</li>
<li>target distribution</li>
<li>log density</li>
<li>overflow and underflow</li>
<li>numerical integration</li>
<li>quadrature</li>
<li>simulation methods</li>
<li>Monte Carlo</li>
<li>stochastic methods</li>
<li>deterministic methods</li>
<li>distributional approximations</li>
<li>crude estimation</li>
<li>direct simulation</li>
<li>grid sampling</li>
<li>rejection sampling</li>
<li>importance sampling</li>
<li>importance ratios/weights</li>
</ul>
</section>
<section id="r-and-python-demos-5" class="level3">
<h3 class="anchored" data-anchor-id="r-and-python-demos-5">R and Python demos</h3>
<ul>
<li>10.1: Rejection sampling. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch10/demo10_1.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch10/demo10_1.ipynb">Python</a>.</li>
<li>10.2: Importance sampling. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch10/demo10_2.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch10/demo10_2.ipynb">Python</a>.</li>
<li>10.3: Sampling-importance resampling. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch10/demo10_3.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch10/demo10_3.ipynb">Python</a>.</li>
</ul>
</section>
<section id="recommended-exercises-5" class="level3">
<h3 class="anchored" data-anchor-id="recommended-exercises-5">Recommended exercises</h3>
<p>Optional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:</p>
<ul>
<li><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=282">10.1, 10.2</a> (<a href="http://www.stat.columbia.edu/~gelman/book/solutions3.pdf">model solution available for 10.4</a>)</li>
</ul>
</section>
<section id="numerical-accuracy-of-computer-arithmetic" class="level3">
<h3 class="anchored" data-anchor-id="numerical-accuracy-of-computer-arithmetic">Numerical accuracy of computer arithmetic</h3>
<p>Many models use continuous real valued parameters. Computers have finite memory and thus the continuous values are also presented with finite number of bits and thus with finite accuracy. Most commonly used presentations are floating-point presentations that try to have balanced accuracy over the range of values where it mostly matters. As the the presentation has finite accuracy there are limitations, for example, with IEC 60559 floating-point (double precision) arithmetic used in current R</p>
<ul>
<li><p>the smallest positive floating-point number <span class="math inline">\(x\)</span> such that <span class="math inline">\(1 + x \neq 1\)</span> is <span class="math inline">\(2.220446\cdot 10^{-16}\)</span></p></li>
<li><p>the smallest non-zero normalized floating-point number is <span class="math inline">\(2.225074\cdot 10^{-308}\)</span></p></li>
<li><p>the largest normalized floating-point number <span class="math inline">\(1.797693\cdot 10^{308}\)</span></p></li>
<li><p>the largest integer which can be represented is <span class="math inline">\(2^{31} - 1 = 2147483647\)</span></p></li>
<li><p>see more in <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/zMachine.html">R documentation: Numerical Characteristics of the Machine</a>.</p></li>
<li><p><a href="https://nhigham.com/2020/05/04/what-is-floating-point-arithmetic/">What Is Floating-Point Arithmetic? blog post by Nick Higham</a> provides nice short introduction.</p></li>
<li><p><a href="https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html">What Every Computer Scientist Should Know About Floating-Point Arithmetic by Goldberg (1991)</a> provides nice overview of floating-point arithmetic and how the computations should be arranged for improved accuracy.</p></li>
<li><p><a href="https://stat.umn.edu/geyer/3701/notes/arithmetic.html">Stat 3701 Lecture Notes: Computer Arithmetic by Geyer (2020)</a> provide code examples in R illustrating the most common issues in floating-point arithmetic including examples similar shown in the BDA course lecture.</p></li>
<li><p><a href="https://mc-stan.org/docs/2_26/stan-users-guide/floating-point-arithmetic.html">Stan User Guide Chapter 15</a> discusses floating point arithmetic in context of Stan.</p></li>
</ul>
</section>
<section id="terms-draw-draws-and-sample" class="level3">
<h3 class="anchored" data-anchor-id="terms-draw-draws-and-sample">Terms draw, draws and sample</h3>
<p>A group of draws is a sample. A sample can consist of one draw, and thus some people use the word sample for both single item and for the group. For clarity, we prefer separate words for a single item (draw) and for the group (sample). Sample can also mean a group of observations, and thus talking about posterior draws reduces ambiguity.</p>
</section>
<section id="monte-carlo-standard-error" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-standard-error">Monte Carlo standard error</h3>
<p>Monte Carlo estimates have some error due to using a finite number of random draws. Monte Carlo standard error (MCSE) estimates this error. BDA3 Section 10.5 discuss the basics of this. Additional information:</p>
<ul>
<li>When we know the needed accuracy for reporting posterior summaries, we can estimate how big sample size is needed to get small enough MCSE for the quantity of interest.</li>
<li>MCSE is based on central limit theorem which assumes that the distribution has finite variance.</li>
<li>Pareto-<span class="math inline">\(k\)</span> diagnostic can be used to estimate whether the draws come from a distribution with finite variance (see below).</li>
<li>BDA3 discusses cases for mean (E<span class="math inline">\((\theta)\)</span>) and probability (<span class="math inline">\(p(\theta &lt; \alpha)\)</span>). MCSE for quantiles can be derived using MCSE for probability and inverse transform (details in Vehtari et al.&nbsp;(2020). Rank-normalization, folding, and localization: An improved <span class="math inline">\(\widehat{R}\)</span> for assessing convergence of MCMC. <em>Bayesian analysis</em>, <a href="https://projecteuclid.org/euclid.ba/1593828229">Online</a>).</li>
<li><a href="https://mc-stan.org/posterior/"><code>posterior</code></a> package provides functions <code>mcse_mean()</code> and <code>mcse_quantile()</code>. MCSE for probabilities is obtained using <code>mcse_mean()</code> for indicator function outcome of the logical comparison.</li>
</ul>
</section>
<section id="central-limit-theorem" class="level3">
<h3 class="anchored" data-anchor-id="central-limit-theorem">Central limit theorem</h3>
<ul>
<li>“In probability theory, the central limit theorem (CLT) states that, under appropriate conditions, the distribution of a normalized version of the sample mean converges to a standard normal distribution. This holds even if the original variables themselves are not normally distributed. There are several versions of the CLT, each applying in the context of different conditions.” <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Wikipedia Central limit theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Wikipedia Central limit theorem</a> article has good summary of the topic and variants with links to further information</li>
<li>3Blue1Brown YouTube videos with nice visualizations
<ul>
<li>CLT with discrete distributions: “But what is the Central Limit Theorem?” </li>
<li>CLT with continuous distributions: “Convolutions | Why X+Y in probability is a beautiful mess” </li>
</ul></li>
</ul>
</section>
<section id="pareto-hatk-diagnostic" class="level3">
<h3 class="anchored" data-anchor-id="pareto-hatk-diagnostic">Pareto-<span class="math inline">\(\hat{k}\)</span> diagnostic</h3>
<p>Pareto-<span class="math inline">\(\hat{k}\)</span> diagnostic estimates the tail shape of the distribution <span class="math inline">\(p(\theta)\)</span> given draws <span class="math inline">\(\theta^{(s)}\)</span> from that distribution. The tail shape indicates whether the distribution has finite variance and mean.</p>
<ul>
<li>If <span class="math inline">\(\hat{k}&gt;1\)</span>, it is likely that the distribution does not have finite mean, and trying to estimate the mean does not make sense.</li>
<li>If <span class="math inline">\(\hat{k}&gt;0.5\)</span>, it is likely that the distribution does not have finite variance, and Monte Carlo standard error estimate based on the variance does not make sense.</li>
<li><span class="math inline">\(\hat{k}\)</span> estimate has it’s own variation given finite sample size, and if in doubt, get more draws to get more accurate <span class="math inline">\(\hat{k}\)</span> estimate.</li>
<li>Pareto-<span class="math inline">\(\hat{k}\)</span> diagnostic is pre-asymptotic based on finite sample size, and indicates the stability of empirical mean estimate given the draws so far. Increasing the number of draws, can get more draws far from the tail and may reveal that the tail shape is different further away.</li>
<li><a href="https://mc-stan.org/posterior/"><code>posterior</code></a> package provides function <code>pareto_khat()</code></li>
<li>Monte Carlo estimates (including importance sampling) can be improved using Pareto-smoothing. Pareto smoothed Monte Carlo estimate has finite variance even if <span class="math inline">\(p(\theta)\)</span> does not, and the corresponding Monte Carlo standard error estimate can be useful if <span class="math inline">\(\hat{k}&lt;0.7\)</span>. More about this later in the course.</li>
<li>More details in Vehtari et al.&nbsp;(2024). Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>, 25(72):1-58. <a href="https://jmlr.org/papers/v25/19-556.html">Online</a>.</li>
</ul>
</section>
<section id="how-many-digits-should-be-displayed" class="level3">
<h3 class="anchored" data-anchor-id="how-many-digits-should-be-displayed">How many digits should be displayed</h3>
<ul>
<li>Too many digits make reading of the results slower and give false impression of the accuracy.</li>
<li>Show meaningful digits given the posterior uncertainty. You can compare posterior standard error or posterior intervals to the mean value. Posterior interval length can be used to determine also how many digits to show for the interval endpoints.</li>
<li>Don’t ever show digits which are just random noise. You can use Monte Carlo standard error estimates to check how many digits are likely to stay the same if the sampling would be continued.</li>
<li>The advise considers the number of significant digits, which is different from the number of decimal digits. For example, the numbers <span class="math inline">\(12\)</span>, <span class="math inline">\(1.2\)</span>, <span class="math inline">\(0.12\)</span>, and <span class="math inline">\(0.012\)</span> all have two significant digits. It is common that even if 2 significant digits would be sufficient, numbers like <span class="math inline">\(123.4\)</span> and <span class="math inline">\(1234.5\)</span> would be shown as rounded to integers <span class="math inline">\(123\)</span> and <span class="math inline">\(1234\)</span> (using the round to nearest even integer rule used by R).</li>
<li>Example: The mean and 90% central posterior interval for temperature increase C<span class="math inline">\(^\circ\)</span>/century (see the slides for the example) based on posterior draws:
<ul>
<li><span class="math inline">\(2.050774\)</span> and <span class="math inline">\([0.7472868 3.3017524]\)</span> (too many digits)</li>
<li><span class="math inline">\(2.1\)</span> and <span class="math inline">\([0.7 3.3]\)</span> (good compared to the interval length)</li>
<li><span class="math inline">\(2\)</span> and <span class="math inline">\([1 3]\)</span> (sufficient accuracy in many cases)</li>
</ul></li>
<li>Example: The probability that temp increase is positive
<ul>
<li><span class="math inline">\(0.9960000\)</span> (too many digits)</li>
<li><span class="math inline">\(1.00\)</span> (depends on the context. <span class="math inline">\(1.00\)</span> hints it’s not exactly <span class="math inline">\(1\)</span>, but larger than <span class="math inline">\(0.99\)</span>)</li>
<li>With 4000 draws MCSE <span class="math inline">\(\approx 0.002\)</span>. We could report that probability is very likely larger than <span class="math inline">\(0.99\)</span>, or sample more to justify reporting three digits</li>
<li>For probabilities close to 0 or 1, consider also when the model assumption justify certain accuracy</li>
</ul></li>
<li>When reporting many numbers in table, for aesthetics reasons, it may be sometimes better for some numbers to show one extra or one too few digits compared to the ideal.</li>
<li>Often it’s better to plot the whole posterior density in addition of any summaries, as summaries always loose some information content.</li>
<li>For your reports: Don’t be lazy and settle for the default number of digits in R or Python. Think for each reported value how many digits is sensible. If the Quiz asks certain number of digits, this overrules other advise.</li>
<li>See <a href="https://users.aalto.fi/~ave/casestudies/Digits/digits.html">Digits case study</a> for examples and how to use <a href="https://mc-stan.org/posterior/"><code>posterior</code></a> package functions.</li>
</ul>
</section>
<section id="quadrature" class="level3">
<h3 class="anchored" data-anchor-id="quadrature">Quadrature</h3>
<p>Sometimes ‘quadrature’ is used to refer generically to any <a href="https://en.wikipedia.org/wiki/Numerical_integration">numerical integration method</a> (including Monte Carlo), sometimes it is used to refer just to deterministic numerical integration methods.</p>
</section>
<section id="rejection-sampling" class="level3">
<h3 class="anchored" data-anchor-id="rejection-sampling">Rejection sampling</h3>
<p>Rejection sampling is mostly used as a part of fast methods for univariate sampling. For example, sampling from the normal distribution is often made using <a href="https://en.wikipedia.org/wiki/Ziggurat_algorithm">Ziggurat method</a>, which uses a proposal distribution resembling stairs.</p>
<p>Rejection sampling is also commonly used for truncated distributions, in which case all draws from the truncated part are rejected.</p>
</section>
<section id="importance-sampling" class="level3">
<h3 class="anchored" data-anchor-id="importance-sampling">Importance sampling</h3>
<p>Popularity of importance sampling is increasing. It is used, for example, as part of other methods as particle filters and pseudo marginal likelihood approaches, and to improve distributional approximations (including variational inference in machine learning).</p>
<p>Importance sampling is useful in importance sampling leave-one-out cross-validation. Cross-validation is discussed in <a href="ch7">Chapter 7</a> and importance sampling leave-one-out cross-validation is discussed in the article</p>
<ul>
<li>Aki Vehtari, Andrew Gelman and Jonah Gabry (2016). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. In Statistics and Computing, 27(5):1413–1432. <a href="http://arxiv.org/abs/1507.04544">arXiv preprint arXiv:1507.04544</a>.</li>
</ul>
<p>After the book was published, we have developed Pareto smoothed importance sampling which is more stable than plain importance sampling and has very useful Pareto-<span class="math inline">\(\hat{k}\)</span> diagnostic to check the reliability</p>
<ul>
<li>Aki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, and Jonah Gabry (2024). Pareto smoothed importance sampling. <em>Journal of Machine Learning Research</em>, 25(72):1-58. <a href="https://jmlr.org/papers/v25/19-556.html">Online</a>.</li>
</ul>
</section>
<section id="importance-resampling-with-or-without-replacement" class="level3">
<h3 class="anchored" data-anchor-id="importance-resampling-with-or-without-replacement">Importance resampling with or without replacement</h3>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=276">BDA3 p.&nbsp;266</a> recommends importance resampling without replacement. At the time of writing that in 2013, we had less experience with importance sampling and there were some reasonable papers showing reduced variance doing resampling without replacement. We don’t recommend this anymore as Pareto smoothed importance sampling works better and is also applicable when the resample sample size is equal to the original sample size.</p>
</section>
<section id="importance-sampling-effective-sample-size" class="level3">
<h3 class="anchored" data-anchor-id="importance-sampling-effective-sample-size">Importance sampling effective sample size</h3>
<p>BDA3 1st (2013) and 2nd (2014) printing have an error for <span class="math inline">\(\tilde{w}(\theta^s)\)</span> used in the effective sample size equation 10.4. The normalized weights equation should not have the multiplier S (the normalized weights should sum to one). The effective sample size estimate mentioned in the book is generic approximation, and more accurate effective sample size estimate would take into account also the functional. For example, importance sampling effective sample size can be different when estimating <span class="math inline">\(\operatorname{E}[\theta]\)</span> or <span class="math inline">\(\operatorname{E}[\theta]^2\)</span>. If you are interested see more details, for example, <a href="https://jmlr.org/papers/v25/19-556.html">the Pareto smoothed importance sampling paper</a>.</p>
<p>The derivation for the effective sample size and Monte Carlo standard error (MCSE) for importance sampling can be found, for example, in <a href="https://statweb.stanford.edu/~owen/mc/">Chapter 9 of <em>Monte Carlo theory, methods and examples</em> by Art B. Owen</a>.</p>
</section>
<section id="buffons-needles" class="level3">
<h3 class="anchored" data-anchor-id="buffons-needles">Buffon’s needles</h3>
<p>Buffon’s needle is considered to be the first Monte Carlo style approach presented in 1777. It’s not known whether Buffon actually did the experiment in addition of describing the approach for estimating the value of <span class="math inline">\(\pi\)</span>. Check out <a href="https://mste.illinois.edu/activity/buffon/">a nice computer simulation of Buffon’s needle dropping method</a>.</p>
</section>
</section>
<section id="ch11" class="level2">
<h2 class="anchored" data-anchor-id="ch11">Chapter 11 Basics of Markov chain simulation</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=285">Chapter 11</a> is related to the Lecture 5 <em>Markov chain Monte Carlo</em>.</p>
<section id="outline-10" class="level3">
<h3 class="anchored" data-anchor-id="outline-10">Outline</h3>
<ul>
<li>Markov chain simulation: before Section 11.1, pages 275-276</li>
<li>11.1 Gibbs sampler (an example of simple MCMC method)</li>
<li>11.2 Metropolis and Metropolis-Hastings (an example of simple MCMC method)</li>
<li>11.3 Using Gibbs and Metropolis as building blocks (can be skipped)</li>
<li>11.4 Inference and assessing convergence (important)</li>
<li>11.5 Effective number of simulation draws (important)</li>
<li>11.6 Example: hierarchical normal model (skip this)</li>
</ul>
</section>
<section id="the-most-important-terms-10" class="level3">
<h3 class="anchored" data-anchor-id="the-most-important-terms-10">The most important terms</h3>
<p>Find all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.</p>
<ul>
<li>Markov chain</li>
<li>Markov chain Monte Carlo</li>
<li>random walk</li>
<li>starting point</li>
<li>transition distribution</li>
<li>jumping / proposal distribution</li>
<li>to converge, convergence, assessing convergence</li>
<li>stationary distribution, stationarity</li>
<li>effective number of simulations</li>
<li>Gibbs sampler</li>
<li>Metropolis sampling / algorithm</li>
<li>Metropolis-Hastings algorithm</li>
<li>acceptance / rejection rule</li>
<li>acceptance / rejection rate</li>
<li>within-sequence correlation, serial correlation</li>
<li>warm-up / burn-in</li>
<li>to thin, thinned</li>
<li>overdispersed starting points</li>
<li>mixing</li>
<li>to diagnose convergence</li>
<li>between- and within-sequence variances</li>
<li>potential scale reduction, <span class="math inline">\(\widehat{R}\)</span></li>
<li>the variance of the average of a correlated sequence</li>
<li>autocorrelation</li>
<li>variogram</li>
<li><span class="math inline">\(n_{\mathrm{eff}}\)</span> (we now prefer ESS / <span class="math inline">\(S_{\mathrm{eff}}\)</span>, which is used also in slides)</li>
</ul>
</section>
<section id="r-and-python" class="level3">
<h3 class="anchored" data-anchor-id="r-and-python">R and Python</h3>
<ul>
<li>11.1: Gibbs sampling. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch11/demo11_1.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch11/demo11_1.ipynb">Python</a>.</li>
<li>11.2: Metropolis sampling. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch11/demo11_2.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch11/demo11_2.ipynb">Python</a>.</li>
<li>11.3: Convergence of Markov chain. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch11/demo11_3.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch11/demo11_3.ipynb">Python</a>.</li>
<li>11.4: potential scale reduction <span class="math inline">\(\widehat{R}\)</span>. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch11/demo11_4.html">R</a>. <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_ch11/demo11_4.ipynb">Python</a>.</li>
</ul>
</section>
<section id="recommended-exercises-6" class="level3">
<h3 class="anchored" data-anchor-id="recommended-exercises-6">Recommended exercises</h3>
<p>Optional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:</p>
<ul>
<li><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=301">11.1</a> (<a href="http://www.stat.columbia.edu/~gelman/book/solutions3.pdf">model solution available for 11.1</a>)</li>
</ul>
</section>
<section id="basics-of-markov-chains" class="level3">
<h3 class="anchored" data-anchor-id="basics-of-markov-chains">Basics of Markov chains</h3>
<p><a href="http://www.netlab.tkk.fi/opetus/s38143/luennot/english.shtml">Slides by J. Virtamo for the course S-38.143 Queuing Theory</a> (<a href="http://www.netlab.hut.fi/opetus/s38143/luennot/index.shtml">Finnish version</a>) have a nice review of the fundamental terms. See specially the slides for the lecture 4. To prove that Metropolis algorithm works, it is sufficient to show that chain is irreducible, aperiodic and not transient.</p>
</section>
<section id="animations" class="level3">
<h3 class="anchored" data-anchor-id="animations">Animations</h3>
<ul>
<li><a href="http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/">Nice animations of some MCMC algorithms with discussion</a></li>
<li><a href="https://chi-feng.github.io/mcmc-demo/">And just the animations with more options to experiment</a></li>
</ul>
</section>
<section id="metropolis-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="metropolis-algorithm">Metropolis algorithm</h3>
<p>There is a lot of freedom in selection of proposal distribution in Metropolis algorithm. There are some restrictions, but we don’t go to the mathematical details in this course.</p>
<p>Don’t confuse rejection in the rejection sampling and in Metropolis algorithm. In the rejection sampling, the rejected samples are thrown away. In Metropolis algorithm the rejected proposals are thrown away, but time moves on and the previous sample <span class="math inline">\(x_t\)</span> is also the sample <span class="math inline">\(x_{t+1}\)</span>.</p>
<p>When rejecting a proposal, the previous sample is repeated in the chain, they have to be included and they are valid samples from the distribution. For basic Metropolis, it can be shown that optimal rejection rate is 55%–77%, so that on even the optimal case quite many of the samples are repeated samples. However, high number of rejections is acceptable as then the accepted proposals are on average further away from the previous point. It is better to jump further away 23%–45% of time than more often to jump really close. Methods for estimating the effective sample size are useful for measuring how effective a given chain is.</p>
</section>
<section id="transition-distribution-vs.-proposal-distribution" class="level3">
<h3 class="anchored" data-anchor-id="transition-distribution-vs.-proposal-distribution">Transition distribution vs.&nbsp;proposal distribution</h3>
<p>Transition distribution is a property of Markov chain. In Metropolis algorithm the transition distribution is a mixture of a proposal distribution and a point mass in the current point. The book uses also term jumping distribution to refer to proposal distribution.</p>
</section>
<section id="convergence" class="level3">
<h3 class="anchored" data-anchor-id="convergence">Convergence</h3>
<p>Theoretical convergence in an infinite time is different than practical convergence in a finite time. There is no exact moment when chain has converged and thus it is not possible to detect when the chain has converged (except for rare <em>perfect sampling</em> methods not discussed in BDA3). The convergence diagnostics can help to find out if the chain is unlikely to be representative of the target distribution. Furthermore, even if would be able to start from a independent sample from the posterior so that chain starts from the convergence, the mixing can be so slow that we may require very large number of samples before the samples are representative of the target distribution.</p>
<p>If starting point is selected at or near the mode, less time is needed to reach the area of essential mass, but still the samples in the beginning of the chain are not representative of the true distribution unless the starting point was somehow samples directly from the target distribution.</p>
</section>
<section id="widehatr-effective-sample-size-ess-previously-n_mathrmeff" class="level3">
<h3 class="anchored" data-anchor-id="widehatr-effective-sample-size-ess-previously-n_mathrmeff"><span class="math inline">\(\widehat{R}\)</span>, effective sample size (ESS, previously <span class="math inline">\(n_\mathrm{eff}\)</span>)</h3>
<p>There are many versions of <span class="math inline">\(\widehat{R}\)</span> and effective sample size. Beware that some software packages compute <span class="math inline">\(\widehat{R}\)</span> using old inferior approaches.</p>
<p>The <span class="math inline">\(\widehat{R}\)</span> and the approach to estimate effective sample size were updated in BDA3, and slightly updated version of this is described in Stan 2.18+ user guide. Since then we have developed even better <span class="math inline">\(\widehat{R}\)</span>, ESS (effective sample size with change from <span class="math inline">\(n_\mathrm{eff}\)</span> to ESS is due to improved consistency in the notation) in</p>
<ul>
<li>Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner (2020). Rank-normalization, folding, and localization: An improved <span class="math inline">\(\widehat{R}\)</span> for assessing convergence of MCMC. <em>Bayesian analysis</em>, <a href="https://projecteuclid.org/euclid.ba/1593828229">Online</a>.</li>
</ul>
<p>New <span class="math inline">\(\widehat{R}\)</span>, ESS, and Monte Carlo error estimates are available in RStan <code>monitor</code> function in R, in <code>posterior</code> package in R, and in ArviZ package in Python.</p>
<p>Due to randomness in chains, <span class="math inline">\(\widehat{R}\)</span> may get values slightly below 1.</p>
<p><a href="https://mc-stan.org/misc/warnings.html">Brief Guide to Stan’s Warnings</a> provides summary of available convergence diagnostics in Stan and how to interpret them.</p>
<p>Sometimes people write *the number of effective samples’’ which is wrong (it is possible that notation <span class="math inline">\(n_\mathrm{eff}\)</span> is partially to blame for this misconception). All the posterior draws in autocorrelated Markov chain are effective, but their efficiency for estimating an expectation depends on the autocorrelation. The effective sample size is not property of individual draws, but joint property of all draws in a sample. Effective sample size also depends on the functional and the effective sample size for a given dependent sample is often different when estimating, for example, <span class="math inline">\(\operatorname{E}[\theta]\)</span> or <span class="math inline">\(\operatorname{E}[\theta^2]\)</span>.</p>
</section>
</section>
<section id="ch12" class="level2">
<h2 class="anchored" data-anchor-id="ch12">Chapter 12 Computationally efficient Markov chain simulation</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=303">Chapter 12</a> is related to the Lecture 6 <em>Stan, HMC, PPL</em>.</p>
<section id="outline-11" class="level3">
<h3 class="anchored" data-anchor-id="outline-11">Outline</h3>
<ul>
<li>12.1 Efficient Gibbs samplers (not part of the course)</li>
<li>12.2 Efficient Metropolis jump rules (not part of the course)</li>
<li>12.3 Further extensions to Gibbs and Metropolis (not part of the course)</li>
<li>12.4 Hamiltonian Monte Carlo (used in Stan)</li>
<li>12.5 Hamiltonian dynamics for a simple hierarchical model (read through)</li>
<li>12.6 Stan: developing a computing environment (read through)</li>
</ul>
<p>For the BDA course, there are only 8 pages to read (Sections 12.4–12.6) what is inside Stan.</p>
</section>
<section id="r-and-python-demos-6" class="level3">
<h3 class="anchored" data-anchor-id="r-and-python-demos-6">R and Python demos</h3>
<p>g - 12.1: The No-U-Turn Sampler (NUTS) / Dynamic Hamiltonian Monte Carlo. <a href="https://avehtari.github.io/BDA_R_demos/demos_ch12/demo12_1.html">R</a>. - <a href="http://avehtari.github.io/BDA_R_demos/demos_rstan/cmdstanr_demo.html">CmdStanR Demos</a> - <a href="https://github.com/avehtari/BDA_py_demos/blob/master/demos_pystan/pystan_demo.ipynb">PyStan demos</a></p>
</section>
<section id="mcmc-animations" class="level3">
<h3 class="anchored" data-anchor-id="mcmc-animations">MCMC animations</h3>
<p>These don’t include the specific version of dynamic HMC in Stan, but are useful illustrations anyway.</p>
<ul>
<li><a href="https://avehtari.github.io/BDA_R_demos/demos_ch12/demo12_1.html">The No-U-Turn Sampler (NUTS) / Dynamic Hamiltonian Monte Carlo R demo</a></li>
<li><a href="http://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/">Markov Chains: Why Walk When You Can Flow?</a></li>
<li><a href="https://chi-feng.github.io/mcmc-demo/">MCMC animation site by Chi Feng</a></li>
</ul>
</section>
<section id="hamiltonian-monte-carlo" class="level3">
<h3 class="anchored" data-anchor-id="hamiltonian-monte-carlo">Hamiltonian Monte Carlo</h3>
<p>An excellent review of static HMC (the number of steps in dynamic simulation are not adaptively selected) is</p>
<ul>
<li>Radford Neal (2011). MCMC using Hamiltonian dynamics. In Brooks et al (ed), <em>Handbook of Markov Chain Monte Carlo</em>, Chapman &amp; Hall / CRC Press. <a href="https://arxiv.org/pdf/1206.1901.pdf">Preprint</a>.</li>
</ul>
<p>Stan uses a variant of dynamic Hamiltonian Monte Carlo (using adaptive number of steps in the dynamic simulation), which has been further developed since BDA3 was published. The first dynamic HMC variant was NUTS</p>
<ul>
<li>Matthew D. Hoffman, Andrew Gelman (2014). The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. <em>JMLR</em>, 15:1593–1623. <a href="http://jmlr.org/papers/v15/hoffman14a.html">Online</a>.</li>
</ul>
<p>The No-U-Turn Sampler gave the name NUTS which you can see often associated with Stan, but the current variant implemented in Stan has some further developments described (mostly) in</p>
<ul>
<li>Michael Betancourt (2018). A Conceptual Introduction to Hamiltonian Monte Carlo. <a href="https://arxiv.org/abs/1701.02434">arXiv preprint arXiv:1701.02434</a>.</li>
</ul>
</section>
<section id="divergences-and-bfmi" class="level3">
<h3 class="anchored" data-anchor-id="divergences-and-bfmi">Divergences and BFMI</h3>
<ul>
<li>Divergence diagnostic checks whether the discretized dynamic simulation has problems due to fast varying density. See more in <a href="http://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html">a Stan case study</a>.</li>
<li><a href="https://mc-stan.org/misc/warnings.html">Brief Guide to Stan’s Warnings</a> provides summary of available convergence diagnostics in Stan,d how to interpret them, and suggestions for solving sampling problems.</li>
</ul>
</section>
<section id="further-information-about-stan" class="level3">
<h3 class="anchored" data-anchor-id="further-information-about-stan">Further information about Stan</h3>
<ul>
<li><a href="http://mc-stan.org/">Stan web page</a></li>
<li><a href="http://mc-stan.org/documentation/">Stan Documentation</a></li>
<li>I recommend to start with these
<ul>
<li>Bob Carpenter, Andrew Gelman, Matt Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus A. Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell (2015) In press for Journal of Statistical Software. Stan: A Probabilistic Programming Language. <a href="http://www.stat.columbia.edu/~gelman/research/published/stan-paper-revision-feb2015.pdf">Preprint</a>.</li>
<li>Andrew Gelman, Daniel Lee, and Jiqiang Guo (2015) Stan: A probabilistic programming language for Bayesian inference and optimization. Journal of Educational and Behavior Science. <a href="http://www.stat.columbia.edu/~gelman/research/published/stan_jebs_2.pdf">Preprint</a>.</li>
<li><a href="https://www.youtube.com/playlist?list=PLuwyh42iHquU4hUBQs20hkBsKSMrp6H0J">Video of Basics of Bayesian inference and Stan tutorial by Jonah Gabry &amp; Lauren Kennedy</a></li>
</ul></li>
<li>More complete information (no need to read the from the beginning to end, but use when needed)
<ul>
<li><a href="https://mc-stan.org/docs/stan-users-guide/index.html">Stan User’s Guide</a></li>
<li><a href="https://mc-stan.org/docs/stan-users-guide/index.html">Stan Reference Manual</a></li>
<li><a href="https://mc-stan.org/docs/functions-reference/index.html">Stan Functions Reference</a></li>
<li><a href="https://mc-stan.org/cmdstanr/">CmdStanR documentation</a></li>
</ul></li>
</ul>
</section>
<section id="compiler-and-transpiler" class="level3">
<h3 class="anchored" data-anchor-id="compiler-and-transpiler">Compiler and transpiler</h3>
<p>This is a minor comment on the terminology. As a shorthand it’s common to see mentioned just Stan compiler, but sometimes the transpiler term is also mentioned as in the slides for this part.</p>
<p><a href="https://en.wikipedia.org/wiki/Source-to-source_compiler">A Wikipedia article</a> states: <em>A source-to-source translator, source-to-source compiler (S2S compiler), transcompiler, or transpiler is a type of translator that takes the source code of a program written in a programming language as its input and produces an equivalent source code in the same or a different programming language. A source-to-source translator converts between programming languages that operate at approximately the same level of abstraction, while a traditional compiler translates from a higher level programming language to a lower level programming language.</em></p>
<p>So it is more accurate to say that the Stan model code is first transpiled to a C++ code, and then that C++ code is compiled to machine code to create an executable program. Cool thing about <a href="https://github.com/stan-dev/stanc3">the new stanc3 transpiler</a> is that it can create also, for example, LLVM IR or TensorFlow code.</p>
<p>Using transpiler and compiler allows to develop Stan language to be good for writing models, but get the benefit of speed and external libraries of C++, TensorFlow, and whatever comes in the future.</p>
</section>
</section>
<section id="chapter-13-modal-and-distributional-approximations" class="level2">
<h2 class="anchored" data-anchor-id="chapter-13-modal-and-distributional-approximations">Chapter 13 Modal and distributional approximations</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=321">Chapter 13</a> is not part of the BDA Aalto course.</p>
<p><a href="ch4">Chapter 4</a> presented normal distribution approximation at the mode (aka Laplace approximation). Chapter 13 discusses more about distributional approximations.</p>
<section id="outline-12" class="level3">
<h3 class="anchored" data-anchor-id="outline-12">Outline</h3>
<ul>
<li>Finding posterior modes
<ul>
<li>Newton’s method is very fast if the distribution is close to normal and the computation of the second derivatives is fast</li>
<li>Stan uses limited-memory Broyden-Fletcher-Goldfarb-Shannon (L-BFGS) which is a quasi-Newton method which needs only the first derivatives (provided by Stan autodiff). L-BFGS is known for good performance for wide variety of functions.</li>
</ul></li>
<li>Boundary-avoiding priors for modal summaries
<ul>
<li>Although full integration is preferred, sometimes optimization of some parameters may be sufficient and faster, and then boundary-avoiding priors maybe useful.</li>
</ul></li>
<li>Normal and related mixture approximations
<ul>
<li>Discusses how the normal approximation can be used to approximate integrals of a a smooth function times the posterior.</li>
<li>Discusses mixture and <span class="math inline">\(t\)</span> approximations.</li>
</ul></li>
<li>Finding marginal posterior modes using EM
<ul>
<li>Expectation maximization is less important in the time of efficient probabilistic programming frameworks, but can be sometimes useful for extra efficiency.</li>
</ul></li>
<li>Conditional and marginal posterior approximations
<ul>
<li>Even in the time of efficient probabilistic programming, the methods discussed in this section can produce very big speedups for a big set of commonly used models. The methods discussed are important part of popular INLA software and are coming also to Stan to speedup latent Gaussian variable models.</li>
</ul></li>
<li>Example: hierarchical normal model</li>
<li>Variational inference
<ul>
<li>Variational inference (VI) is very popular in machine learning, and this section presents it in terms of BDA. Auto-diff variational inference in Stan was developed after BDA3 was published.</li>
</ul></li>
<li>Expectation propagation
<ul>
<li>Practical efficient computation for expectation propagation (EP) is applicable for more limited set of models than post-BDA3 black-box VI, but for those models EP provides better posterior approximation. Variants of EP can be used for parallelization of any Bayesian computation for hierarchical models.</li>
</ul></li>
<li>Other approximations
<ul>
<li>Just briefly mentions of INLA (in 13.5), CCD (deterministic adaptive quadrature approach) and ABC (inference when you can only sample from the generative model).</li>
</ul></li>
<li>Unknown normalizing factors
<ul>
<li>Often the normalizing factor is not needed, but it can be estimated using importance, bridge or path sampling.</li>
</ul></li>
</ul>
</section>
</section>
<section id="ch14" class="level2">
<h2 class="anchored" data-anchor-id="ch14">Chapter 14 Introduction to regression models</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=363">Chapter 14</a> is not part of the BDA Aalto course.</p>
<section id="outline-13" class="level3">
<h3 class="anchored" data-anchor-id="outline-13">Outline</h3>
<ul>
<li>Conditional modeling
<ul>
<li>formal justification of conditional modeling</li>
<li>if joint model factorizes <span class="math inline">\(p(y,x|\theta,\phi)=p(y|x,\theta)}p(x|\phi)\)</span><br>
we can model just <span class="math inline">\(p(y|x,\theta)}\)</span></li>
</ul></li>
<li>Bayesian analysis of classical regression
<ul>
<li>uninformative prior on <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span></li>
<li>connection to multivariate normal (cf.&nbsp;Chapter 3) is useful to understand as it then reveals what would be the conjugate prior</li>
<li>closed form posterior and posterior predictive distribution</li>
<li>these properties are sometimes useful and thus good to know, but with probabilistic programming less often needed</li>
</ul></li>
<li>Regression for causal inference: incumbency and voting
<ul>
<li>Modeling example with bit of discussion on causal inference (<a href="https://avehtari.github.io/ROS-Examples/">see more in Regression and Other Stories (ROS)</a> Chs. 18-21)</li>
</ul></li>
<li>Goals of regression analysis
<ul>
<li>discussion of what we can do with regression analysis (see more in <a href="https://avehtari.github.io/ROS-Examples/">ROS</a>)</li>
</ul></li>
<li>Assembling the matrix of explanatory variables
<ul>
<li>transformations, nonlinear relations, indicator variables, interactions (see more in <a href="https://avehtari.github.io/ROS-Examples/">ROS</a>)</li>
</ul></li>
<li>Regularization and dimension reduction
<ul>
<li>a bit outdated and short (Bayesian Lasso is not a good idea), see more in Lecture 9.3.</li>
</ul></li>
<li>Unequal variances and correlations
<ul>
<li>useful concept, but computation is easier with probabilistic programming frameworks</li>
</ul></li>
<li>Including numerical prior information
<ul>
<li>useful conceptually, but easy computation with probabilistic programming frameworks makes it easier to define prior information as the prior doesn’t need to be conjugate</li>
<li>see more about priors in <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Prior Choice Recommendations Wiki</a></li>
</ul></li>
</ul>
</section>
</section>
<section id="chapter-15-hierarchical-linear-models" class="level2">
<h2 class="anchored" data-anchor-id="chapter-15-hierarchical-linear-models">Chapter 15 Hierarchical linear models</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=391">Chapter 15</a> is not part of the BDA Aalto course.</p>
<p>Chapter 15 combines hierarchical models from <a href="ch5">Chapter 5</a> and linear models from <a href="ch14">Chapter 14</a>. The chapter discusses some computational issues, but probabilistic programming frameworks make computation for hierarchical linear models easy.</p>
<section id="outline-14" class="level3">
<h3 class="anchored" data-anchor-id="outline-14">Outline</h3>
<ul>
<li>Regression coefficients exchangeable in batches
<ul>
<li>exchangeability of parameters</li>
<li>the discussion of fixed-, random- and mixed-effects models is incomplete
<ul>
<li>we don’t recommend using these terms, but they are so popular that it’s useful to know them</li>
<li>a relevant comment is <em>The terms ‘fixed’ and ‘random’ come from the non-Bayesian statistical tradition and are somewhat confusing in a Bayesian context where all unknown parameters are treated as ‘random’ or, equivalently, as having fixed but unknown values.</em></li>
<li>often fixed effects correspond to population level coefficients, random effects correspond to group or individual level coefficients, and mixed model has both ——————————- —————————————– <code>y \sim 1 + x</code> fixed / population effect; pooled model <code>y \sim 1 + (0 + x | g)</code> random / group effects <code>y \sim 1 + x + (1 + x | g)</code> mixed effects; hierarchical model ——————————- —————————————–</li>
</ul></li>
</ul></li>
<li>Example: forecasting U.S. presidential elections
<ul>
<li>illustrative example</li>
</ul></li>
<li>Interpreting a normal prior distribution as extra data
<ul>
<li>includes very useful interpretation of hierarchical linear model as a single linear model with certain design matrix</li>
</ul></li>
<li>Varying intercepts and slopes
<ul>
<li>extends from hierarchical model for scalar parameter to joint hierarchical model for several parameters</li>
</ul></li>
<li>Computation: batching and transformation
<ul>
<li>Gibbs sampling part is mostly outdated</li>
<li>transformations for HMC is useful if you write your own models, but the section is quite short and you can get more information from Stan user guide 21.7 Reparameterization and <a href="https://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html">Divergences case study</a></li>
</ul></li>
<li>Analysis of variance and the batching of coefficients
<ul>
<li>ANOVA as Bayesian hierarchical linear model</li>
<li>rstanarm and brms packages make it easy to make ANOVA</li>
</ul></li>
<li>Hierarchical models for batches of variance components
<ul>
<li>more variance components</li>
</ul></li>
</ul>
</section>
</section>
<section id="chapter-16-generalized-linear-models" class="level2">
<h2 class="anchored" data-anchor-id="chapter-16-generalized-linear-models">Chapter 16 Generalized linear models</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=415">Chapter 16</a> is not part of the BDA Aalto course.</p>
<p>Chapter 16 extends linear models to have non-normal observation models. Model in Bioassay example in <a href="ch3">Chapter 3</a> is also generalized linear model. Chapter reviews the basics and discusses some computational issues, but probabilistic programming frameworks make computation for generalized linear models easy (especially with rstanarm and brms). See <a href="https://avehtari.github.io/ROS-Examples/">ROS</a>) for discussion of generalized linear models from the modeling perspective more thoroughly.</p>
<section id="outline-15" class="level3">
<h3 class="anchored" data-anchor-id="outline-15">Outline</h3>
<ul>
<li>Parts of generalized linear model (GLM):
<ul>
<li>The linear predictor <span class="math inline">\(\eta = X\beta\)</span></li>
<li>The link function <span class="math inline">\(g(\cdot)\)</span> and <span class="math inline">\(\mu = g^{-1}(\eta)\)</span></li>
<li>Outcome distribution model with location parameter <span class="math inline">\(\mu\)</span>
<ul>
<li>the distribution can also depend on dispersion parameter <span class="math inline">\(\phi\)</span></li>
<li>originally just exponential family distributions (e.g. Poisson, binomial, negative-binomial), which all have natural location-dispersion parameterization</li>
<li>after MCMC made computation easy, GLM can refer to models where outcome distribution is not part of exponential family and dispersion parameter may have its own latent linear predictor</li>
</ul></li>
</ul></li>
<li>Standard generalized linear model likelihoods
<ul>
<li>section title says “likelihoods”, but it would be better to say “observation models”</li>
<li>continuous data: normal, gamma, Weibull mentioned, but common are also Student’s <span class="math inline">\(t\)</span>, log-normal, log-logistic, and various extreme value distributions like generalized Pareto distribution</li>
<li>binomial (Bernoulli as a special case) for binary and count data with upper limit
<ul>
<li>Bioassay model uses binomial observation model</li>
</ul></li>
<li>Poisson for count data with no upper limit
<ul>
<li>Poisson is useful approximation of Binomial when the observed counts are much smaller than the upper limit</li>
</ul></li>
</ul></li>
<li>Working with generalized linear models
<ul>
<li>bit of this and that information on how think about GLMs (see <a href="https://avehtari.github.io/ROS-Examples/">ROS</a> for more)</li>
<li>normal approximation to the likelihood is good for thinking how much information non-normal observations provide, can be useful for someone thinking about computation, but easy computation with probabilistic programming frameworks means not everyone needs this</li>
</ul></li>
<li>Weakly informative priors for logistic regression
<ul>
<li>an excellent section although the recommendation on using Cauchy has changed (see <a href="https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations">Prior Choice Recommendations Wiki</a>)</li>
<li>the problem of separation is useful to understand</li>
<li>computation part is outdated as probabilistic programming frameworks make the computation easy</li>
</ul></li>
<li>Overdispersed Poisson regression for police stops
<ul>
<li>an example</li>
</ul></li>
<li>State-level opinions from national polls
<ul>
<li>another example</li>
</ul></li>
<li>Models for multivariate and multinomial responses
<ul>
<li>extension to multivariate responses</li>
<li>polychotomous data with multivariate binomial or Poisson</li>
<li>models for ordered categories</li>
</ul></li>
<li>Loglinear models for multivariate discrete data
<ul>
<li>multinomial or Poisson as loglinear models</li>
</ul></li>
</ul>
</section>
</section>
<section id="chapter-17-models-for-robust-inference" class="level2">
<h2 class="anchored" data-anchor-id="chapter-17-models-for-robust-inference">Chapter 17 Models for robust inference</h2>
<p><a href="https://users.aalto.fi/~ave/BDA3.pdf#page=445">Chapter 17</a> is not part of the BDA Aalto course.</p>
<p>Chapter 17 discusses over-dispersed observation models. The discussion is useful beyond generalized linear models. The computation is outdated. See <a href="https://avehtari.github.io/ROS-Examples/">ROS</a> for more examples.</p>
<section id="outline-16" class="level3">
<h3 class="anchored" data-anchor-id="outline-16">Outline</h3>
<ul>
<li>Aspects of robustness
<ul>
<li>overdispersed models are often connected to robustness of inferences to outliers, but the observed data can be overdispersed without any observation being outlier</li>
<li>outlier is sensible only in the context of the model, being something not well modeled or something requiring extra model component</li>
<li>switching to generic overdispersed model can help to recognize problem in the non-robust model (sensitivity analysis), but it can also throw away useful information in the “outliers” and it would be useful to think what is the generative mechanism for observations which are not like others</li>
</ul></li>
<li>Overdispersed versions of standard models:
<ul>
<li>normal <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(t\)</span>-distribution</li>
<li>Poisson <span class="math inline">\(\rightarrow\)</span> negative-binomial</li>
<li>binomial <span class="math inline">\(\rightarrow\)</span> beta-binomial</li>
<li>probit <span class="math inline">\(\rightarrow\)</span> logistic / robit</li>
</ul></li>
<li>Posterior inference and computation
<ul>
<li>computation part is outdated as probabilistic programming frameworks and MCMC make the computation easy</li>
<li>posterior is more likely to be multimodal</li>
</ul></li>
<li>Robust inference for the eight schools
<ul>
<li>eight schools example is too small too see much difference</li>
</ul></li>
<li>Robust regression using t-distributed errors
<ul>
<li>computation part is outdated as probabilistic programming frameworks and MCMC make the computation easy</li>
<li>posterior is more likely to be multimodal</li>
</ul></li>
</ul>
</section>
</section>
<section id="chapter-18-models-for-missing-data" class="level2">
<h2 class="anchored" data-anchor-id="chapter-18-models-for-missing-data">Chapter 18 Models for missing data</h2>
<p>Chapter 18 is not part of the BDA Aalto course.</p>
<p>Chapter 18 extends the data collection modeling from Chapter 8. See <a href="https://avehtari.github.io/ROS-Examples/">ROS</a> for more examples.</p>
<section id="outline-17" class="level3">
<h3 class="anchored" data-anchor-id="outline-17">Outline</h3>
<ul>
<li>Notation
<ul>
<li>Missing completely at random (MCAR) missingness does not depend on missing values or other observed values (including covariates)</li>
<li>Missing at random (MAR)<br>
missingness does not depend on missing values but may depend on other observed values (including covariates)</li>
<li>Missing not at random (MNAR)<br>
missingness depends on missing values</li>
</ul></li>
<li>Multiple imputation
<ul>
<li>make a model predicting missing data</li>
<li>sample repeatedly from the missing data model to generate multiple imputed data sets</li>
<li>make usual inference for each imputed data set</li>
<li>combine results</li>
<li>discussion of computation is partially outdated</li>
</ul></li>
<li>Missing data in the multivariate normal and <span class="math inline">\(t\)</span> models
<ul>
<li>a special continuous data case computation, which can still be useful as fast starting point</li>
</ul></li>
<li>Example: multiple imputation for a series of polls
<ul>
<li>an example</li>
</ul></li>
<li>Missing values with counted data
<ul>
<li>discussion of computation for count data (ie. computation in 18.3 is not applicable)</li>
</ul></li>
<li>Example: an opinion poll in Slovenia
<ul>
<li>another example</li>
</ul></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/avehtari\.github\.io\/BDA_course_Aalto\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>