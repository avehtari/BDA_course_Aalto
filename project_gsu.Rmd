---
title: "Bayesian Data Analysis course - Project work (GSU)"
date: "Page updated: `r format.Date(file.mtime('project_gsu.Rmd'),'%Y-%m-%d')`"
---

Bayesian Data Analysis Global South (GSU) 2021

## Project work details

Project work involves choosing a data set and performing a whole
analysis according to all the parts of Bayesian workflow studied along
the course. In this course instance there are no project
presentations, but you will get feedback from your peers. You can do
the project work in groups if you like.

## Project schedule

- See [the overall schedule of the GSU 2021 course](gsu2021.html#Schedule_2021)
- Start working on the project in midway of the course.
- Project report deadline 21.5. Submit in peergrade (separate "class", the class code will be posted in the course chat announcements).
- Project report peer grading 24-26.5. 

## Groups

In this course instance the project work can be done in groups of 1-4
persons, but you don't need to find a group.

If you don't have a group, you can ask other students in the group
chat channel **#project**. Tell what kind of data you are interested
in (e.g. medicine, health, biological, engineering, political,
business), whether you prefer R or Python, and whether you have
already more concrete idea for the topic.

## Evaluation

In this course instanve the project work's evaluation consists of only from 

- peergraded project report (40%) (within peergrade submission 80% and feedback 20%)

## Project report

In the project report you practice presenting the problem and data
analysis results, which means that minimal listing of code and figures
is not a good report. There are different levels for how data analysis
project could be reported. This report should be more than a summary
of results without workflow steps. While describing the steps and
decisions made during the workflow, to keep the report readable some
of the diagnostic outputs and code can be put in the appendix. If you
are uncertain you can ask TAs in TA sessions whether you are on a good
level of amount of details.

The report should include

  1. Introduction describing
      - the motivation
      - the problem
      - and the main modeling idea.
      - Showing some illustrative figure is recommended.
  1. Description of the data and the analysis problem. Provide information where the data was obtained, and if it has been previously used in some online case study and how your analysis differs from the existing analyses.
  1. Description of at least two models, for example:
      - non hierarchical and hierarchical,
      - linear and non linear,
      - variable selection with many models.
  1. Informative or weakly informative priors, and justification of their
    choices.
  1. The model code (Stan, brms, rstanarm, PyMC3).
  1. How to the inference for models was run, that is, what options were used. This is also more clear as combination of textual explanation and the actual code line.
  1. Convergence diagnostics ($\widehat{R}$, ESS, divergences) and what was done if the convergence was not good with the first try.
  1. Posterior predictive checks and what was done to improve the model.
  1. Model comparison (e.g. with LOO-CV).
  1. Predictive performance assessment if applicable (e.g. classification
    accuracy) and evaluation of practical usefulness of the accuracy.
  1. Sensitivity analysis with respect to prior choices (i.e. checking whether the result changes a lot if prior is changed)
  1. Discussion of issues and potential improvements.
  1. Conclusion what was learned from the data analysis.
  1. Self-reflection of what the you/group learned while making the project.

## Data sets

As some data sets have been overused for these particular goals, note that the
following ones are forbidden in this work (more can be added to this list so
make sure to check it regularly):

  - R data sets titanic and mtcars
  - Baseball batting (used by Bob Carpenter's StanCon case study).
  - Data sets used in the course demos

It's best to use a dataset for which there is no ready made analysis in internet, but if you choose a dataset used already in some online case study, provide the link to previous studies and report how your analysis differs from those (for example if someone has made non-Bayesian analysis and you do the full Bayesian analysis).

Depending on the model and the structure of the data, a good data set would have more than 100 observations but less than 1 million. If you know an interesting big data set, you can use a smaller subset of the data to keep the computation times feasible. It would be good that the data has some structure, so that it is sensible to use multilevel/hierarchical models.

## Model requirements

  - Every parameter needs to have an explicit proper prior. Improper flat priors
    are not allowed.
  - A hierarchical model is a model where the prior of certain parameter
    contain other parameters that are also estimated in the model. For instance,
    `b ~ normal(mu, sigma)`, `mu ~ normal(0, 1)`, `sigma ~ exponential(1)`.
  - Do not impose hard constrains on a parameter unless they are natural to
    them. `uniform(a, b)` should not be used unless the boundaries are really logical boundaries and values beyond the boundaries are completely impossible.
  - At least some models should include covariates. Modelling the outcome without predictors
    is likely too simple for the project.

## Some examples

The following case study examples demonstrate how text, equations, figures, and code, and inference results can be included in one report. These examples don't necessarily have all the workflow steps required in your report, but different steps are illustrated in different case studies and you can get good ideas for your report just by browsing through them.

  - [BDA R and Python demos](demos.html) are quite minimal in description of the data and discussion of the results, but show many diagnostics and basic plots.
  - Some [Stan case studies](https://mc-stan.org/users/documentation/case-studies) focus on some specific methods, but there are many case studies that are excellent examples for this course. They don't include all the steps required in this course, but are good examples of writing. Some of them are longer or use more advanced models than required in this course.
      - [Bayesian workflow for disease transmission modeling in Stan](https://mc-stan.org/users/documentation/case-studies/boarding_school_case_study.html)
      - [Model-based Inference for Causal Effects in Completely Randomized Experments](https://mc-stan.org/users/documentation/case-studies/model-based_causal_inference_for_RCT.html)
      - [Tagging Basketball Events with HMM in Stan](https://mc-stan.org/users/documentation/case-studies/bball-hmm.html)
      - [Model building and expansion for golf putting](https://mc-stan.org/users/documentation/case-studies/golf.html)
      - [A Dyadic Item Response Theory Model](https://mc-stan.org/users/documentation/case-studies/dyadic_irt_model.html)
      - [Predator-Prey Population Dynamics:
the Lotka-Volterra model in Stan](https://mc-stan.org/users/documentation/case-studies/lotka-volterra-predator-prey.html)
  - Some [StanCon case studies](https://github.com/stan-dev/stancon_talks) (scroll down) can also provide good project ideas.
