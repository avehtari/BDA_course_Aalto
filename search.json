[
  {
    "objectID": "ta_info_gsu.html",
    "href": "ta_info_gsu.html",
    "title": "Bayesian Data Analysis course - Assignments (GSU)",
    "section": "",
    "text": "Bayesian Data Analysis Global South (GSU) 2023\nSome TA instructions\nPlease ask more information and clarifications.\nI’ll choose the students on 18th February Finland time. I’ll email them with starting instructions."
  },
  {
    "objectID": "ta_info_gsu.html#assignments",
    "href": "ta_info_gsu.html#assignments",
    "title": "Bayesian Data Analysis course - Assignments (GSU)",
    "section": "Assignments",
    "text": "Assignments\nThere are 9 assignments and project.\nAssignments are peergraded so you don’t need to do grading.\nStudents can submit to peergrade when they want, but if they submit before the deadlines mentioned in the schedule, then they get their submission peergraded by other students. Each student peergrades 3 other students.\nIf you want limit your time usage for the course, choose one or more assignments and be active before deadlines for those assignments. Read that assignment and solve it in your mind or even better solve it completely so you know how to help. Instead of assignments you can choose also to focus on answering R, Python or Stan questions. Add your course slack id in a sheet (link provided in staff channel) so we can see that each assignment gets enough TA love."
  },
  {
    "objectID": "ta_info_gsu.html#communication-with-students",
    "href": "ta_info_gsu.html#communication-with-students",
    "title": "Bayesian Data Analysis course - Assignments (GSU)",
    "section": "Communication with students",
    "text": "Communication with students\nThe students are instructed not to send you direct messages (unless you sometimes give a permission to someone).\nThere are a separate channels for each assignment, R, Python, and Stan. Students may ask questions in the channels and other students and TAs can answer. If you see good answers by other students or TAs give a thumb up, as I can later collect these and thank active students and TAs. If you notice frequently asked questions it’s good to inform me (so I can add it to the course material) and other TAs (so they can find the answer quickly).\nIn addition of answering questions in general chat, it is very helpful to have sometimes - one-on-one slack chat with direct messages to not reveal too much how solve the assignment - one-on-one video call with screen sharing etc. For these there is a #queue channel. You can in self-organized way tell when you would be available for one-on-one questions and then students can in queue channel ask for help and you start answering questions in the queue order. When you start the chat mark the request with talking head emoji, so other TAs know it’s being handled. When you finish answering the question mark it with green tick mark so other know it’s done. It can be helpful to have some times when more than one TA is handling the queue, but since you are in different time zones I don’t know yet how to organize this else than by self organization and some polls for students. If you notice frequently asked questions it’s good to inform me (so I can add it to the course material) and other TAs (so they can find the answer quickly).\nIn addition of answering questions in general chat and one-on-one chats, if there are questions many students would like to hear answers but you think itäs faster by live discussion instead of writing it’s possible to also have video calls with several students once.\nI also try to have some Q&A sessions on usually confusing topics and make video recordings if there are good new questions."
  },
  {
    "objectID": "ta_info_gsu.html#project-work",
    "href": "ta_info_gsu.html#project-work",
    "title": "Bayesian Data Analysis course - Assignments (GSU)",
    "section": "Project work",
    "text": "Project work\nIn Aalto course the project work has been only Stan. In this course I think it’s fine to use any common PPL as we have TAs wth experience on many frameworks. There will be more information on project work later."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Bayesian Data Analysis course - Project work",
    "section": "",
    "text": "Project work involves choosing a data set and performing a whole analysis according to all the parts of Bayesian workflow studied along the course.\n\nThe project work is meant to be done in period II.\nIn the beginning of the period II\n\nForm a group. We prefer groups of 2–3, but the project can be done alone.\nSelect a topic. You may ask in the course chat channel #project for opinion whether it’s a good topic and a good dataset. You can change the topic later.\nStart planning.\n\nThe main work for the project and the presentation will be done in the second half of the period II after all the workflow parts have been discussed in the course.\nThe in person presentations will be made on the evaluation week after period II.\nUse of AI is allowed on the course, but the most of the work needs to by the students, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching). We have tested some AI on the course topics and assignments and the output can be copy of existing text without attribution (ie. plagiarism), vague or have mistakes, so you need to be careful when using such outputs.\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof."
  },
  {
    "objectID": "project.html#project-work-details",
    "href": "project.html#project-work-details",
    "title": "Bayesian Data Analysis course - Project work",
    "section": "",
    "text": "Project work involves choosing a data set and performing a whole analysis according to all the parts of Bayesian workflow studied along the course.\n\nThe project work is meant to be done in period II.\nIn the beginning of the period II\n\nForm a group. We prefer groups of 2–3, but the project can be done alone.\nSelect a topic. You may ask in the course chat channel #project for opinion whether it’s a good topic and a good dataset. You can change the topic later.\nStart planning.\n\nThe main work for the project and the presentation will be done in the second half of the period II after all the workflow parts have been discussed in the course.\nThe in person presentations will be made on the evaluation week after period II.\nUse of AI is allowed on the course, but the most of the work needs to by the students, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching). We have tested some AI on the course topics and assignments and the output can be copy of existing text without attribution (ie. plagiarism), vague or have mistakes, so you need to be careful when using such outputs.\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof."
  },
  {
    "objectID": "project.html#project-schedule",
    "href": "project.html#project-schedule",
    "title": "Bayesian Data Analysis course - Project work",
    "section": "Project schedule",
    "text": "Project schedule\n\nForm a group and pick a topic. Register the group before end of 7th Nov, 2023. The registration will open soon.\nGroups of 3 can reserve a presentation slot starting 7th Nov, 2024.\nGroups of 1-2 can reserve a presentation slot starting 8th Nov, 2024.\nGroups that register late can reserve a presentation slot starting 9th Nov, 2024. \nWork on the project. TA session queue is also for project questions.\nProject report deadline 1.12.. Submit in FeedbackFruits (the link will be added to MyCourses).\nProject report peer grading 1. - 6.12. (so that you’ll get feedback for the report before the presentations).\nProject presentations 9.-13.12.."
  },
  {
    "objectID": "project.html#groups",
    "href": "project.html#groups",
    "title": "Bayesian Data Analysis course - Project work",
    "section": "Groups",
    "text": "Groups\nProject work is done in groups of 1-3 persons. Preferred group size is 3, because you learn more when you talk about the project with someone else.\nIf you don’t have a group, you can ask other students in the group chat channel #project. Tell what kind of data you are interested in (e.g. medicine, health, biological, engineering, political, business), whether you prefer R or Python, and whether you have already more concrete idea for the topic.\nGroups of 3 students can choose their presentation time slot before 1-2 student groups. 3 person group is expected to do a bit more work than 1-2 person groups.\nYou can do the project alone, but the amount of work is expected to the same for 2 person groups."
  },
  {
    "objectID": "project.html#ta-sessions",
    "href": "project.html#ta-sessions",
    "title": "Bayesian Data Analysis course - Project work",
    "section": "TA sessions",
    "text": "TA sessions\nThe groups will get help for the project work in TA sessions. When there are no weekly assignments, the TA sessions are still organized for helping in the project work."
  },
  {
    "objectID": "project.html#evaluation",
    "href": "project.html#evaluation",
    "title": "Bayesian Data Analysis course - Project work",
    "section": "Evaluation",
    "text": "Evaluation\nThe project work’s evaluation consists of\n\npeergraded project report (30%) (within FeedbackFruits submission 90% and feedback 10%)\npresentation and oral exam graded by the course staff (70%)\n\nclarity of slides + use of figures\nclarity of oral presentation + flow of the presentation\nall required parts included (not necessarily all in main slides, but it needs to be clear that all required steps were performed)\naccuracy of use of terms (oral exam)\nresponses to questions (oral exam)"
  },
  {
    "objectID": "project.html#project-report",
    "href": "project.html#project-report",
    "title": "Bayesian Data Analysis course - Project work",
    "section": "Project report",
    "text": "Project report\nIn the project report you practice presenting the problem and data analysis results, which means that minimal listing of code and figures is not a good report. There are different levels for how data analysis project could be reported. This report should be more than a summary of results without workflow steps. While describing the steps and decisions made during the workflow, to keep the report readable some of the diagnostic outputs and code can be put in the appendix. If you are uncertain you can ask TAs in TA sessions whether you are on a good level of amount of details.\nThe report should not be over 20 pages. The report can be much shorter than 20 pages, but as figures ca take a lot of space, the upper limit is quite high. There is a upper limit, so that you need to do at least some selection of what to show and specifically don’t include all possible output from the inference. If you are uncertain whether your report is containing sufficient information, ask TAs.\nThe report should include\n\nIntroduction describing\n\nthe motivation\nthe problem\nthe main modeling idea\nIn addition showing some illustrative figure is recommended.\n\nDescription of the data and the analysis problem. Provide information where the data was obtained, and if it has been previously used in some online case study and how your analysis differs from the existing analyses.\nDescription of at least two models, for example:\n\nnon hierarchical and hierarchical,\nlinear and non linear,\ndifferent plausible observation models,\nvariable selection with many models.\n\nInformative or weakly informative priors, and justification of the choice of these priors.\nbrms, rstanarm, or Stan code.\nHow the MCMC inference was run, that is, what options were used. A good option is to show the command you did run, and a textual explanation of the choice of options.\nConvergence diagnostic (\\(\\widehat{R}\\), ESS, divergences) value and what can be interpreted from them. What was done if the convergence was not good with the first try. This should be reported for all models.\nPosterior predictive checks and what can be interpreted from them. What was done to improve the model if the checks indicated misspecification. This should be reported for all models.\nOptional/Bonus: Predictive performance assessment if applicable (e.g. classification accuracy) and evaluation of practical usefulness of the accuracy. This should be reported for all models as well.\nSensitivity analysis with respect to prior choices (i.e. checking whether the result changes a lot if prior is changed). This should be reported for all models.\nModel comparison (e.g. with LOO-CV).\nDiscussion of issues and potential improvements.\nConclusion what was learned from the data analysis.\nSelf-reflection of what the group learned while making the project.\n\nYou can check also the FeedbackFruits rubric for the project report.\nFor guidance on how many digits to report, see Digits notebook."
  },
  {
    "objectID": "project.html#project-presentation",
    "href": "project.html#project-presentation",
    "title": "Bayesian Data Analysis course - Project work",
    "section": "Project presentation",
    "text": "Project presentation\nIn addition to the submitted report, each project must be presented by the authoring group, according to the following guidelines:\n\nThe presentation should be high level but sufficiently detailed information should be readily available to help answering questions from the audience.\nThe duration of the presentation should be 10 minutes (groups of 1-2 students) or 15 minutes (groups of 3 students).\nAt the end of the presentation there will be an extra 5-10 minutes of questions by anyone in the audience or two members of the course staff who are present. The questions from lecturer/TAs can be considered as an oral exam questions, and if answers to these questions reveal weak knowledge of the methods and workflow steps which should be part of the project, that can reduce the grade.\nGrading will be done by the two members of the course staff using standardized grading instructions.\n\nSpecific recommendations for the presentations include:\n\nThe first slide should include project’s title and group members’ names.\nThe chosen statistical model(s), including observation model and priors, must be explained and justified,\nMake sure the font is big enough to be easily readable by the audience. This includes figure captions, legends and axis information,\nThe last slide should be a summary or take-home-messages and include contact information or link to a further information. (The grade will be reduced by one if the last slide has only something like “Thank you” or “Questions?”),\nIn general, the best presentations are often given by teams that have frequently attended TA sessions and gotten feedback, so we strongly recommend attending these sessions.\n\nMore details on the presentation sessions in 2024\n\nPresentations will be given on campus \nIf you reserved a presentation slot but need to cancel, do it ASAP via the reservation link on MyCourses. If you cancel after the reservation link has closed, send a message to David Kohns (TA) on Zulip.\nAs we have many presentation in each slot join the meeting in time. Late arrivals will lower the grade. Very late arrivals will fail the presentation and can present in period III.\nIt is easiest if just one from the group shares the slides, but it is expected that all group members present some part of the presentation orally.\nPresentation time is 10 min for 1-2 person groups and 15min for 3 person groups\nTime limit is strict. It’s good idea to practice the talk so that you get the timing right. Staff will announce 2min and 1min left and time ended. Going overtime reduces the grade.\nAfter the presentation there will be 5min for questions, answers, and feedback.\nEach student has to come up with at least one question during the session. Students can ask more questions.\nStaff will ask further questions (kind of oral exam)\nGrading of the project presentation takes into account following things in the order of importance. You can get feedback for all parts.\n\nStudents show understanding of essential concepts covered in the course\n\nTAs will ask clarifying questions to check these\n\nInclusion of required workflow steps\nClarity of the problem, model and prior description\nFocus of the presentation\n\nfocus on the most relevant points in the limited time\n\nOverall clarity and coherence\n\nhow well the slides and presentation flows\neffective presentation of the results\n\nSlide design and visual aspects\n\ne.g. use of figures, font size, number of digits\neffective use of figures\n\nLack of typos, grammatical and spelling errors\nOral\n\nclarity of oral presentation, rhythm and timing, rate of speech\nclarity of voice projection and appropriate volume\npresenting style created interest and was engaging\npresence (e.g. speaking to the audience / camera)\npossible distracting mannerisms\n\nnote: this has minor effect, and involuntary aspects (e.g. ticks) do not affect grading\nTiming\n\npoint reduction if overtime\nfair amount of time for all group members\n\nBonus: going beyond the expectations\n\nyou may get a bonus point for going beyond the explicit requirements\n\n\nStudents will also self-evaluate their project. Instructions for this will provided."
  },
  {
    "objectID": "project.html#data-sets",
    "href": "project.html#data-sets",
    "title": "Bayesian Data Analysis course - Project work",
    "section": "Data sets",
    "text": "Data sets\nAs some data sets have been overused for these particular goals, note that the following ones are forbidden in this work (more can be added to this list so make sure to check it regularly):\n\nextremely common data sets like titanic, mtcars, iris, penguins (Palmer Archipelago, n=344)\nBaseball batting (used by Bob Carpenter’s StanCon case study).\nData sets used in the course demos like bodyfat, diabetes,\nData sets used in other CS courses\n\nIt’s best to use a dataset for which there is no ready made analysis in internet, but if you choose a dataset used already in some online case study, provide the link to previous studies and report how your analysis differs from those (for example if someone has made non-Bayesian analysis and you do the full Bayesian analysis).\nDepending on the model and the structure of the data, a good data set would have more than 100 observations but less than 1 million. If you know an interesting big data set, you can use a smaller subset of the data to keep the computation times feasible. It would be good that the data has some structure, so that it is sensible to use multilevel/hierarchical models. If you are uncertain, ask TAs.\nHere are some public datasets with plenty of choices:\n\nMedical datasets\nVanderbilt Biostatistics datasets\nEU data\nInter-university Consortium for Political and Social Research\nWHO mortality data\nThe World Bank Data\nA long list of public datasets grouped by topic\nR datasets"
  },
  {
    "objectID": "project.html#model-requirements",
    "href": "project.html#model-requirements",
    "title": "Bayesian Data Analysis course - Project work",
    "section": "Model requirements",
    "text": "Model requirements\n\nEvery parameter needs to have an explicit proper prior. Improper flat priors are not allowed.\nA hierarchical model is a model where the prior of certain parameter contain other parameters that are also estimated in the model. For instance, b ~ normal(mu, sigma), mu ~ normal(0, 1), sigma ~ exponential(1).\nDo not impose hard constrains on a parameter unless they are natural to them. uniform(a, b) should not be used unless the boundaries are really logical boundaries and values beyond the boundaries are completely impossible.\nAt least some models should include covariates. Modelling the outcome without predictors is likely too simple for the project.\nbrms or rstanarm is recommended (less time spent for coding the model). brms uses improper flat prior for many parameters, so you need to define proper priors. Report also the proper default priors which you didn’t change."
  },
  {
    "objectID": "project.html#some-examples",
    "href": "project.html#some-examples",
    "title": "Bayesian Data Analysis course - Project work",
    "section": "Some examples",
    "text": "Some examples\nThe following case study examples demonstrate how text, equations, figures, and code, and inference results can be included in one report. These examples don’t necessarily have all the workflow steps required in your report, but different steps are illustrated in different case studies and you can get good ideas for your report just by browsing through them.\n\nBDA R and Python demos are quite minimal in description of the data and discussion of the results, but show many diagnostics and basic plots.\nSome Stan case studies focus on some specific methods, but there are many case studies that are excellent examples for this course. They don’t include all the steps required in this course, but are good examples of writing. Some of them are longer or use more advanced models than required in this course.\n\nBayesian workflow for disease transmission modeling in Stan\nModel-based Inference for Causal Effects in Completely Randomized Experiments\nTagging Basketball Events with HMM in Stan\nModel building and expansion for golf putting\nA Dyadic Item Response Theory Model\nPredator-Prey Population Dynamics: the Lotka-Volterra model in Stan\nHierarchical model for motivational shifts in aging monkeys\n\nSome StanCon case studies (scroll down) can also provide good project ideas."
  },
  {
    "objectID": "gsu2023.html",
    "href": "gsu2023.html",
    "title": "Bayesian Data Analysis course - GSU 2023",
    "section": "",
    "text": "Bayesian Data Analysis Global South (GSU) 2023"
  },
  {
    "objectID": "gsu2023.html#summary",
    "href": "gsu2023.html#summary",
    "title": "Bayesian Data Analysis course - GSU 2023",
    "section": "Summary",
    "text": "Summary\n\nMax 300 students with priority for global south and other underrepresented groups (GSU).\nFrom 18th February (first assignment deadline 26th February, last day of the course in the end of May)\nAll the material (textbook, videos, assignments, extra reading material) are freely available (see below) so you can also self-study at your own pace.\nThe course is free (no cost) and possible to organize with help of volunteer TAs.\nThis BDA course instance is aimed to support learning with peer support. By following the videos and doing assignments at the same time with others, you can discuss the material in assignments in the course slack, there is peer-grading platform to get feedback about your assignment solutions, and voluntary TAs help answering questions. As everything is volunteer based we can’t guarantee quick responses, but at least you will get something more than when studying only by yourself.\nThis course is not the easiest Bayesian course available in internet, but it can be your first Bayesian course if your mathematical and programming skills are sufficient. Compared to some other free online courses, this course has big emphasis in the computational methods, and thus goes deeper in many parts. See the prerequisites below. For easier material to start with see the end of Prerequisites section below.\nYou will not get a formal certificate for passing the course from Aalto University.\nThe communication happens in the course slack, please don’t email the lecturer or TAs. The slack link has been emailed to the accepted students.\n\nAll the course material is available in a git repo and via these pages for easier navigation. All the material can be used in other courses. Text and videos licensed under CC-BY-NC 4.0. Code licensed under BSD-3."
  },
  {
    "objectID": "gsu2023.html#registration",
    "href": "gsu2023.html#registration",
    "title": "Bayesian Data Analysis course - GSU 2023",
    "section": "Registration",
    "text": "Registration\nYou can apply for the course by filling a registration form. The registration form asks several pre-requisites questions, and filling the form takes some time. We will email whether you have been accepted to the course at latest 17th February (Finland time). If you are not accepted for the course, you can still self-study all the material."
  },
  {
    "objectID": "gsu2023.html#ta-registration",
    "href": "gsu2023.html#ta-registration",
    "title": "Bayesian Data Analysis course - GSU 2023",
    "section": "TA Registration",
    "text": "TA Registration\nWe have some volunteer TAs already, but a few more would be great. All TAs will get a personal certificate from the lecturer Aki Vehtari if they actively participate helping students, answering questions, and possibly organize some TA sessions. We assume to have enough TAs that no-one needs to take part every week of the course and you can drop out if other obligations require so. The lecturer and two head TAs will support TAs.\nTo register as volunteer TA fill in your information (email, country, prerequisites check, a brief comment on justification why you think you can be a TA) in TA registration form."
  },
  {
    "objectID": "gsu2023.html#book-bda3",
    "href": "gsu2023.html#book-bda3",
    "title": "Bayesian Data Analysis course - GSU 2023",
    "section": "Book: BDA3",
    "text": "Book: BDA3\n\n\n\nThe electronic version of the course book Bayesian Data Analysis, 3rd ed, by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin is available for non-commercial purposes. Hard copies are available from the publisher and many book stores. See also home page for the book, errata for the book, and chapter notes."
  },
  {
    "objectID": "gsu2023.html#prerequisites",
    "href": "gsu2023.html#prerequisites",
    "title": "Bayesian Data Analysis course - GSU 2023",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic terms of probability theory\n\nprobability, probability density, distribution\nsum, product rule, and Bayes’ rule\nexpectation, mean, variance, median\nin English, see e.g. Wikipedia and Introduction to probability and statistics\n\nSome algebra and calculus\nBasic visualisation techniques (R or Python)\n\nhistogram, density plot, scatter plot\nsee e.g. BDA R demos\nsee e.g. BDA Python demos\n\n\nThis course has been designed so that there is strong emphasis in computational aspects of Bayesian data analysis and using the latest computational tools.\nIf you find BDA3 too difficult to start with, I recommend\n\nFor regression models, their connection to statistical testing and causal analysis see Gelman, Hill and Vehtari, “Regression and Other Stories”.\nRichard McElreath’s Statistical Rethinking, 2nd ed book is easier than BDA3 and the 2nd ed is excellent, but doesn’t go as deep in computational methods as this BDA course. Richard’s lecture videos of Statistical Rethinking: A Bayesian Course Using R and Stan are highly recommended even if you are following BDA3.\nAlicia A. Johnson, Miles Q. Ott, and Mine Dogucu Bayes Rules! An Introduction to Applied Bayesian Modeling book is excellent and easier than BDA3, but doesn’t go as deep in computational methods as this BDA course.\nFor background prerequisites some students have found chapters 2, 4 and 5 in Kruschke, “Doing Bayesian Data Analysis” useful."
  },
  {
    "objectID": "gsu2023.html#communication-channels",
    "href": "gsu2023.html#communication-channels",
    "title": "Bayesian Data Analysis course - GSU 2023",
    "section": "Communication channels",
    "text": "Communication channels\n\nThe primary communication channel is the course slack. The slack link has been emailed to the accepted students.\n\nDon’t ask via email or direct messages. By asking via common channels in the course chat, more eyes will see your question, it will get answered faster and it’s likely that other students benefit from the answer.\nIn the chat system, there are separate channels for each assignment and the project.\nChannel #general can be used for any kind of general discussions and questions related to the course.\nAll important announcements will be posted to #announcements (no discussion on this channel).\nAny kind of feedback is welcome on channel #feedback.\nWe have also channels #r, #python, and #stan for questions that are not specific to assignments or the project.\nThe lecturer and teaching assistants have names with “(staff)” or “(TA)” in the end of their names.\n\nIn this course instance, most TAs are volunteers from different time zones, but there may be possibility for live “TA sessions” with TAs depending on the volunteers\nIf you find errors in material, post in #feedback channel or submit an issue in github.\nPeergrade alerts: If you are worried that you forget the deadlines, you can set Peergrade to send you email when assignment opens for submission, 24 hours before assignment close for submission, assignment is open for reviewing, 24 hours before an assignment closes for reviewing if you haven’t started yet, someone likes my feedback (once a day). Click your name -&gt; User Settings to choose which alerts you want."
  },
  {
    "objectID": "gsu2023.html#assessment",
    "href": "gsu2023.html#assessment",
    "title": "Bayesian Data Analysis course - GSU 2023",
    "section": "Assessment",
    "text": "Assessment\nAssignments (67%) and a project work with presentation (33%). Minimum of 50% of points must be obtained from both the assignments and project work. But as in this course there is no formal certificate, the assignment scores are just for your own self-evaluation. The biggest benefit from the course is the support and feedback from other students and volunteer TAs.\nWe use peergrade.io for providing peer feedback for the assignments and the project work. See more information at Assignments."
  },
  {
    "objectID": "gsu2023.html#schedule-2023",
    "href": "gsu2023.html#schedule-2023",
    "title": "Bayesian Data Analysis course - GSU 2023",
    "section": "Schedule 2023",
    "text": "Schedule 2023\nThe course consists of 12 blocks from February to May 2023. The blocks don’t match exactly specific weeks. For example, it’s good start reading the material for the next block while making the assignment for one block. There are 9 assignments and a project work with presentation, and thus the assignments are not in one-to-one correspondence with the blocks. The schedule below lists the blocks and how they connect to the topics, book chapters and assignments.\n\nSchedule overview\nHere is an overview of the schedule. Scroll down the page to see detailed instructions for each block. Remember that blocks are overlapping so that when you are working on assignment for one block, you should start watching videos and reading text for the next block.\nNote: All assignment deadlines are on Monday 15:59 UTC+0\n[Link to Schedule of Assignments]\n\n\n\n\nBlock\nReadings\nLectures\nAssignment\n\nAssignment due date\n\n\n\n\n\n1. Introduction\nBDA3 Chapter 1\nComputational probabilistic modeling,  Introduction to uncertainty and modelling,  Introduction to the course contents\nAssignment 1 and Rubric questions\n\n26 February\n\n\n\n2. Basics of Bayesian inference\nBDA3 Chapter 1,  BDA3 Chapter 2\nLecture 2.1,  Lecture 2.2,  Optional:  Extra explanations 2,  Summary 2.1,  Summary 2.2\nAssignment 2 and Rubric questions\n\n6 March\n\n\n\n3. Multidimensional posterior\nBDA3 Chapter 3\nLecture 3\nAssignment 3 and Rubric questions\n\n13 March\n\n\n\n4. Monte Carlo\nBDA3 Chapter 10\nLecture 4.1,  Lecture 4.2\nAssignment 4 and Rubric questions\n\n20 March\n\n\n\n5. Markov chain Monte Carlo\nBDA3 Chapter 11\nLecture 5.1,  Lecture 5.2\nAssignment 5 and Rubric questions\n\n27 March\n\n\n\n6. Stan, HMC, PPL\nBDA3 Chapter 12 + extra material on Stan\nLecture 6.1, Lecture 6.2\nAssignment 6 and Rubric questions\n\n7 April\n\n\n\n7. Hierarchical models and exchangeability\nBDA3 Chapter 5\nLecture 7.1,  Lecture 7.2\nAssignment 7 and Rubric questions\n\n17 April\n\n\n\n8. Model checking & cross-validation\nBDA3 Chapter 6, BDA3 Chapter 7, Visualization in Bayesian workflow, Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC\nLecture 8.1,  Lecture 8.2\nStart project work\n\n\n\n\n\n9. Model comparison and selection\nBDA3 Chapter 7 (not 7.2 and 7.3),  Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC\nLecture 9.1,  Optional:  Lecture 9.2,  Lecture 9.3\nAssignment 8 and Rubric questions\n\n1 May\n\n\n\n10. Decision analysis\nBDA3 Chapter 9\nLecture 10.1\nAssignment 9 and Rubric questions\n\n8 May\n\n\n\n11. Normal approximation, frequency properties\nBDA3 Chapter 4\nLecture 11.1,  Lecture 11.2\n\n\n\n\n\n\n12. Extended topics\nOptional: BDA3 Chapter 8,  BDA3 Chapter 14-18,  BDA3 Chapter 21\nOptional:  Lecture 12.1,  Lecture 12.2\nProject work\n\n15 May\n\n\n\n13. Project feedback\n\n\nProject feedback\n\n22 May\n\n\n\n\n\n1) Course introduction, BDA 3 Ch 1, prerequisites assignment\nCourse practicalities, material, assignments, project work, peergrading, TA sessions, prerequisites, chat, etc.\n\nLogin to the course chat (link will be emailed to the registered students and volunteer TAs)\nSign in to Peergrade with the class code shared in email\nWatch videos\n\nComputational probabilistic modeling (15min)\nIntroduction to uncertainty and modelling (48min)\nIntroduction to the course contents (8min)\nSlides /- Read BDA3 Chapter 1\nstart with reading instructions for Chapter 1 and afterwards read the additional comments in the same document\n\nThere are no R/Python demos for Chapter 1\nMake and submit Assignment 1. Deadline Sunday 26 February 23:59 UTC+0\n\nThis assignment checks that you have sufficient prerequisite skills (basic probability calculus, and R or Python)\n\nRubric questions used in peergrading are included above\n\nGeneral information about assignments\n\nR markdown template for assignments\nFAQ for the assignments has solutions to commonly asked questions related RStudio setup, errors during package installations, etc.\n\n\nOptional: Make BDA3 exercises 1.1-1.4, 1.6-1.8 (model solutions available for 1.1-1.6)\nStart reading Chapters 1+2, see instructions below\n\n\n\n2) BDA3 Ch 1+2, basics of Bayesian inference\nBDA3 Chapters 1+2, basics of Bayesian inference, observation model, likelihood, posterior and binomial model, predictive distribution and benefit of integration, priors and prior information, and one parameter normal model.\n\nRead BDA3 Chapter 2\n\nsee reading instructions for Chapter 2\n\nWatch videos Lecture 2.1 and Lecture 2.2 on basics of Bayesian inference, observation model, likelihood, posterior and binomial model, predictive distribution and benefit of integration, priors and prior information, and one parameter normal model. BDA3 Ch 1+2.\n\nExtra explanations about likelihood, normalization term, density, and conditioning on model M\nSlides\n\nOptional summary videos:\n\n2.1 Observation model, likelihood, posterior and binomial model\n2.2 Predictive distribution and benefit of integration\n\nRead the additional comments for Chapter 2\nCheck R demos or Python demos for Chapter 2\nMake and submit Assignment 2. Deadline Monday 6 March 15:59 UTC+0\n\nRubric questions used in peergrading are included above\nReview Assignment 1 done by your peers before 15:59 UTC+0 4 March\nReflect on your feedback\n\nOptional: Make BDA3 exercises 2.1-2.5, 2.8, 2.9, 2.14, 2.17, 2.22 (model solutions available for 2.1-2.5, 2.7-2.13, 2.16, 2.17, 2.20, and 2.14 is in course slides)\nStart reading Chapter 3, see instructions below\n\n\n\n3) BDA3 Ch 3, multidimensional posterior\nMultiparameter models, joint, marginal and conditional distribution, normal model, bioassay example, grid sampling and grid evaluation. BDA3 Ch 3.\n\nRead BDA3 Chapter 3\n\nsee reading instructions for Chapter 3\n\nWatch Lecture 3 on multiparameter models, joint, marginal and conditional distribution, normal model, bioassay example, grid sampling and grid evaluation. BDA3 Ch 3.\n\nSlides\n\nRead the additional comments for Chapter 3\nCheck R demos or Python demos for Chapter 3\nMake and submit Assignment 3. Deadline Monday 13 March 15:59 UTC+0\n\nRubric questions used in peergrading for Assignment 3\nReview Assignment 2 done by your peers before 15:59 UTC+0 11 March, and reflect on your feedback\n\nOptional: Make BDA3 exercises 3.2, 3.3, 3.9 (model solutions available for 3.1-3.3, 3.5, 3.9, 3.10)\nStart reading Chapter 10, see instructions below\n\n\n\n4) BDA3 Ch 10, Monte Carlo\nNumerical issues, Monte Carlo, how many simulation draws are needed, how many digits to report, direct simulation, curse of dimensionality, rejection sampling, and importance sampling. BDA3 Ch 10.\n\nRead BDA3 Chapter 10\n\nsee reading instructions for Chapter 10\n\nWatch Lecture 4.1 on numerical issues, Monte Carlo, how many simulation draws are needed, how many digits to report, and Lecture 4.2 on direct simulation, curse of dimensionality, rejection sampling, and importance sampling. BDA3 Ch 10.\n\nSlides\n\nRead the additional comments for Chapter 10\nCheck R demos or Python demos for Chapter 10\nMake and submit Assignment 4. Deadline Monday 20 March 15:59 UTC+0\n\nRubric questions used in peergrading for Assignment 4\nReview Assignment 3 done by your peers before 15:59 UTC+0 18 March, and reflect on your feedback\n\nOptional: Make BDA3 exercises 10.1, 10.2 (model solution available for 10.4)\nStart reading Chapter 11, see instructions below\n\n\n\n5) BDA3 Ch 11, Markov chain Monte Carlo\nMarkov chain Monte Carlo, Gibbs sampling, Metropolis algorithm, warm-up, convergence diagnostics, R-hat, and effective sample size. BDA3 Ch 11.\n\nRead BDA3 Chapter 11\n\nsee reading instructions for Chapter 11\n\nWatch Lecture 5.1 on Markov chain Monte Carlo, Gibbs sampling, Metropolis algorithm, and Lecture 5.2 on warm-up, convergence diagnostics, R-hat, and effective sample size. BDA3 Ch 11.\n\nSlides\n\nRead the additional comments for Chapter 11\nCheck R demos or Python demos for Chapter 11\nMake and submit Assignment 5. Deadline Monday 27 March 15:59 UTC+0\n\nRubric questions used in peergrading for Assignment 5\nReview Assignment 4 done by your peers before 15:59 UTC+0 25 March, and reflect on your feedback\n\nOptional: Make BDA3 exercise 11.1 (model solution available for 11.1)\nStart reading Chapter 12 + Stan material, see instructions below\n\n\n\n6) BDA3 Ch 12 + Stan, HMC, PPL, Stan\nHMC, NUTS, dynamic HMC and HMC specific convergence diagnostics, probabilistic programming and Stan. BDA3 Ch 12 + extra material\n\nRead BDA3 Chapter 12\n\nsee reading instructions for Chapter 12\n\nWatch Lecture 6.1 on HMC, NUTS, dynamic HMC and HMC specific convergence diagnostics, and Lecture 6.2 on probabilistic programming and Stan. BDA3 Ch 12 + extra material.\n\nSlides\nOptional: Stan Extra introduction recorded 2020 Golf putting example, main features of Stan, benefits of probabilistic programming, and comparison to some other software.\n\nRead the additional comments for Chapter 12\nRead Stan introduction article\nCheck R demos for RStan or Python demos for PyStan\nAdditional material for Stan:\n\nDocumentation\nRStan installation\nPyStan installation\nBasics of Bayesian inference and Stan, Jonah Gabry & Lauren Kennedy Part 1 and Part 2\n\nMake and submit Assignment 6. Deadline Monday 3 April 15:59 UTC+0\n\nRubric questions used in peergrading for Assignment 6\nReview Assignment 5 done by your peers before 15:59 UTC+0 1 April, and reflect on your feedback\n\nStart reading Chapter 5 + Stan material, see instructions below\n\n\n\n7) BDA3 Ch 5, hierarchical models\nHierarchical models and exchangeability. BDA3 Ch 5.\n\nRead BDA3 Chapter 5\n\nsee reading instructions for Chapter 5\n\nWatch Lecture 7.1 on hierarchical models, and Lecture 7.2 on exchangeability. BDA3 Ch 5.\n\nSlides\n\nRead the additional comments for Chapter 5\nCheck R demos or Python demos for Chapter 5\nMake and submit Assignment 7. Deadline Monday 17 April 21:59 UTC+0\n\nRubric questions used in peergrading for Assignment 7\nReview Assignment 6 done by your peers before 21:59 UTC+0 16 April, and reflect on your feedback\n\nOptional: Make BDA3 exercises 5.1 and 5.1 (model solution available for 5.3-5.5, 5.7-5.12)\nStart reading Chapters 6-7 and additional material, see instructions below.\n\n\n\n8) BDA3 Ch 6+7 + extra material, model checking, cross-validation\nModel checking and cross-validation.\n\nRead BDA3 Chapters 6 and 7 (skip 7.2 and 7.3)\n\nsee reading instructions for Chapter 6 and Chapter 7\n\nRead Visualization in Bayesian workflow\n\nmore about workflow and examples of prior predictive checking and LOO-CV probability integral transformations\n\nRead Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC (Journal link)\n\nreplaces BDA3 Sections 7.2 and 7.3 on cross-validation\n\nWatch Lecture 8.1 on model checking, and Lecture 8.2 on cross-validation part 1. BDA3 Ch 6-7 + extra material.\n\nSlides\n\nRead the additional comments for Chapter 6 and Chapter 7\nCheck R demos or Python demos for Chapter 6\nAdditional reading material\n\nModel selection\nCross-validation FAQ\n\nNo new assignment in this block\nStart the project work\nOptional: Make BDA3 exercise 6.1 (model solution available for 5.3-5.5, 5.7-5.12)\n\n\n\n9) BDA3 Ch 7, extra material, model comparison and selection\nPSIS-LOO, K-fold-CV, model comparison and selection. Extra lecture on variable selection with projection predictive variable selection.\n\nRead Chapter 7 (no 7.2 and 7.3)\n\nsee reading instructions for Chapter 7\n\nRead Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC (Journal link)\n\nreplaces BDA3 Sections 7.2 and 7.3 on cross-validation\n\nWatch Lecture 9.1 PSIS-LOO and K-fold-CV.\nOptional: Lecture 9.2 model comparison and selection.\n\nSlides\n\nAdditional reading material\n\nModel selection\nCross-validation FAQ\n\nMake and submit Assignment 8. Deadline Monday 01 May 21:59 UTC+0\n\nRubric questions used in peergrading for Assignment 8\nReview Assignment 7 done by your peers before 21:59 UTC+0 29 April, and reflect on your feedback\n\nStart reading Chapter 9, see instructions below.\n\n\n\n10) BDA3 Ch 9, decision analysis\nDecision analysis. BDA3 Ch 9.\n\nRead Chapter 9\n\nsee reading instructions for Chapter 9\n\nWatch Lecture 10.1 on decision analysis. BDA3 Ch 9.\n\nSlides\n\nProject presentation info will be updated soon.\nMake and submit Assignment 9. Deadline Monday 8 May 21:59 UTC+0\n\nRubric questions used in peergrading for Assignment 9\nReview Assignment 8 done by your peers before 21:59 UTC+0 6 May, and reflect on your feedback\n\nStart reading Chapter 4, see instructions below.\n\n\n\n11) BDA3 Ch 4 + extra material, normal approximation, frequency properties\nNormal approximation (Laplace approximation), and large sample theory and counter examples. BDA3 Ch 4.\n\nRead Chapter 4\n\nsee reading instructions for Chapter 4\n\nWatch Lecture 11.1 on normal approximation (Laplace approximation) and Lecture 11.2 on large sample theory and counter examples. BDA3 Ch 4.\n\nSlides\n\nNo new assignment. Work on project.\n\nReview Assignment 9 done by your peers before 21:59 UTC+0 13 May, and reflect on your feedback\n\n\n\n\n12) extra material + overview of BDA3 Ch 8, 14-18, 21\nFrequency evaluation of Bayesian methods, hypothesis testing and variable selection. Overview of modeling data collection, BDA3 Ch 8, linear models, BDA Ch 14-18, lasso, horseshoe and Gaussian processes, BDA3 Ch 21.\n\nThese lectures are optional, but especially the lecture on hypothesis testing and variable selection is useful for project work.\nWatch Lecture 12.1 on frequency evaluation, hypothesis testing and variable selection and Lecture 12.2 overview of modeling data collection, BDA3 Ch 8, linear models, BDA Ch 14-18, lasso, horseshoe and Gaussian processes, BDA3 Ch 21.\n\nSlides\n\nWork on project. TAs help with projects. Project deadline 15 May 21:59 UTC+0\n\n\n\n13) Project evaluation\n\nProject report deadline 7 May 21:59 UTC+0 (submit to peergrade).\n\nReview project reports done by your peers before 22 May 15:59 UTC+0, and reflect on your feedback"
  },
  {
    "objectID": "gsu2023.html#r-and-python",
    "href": "gsu2023.html#r-and-python",
    "title": "Bayesian Data Analysis course - GSU 2023",
    "section": "R and Python",
    "text": "R and Python\nWe strongly recommend using R in the course as there are more packages for Stan and statistical analysis in R. If you are already fluent in Python, but not in R, then using Python may be easier, but it can still be more useful to learn also R. Unless you are already experienced and have figured out your preferred way to work with R, we recommend\n\ninstalling RStudio Desktop,\n\nSee FAQ for frequently asked questions about R problems in this course. The demo codes provide useful starting points for all the assignments.\n\nFor learning R programming basics\n\nGarrett Grolemund, Hands-On Programming with R\n\nFor learning basic and advanced plotting using R\n\nKieran Healy, Data Visualization - A practical introduction\nAntony Unwin, Graphical Data Analysis with R"
  },
  {
    "objectID": "assignments_gsu.html",
    "href": "assignments_gsu.html",
    "title": "Bayesian Data Analysis course - Assignments (GSU)",
    "section": "",
    "text": "Bayesian Data Analysis Global South (GSU) 2023\nYou are free to use these assignments in self study and other courses (CC-BY-NC 4.0), but please do not publish complete answers online.\nIf you are a student on this course, you are allowed to discuss assignments with your friends, but it is not allowed to copy solutions directly from other students, from the internet, or from large language models. You can copy, e.g., plotting code from the course demos, but really try to solve the actual assignment problems with your own code and explanations. Do not share your answers publicly."
  },
  {
    "objectID": "assignments_gsu.html#schedule",
    "href": "assignments_gsu.html#schedule",
    "title": "Bayesian Data Analysis course - Assignments (GSU)",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\nBlock\nReadings\nLectures\nAssignment\n\nAssignment due date\n\n\n\n\n\n1. Introduction\nBDA3 Chapter 1\nComputational probabilistic modeling,  Introduction to uncertainty and modelling,  Introduction to the course contents\nAssignment 1 and Rubric questions\n\n26 February\n\n\n\n2. Basics of Bayesian inference\nBDA3 Chapter 1,  BDA3 Chapter 2\nLecture 2.1,  Lecture 2.2,  Optional:  Extra explanations 2,  Summary 2.1,  Summary 2.2\nAssignment 2 and Rubric questions\n\n6 March\n\n\n\n3. Multidimensional posterior\nBDA3 Chapter 3\nLecture 3\nAssignment 3 and Rubric questions\n\n13 March\n\n\n\n4. Monte Carlo\nBDA3 Chapter 10\nLecture 4.1,  Lecture 4.2\nAssignment 4 and Rubric questions\n\n20 March\n\n\n\n5. Markov chain Monte Carlo\nBDA3 Chapter 11\nLecture 5.1,  Lecture 5.2\nAssignment 5 and Rubric questions\n\n27 March\n\n\n\n6. Stan, HMC, PPL\nBDA3 Chapter 12 + extra material on Stan\nLecture 6.1, Lecture 6.2\nAssignment 6 and Rubric questions\n\n3 April\n\n\n\n7. Hierarchical models and exchangeability\nBDA3 Chapter 5\nLecture 7.1,  Lecture 7.2\nAssignment 7 and Rubric questions\n\n17 April\n\n\n\n8. Model checking & cross-validation\nBDA3 Chapter 6, BDA3 Chapter 7, Visualization in Bayesian workflow, Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC\nLecture 8.1,  Lecture 8.2\nStart project work\n\n\n\n\n\n9. Model comparison and selection\nBDA3 Chapter 7 (not 7.2 and 7.3),  Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC\nLecture 9.1,  Optional:  Lecture 9.2,  Lecture 9.3\nAssignment 8 and Rubric questions\n\n1 May\n\n\n\n10. Decision analysis\nBDA3 Chapter 9\nLecture 10.1\nAssignment 9 and Rubric questions\n\n8 May\n\n\n\n11. Normal approximation, frequency properties\nBDA3 Chapter 4\nLecture 11.1,  Lecture 11.2\n\n\n\n\n\n\n12. Extended topics\nOptional: BDA3 Chapter 8,  BDA3 Chapter 14-18,  BDA3 Chapter 21\nOptional:  Lecture 12.1,  Lecture 12.2\nProject work\n\n15 May\n\n\n\n13. Project feedback\n\n\nProject feedback\n\n22 May"
  },
  {
    "objectID": "assignments_gsu.html#weekly-assignments",
    "href": "assignments_gsu.html#weekly-assignments",
    "title": "Bayesian Data Analysis course - Assignments (GSU)",
    "section": "Weekly assignments",
    "text": "Weekly assignments\nThere are 9 weekly assignments. Assignments are linked from the schedule and can also be found from git repo assignments folder. The deadline days for the assignments are given in the course schedule.\n\nThere are chat channels #assignment1 etc. you can ask questions about the assignments. Other students and TAs can answer these. There is no guaranteed response time. These channels are best for questions that are likely to have relatively simple answer.\nThis course instance is volunteer based, but we try to organize some TA sessions for getting extra help. These sessions are useful if you think you need help that requires a bit more discussion. The questions are answered during the TA session time (if there are too many questions, they may be answered in the chat or next TA session).\nYou can use R markdown template for the report. You can also use any other software, but please follow the outline and ideas in the template also available as PDF\nStudents return their answers to peergrade by the end of Sunday (hand-in period). The deadlines are on Sundays 23:59 (UTC+3). We can’t accept late submissions due to the peergrading process.\nAfter this, each student reviews 3 random other students’ answers and provides feedback via online rubric form in peergrade during Monday (actually the feedback opens on Saturday, but I recommend to take weekends off) to Wednesday 23:59 (peer grading period). By reviewing other report you will see the correct answers in the rubrics, see others answers, and learn more actively by providing feedback for others.\nIf you happen to get empty or almost empty report to review, check first if it is better when opened in external pdf reader. If the report is still empty or almost empty, mark the submission as problematic and you will get another report to review.\nAfter peergrading, each student reflects on the feedback (reactions, e.g. not helpful/helpful).\nIf a student receives inappropriate feedback/grading or reaction, they may “flag” it for TAs to check from Wednesday to Sunday (flagging period). In this course instance there is no formal certificate for passing the course, so there is no need to flag lightly, but you can use flagging to report clear misbehavior and we can remove misbehaving students from the system.\nPeergrade alerts: If you are worried that you forget the deadlines, you can set peergrade to send you email when assignment opens for submission, 24 hours before assignment close for submission, assignment is open for reviewing, 24 hours before an assignment closes for reviewing if you haven’t started yet, someone likes my feedback (once a day). Click your name -&gt; User Settings to choose which alerts you want.\n\nReport all results in a single, anonymous *.pdf -file and submit it in peergrade. Submitting empty or almost empty pdf files is not allowed. Include also any source code to the report (either in appendix or embedded in the answer). By anonymity it is meant that the report should not contain your name or email. In addition to the correctness of the answers, the overall quality and clearness of the report is also evaluated.\nThe assignments are mostly solved using computer (R or Python). Related demos for each assignment are available in the course web pages (links in Materials section).\nOverall recommendation for many students is to include more text explanations as writing that text is active way of learning, and also it’s easier to see if you have understood the concepts correctly. If you make minimal reports, you will also get minimal feedback and you learn less. The main purpose of the course is to learn, and learning is more effective if you do it in active and interactive way."
  },
  {
    "objectID": "assignments_gsu.html#peergrade",
    "href": "assignments_gsu.html#peergrade",
    "title": "Bayesian Data Analysis course - Assignments (GSU)",
    "section": "Peergrade",
    "text": "Peergrade\nRegister to peergrade with your email. The class code needed for registration will be posted to all registered students before the course starts."
  },
  {
    "objectID": "assignments_gsu.html#peergrading-feedback",
    "href": "assignments_gsu.html#peergrading-feedback",
    "title": "Bayesian Data Analysis course - Assignments (GSU)",
    "section": "Peergrading / feedback",
    "text": "Peergrading / feedback\nPeergrading is called giving feedback in peergrade.io. The peergrading is guided by the rubric questions which are visible in peergrade and in the schedule. Giving feedback is good term as it is not meant to be just grading wrong vs. correct answers. Please provide also feedback on the presentation. Part of the course is practicing making data analysis reports. By providing feedback on the report presentation other students can learn what they can improve or what they already did well. You should be able to provide constructive or positive feedback for all non-empty reports, even if there is nothing to say about the technical correctness of the answers. If you all provide more feedback, you will all learn more about good data analysis reports. Be kind."
  },
  {
    "objectID": "assignments_gsu.html#assignment-scoring",
    "href": "assignments_gsu.html#assignment-scoring",
    "title": "Bayesian Data Analysis course - Assignments (GSU)",
    "section": "Assignment scoring",
    "text": "Assignment scoring\nPoints are given from both submitting an assignment and giving feedback. Submission performance gives 75% and feedback performance 25% of the total score. Only those students who returned a given assignment are allowed to give feedback to the answers submitted by others.\n\nSubmission score\nAfter peer grading is over for an assignment, each submission receives a submission score. This score is between 0% and 100% depending on how well the peers evaluated the submission. This is mostly about the technical correctness and small part about the presentation.\nThe numerical score for each criteria is normalised to be between 0% and 100%, where 0% is given to the worst, and 100% to the best possible answer. As an example, in a Scale-question with 5 levels, the different answers will count 0%, 25%, 50%, 75%, 100%.\nThe total submission score is calculated for a submission by averaging the scores from each of the peers. The score from an individual peer evaluating a submission is obtained by averaging over the criteria in the feedback rubric.\nWithin each assignment, there are two sections Basic requirements and Overall quality of the report. These segments are weighted so that they each provide 7.5% of that assignment’s total score. The remaining 85% is distributed evenly to the rest of the sections. Within each section, the points are distributed evenly among each question.\nIf a student has flagged some of their received evaluations and a TA has accepted the flag and overruled the original evaluation, then TA’s new evaluation is used for computing the score instead. If a teacher overrules a specific evaluation by one student, that answer is taken to be the correct one for that part of that submission. This means that the evaluations by other students on that part are disregarded in the final score of the submission.\nThere are 9 assignment rounds in total. Each round has different weighting when computing the total score. The assignment instructions list max points for each assignment as 3, 3, 9, 6, 6, 6, 6, 6, 3. These translate (approximately) to weights 6%, 6%, 19%, 12%, 12%, 12%, 12%, 12%, 6%.\n\n\nFeedback score\nThe feedback score depends on the reactions, such that a student’s feedback score on an assignment is an average of the reactions received. If no reactions have been given to the student’s reviews, no feedback score is computed and it will simply show as “-”.\nWhen you provide feedback, you can provide constructive and positive feedback n how to improve presentation or tell what is already very good. By providing that kind of feedback you are more likely to receive higher feedback score.\nThe following are reasons to react positively to feedback:\n\nReviewer giving you tips on how to improve the presentation of the report or specific answer.\nReviewer making the effort of finding and pointing out where your bug originates from.\n\nBe kind when reacting to to the feedback. If you didn’t like the feedback, think first a moment, and then re-consider whether it was just the wording that got you angry and maybe there is something useful in the feedback.\nNote that the baseline feedback reaction is “Somewhat useful. Could be more elaborate.” which corresponds to 0.5 points or as Peergrade shows it to 50%. It might make you feel better if you consider that the middle option is 100%, as then you have already provided useful feedback which is the goal, and then the higher categories “Very useful” and “Extremely useful” would be 150% and 200% and you can consider receiving bonus points."
  },
  {
    "objectID": "assignments_gsu.html#be-polite",
    "href": "assignments_gsu.html#be-polite",
    "title": "Bayesian Data Analysis course - Assignments (GSU)",
    "section": "Be polite",
    "text": "Be polite\nRemember to be polite when peergrading, giving feedback, and reacting to feedback. Do not spend a lot of time fighting over the grading of one question from one student. If you don’t agree you can submit a flag or in extreme case contact the TAs. Also, keep in mind that in most of the cases, which we’ve seen, the students have been fighting for points which have less than a 1/1000 effect on the final score. Long fight over that is not worth the time and energy. If you get feedback which makes you angry, breath and wait a moment before unleashing your anger back. We ask you to honor the system and be polite to your peers."
  },
  {
    "objectID": "assignments_gsu.html#ta-sessions",
    "href": "assignments_gsu.html#ta-sessions",
    "title": "Bayesian Data Analysis course - Assignments (GSU)",
    "section": "TA sessions",
    "text": "TA sessions\nYou can get help for the assignments by asking in the course chat from other students or in TA sessions by asking TAs.\n\nThere are chat channels #assignment1 etc. you can ask questions about the assignments. Other students and TAs can answer these. There is no guaranteed response time. These channels are best for questions that are likely to have relatively simple answer and thus are likely to be answered before the next TA session.\nThere are TA sessions for getting one-on-one help. These sessions are not obligatory. These sessions are useful if you think you need help that requires a bit more discussion. The questions are answered duirng the TA session time (if there are two many questions, they may be answered in the chat or next TA session).\n\nAs this course instance is based on volunteer TAs we can’t guarantee how many TA session we have each week. TA sessions will be announced during the course.\nDuring the TA session you can get help in the following forms:\n\nWritten communication on the course chat: you will chat with a TA using the “direct messages” feature on the course chat. You can also, for example, share code snippets and equations through chat direct messages if it helps.\nOral communication on Zoom: you will chat with a TA using a video conference on Zoom. You can also use, for example, screen sharing on Zoom if it helps.\n\nWe will use the channel #queue in the course chat to coordinate everything. We announce there when the TA session starts. Then you can write your help request there, describing in sufficient detail exactly what is the problem with which you would need help (see below).\nOnce a TA is free and your question is the first request in the queue, a TA will mark it with a check mark reaction. Then the TA will contact you and help with your problem. Finally, once the problem is solved, the TA who helped you will delete your request from the queue.\n\nGetting help via the course chat\n\nGo to the chat channel #queue.\nWrite a help request (see below), starting with the keyword “Chat”.\nA TA will send you a direct message on Chat.\nYou will discuss through direct messages until your problem is solved, and then the TA will close the discussion and delete your help request.\n\nPlease do not send direct messages to TAs without going through the above protocol. If you have follow-up questions later, please put a new request to the queue.\n\n\nGetting help via Zoom\n\nOpen Zoom, and make sure your video and audio are configured correctly. Create a new Zoom conference call, and copy the meeting URL.\nGo to the Chat channel #queue.\nWrite a help request (see below), starting with the keyword “Zoom”, and end it with the Zoom meeting URL.\nA TA will click on the meeting URL to join the Zoom conference that you created.\nYou will get help until your problem is solved, and then the TA will close the call and delete your help request.\n\n\n\nWhat to write in the help request?\nYour help request should contain a concise summary of exactly what kind of help you would need. Ideally, after reading the help request, a TA should be able to already have an answer for you.\nTry to describe what is the problem, what you have tried, what you already know, and exactly what is the relevant part of the code. Please highlight the important parts. Here are some fictional examples of good help requests:\n\nChat: I am not able to get the correct solution to assignment 2 part c) by using the test data. According to the assignment pdf, the correct solution is 0.4511238, but I am getting the answer 0.3771946. The code for my function is:\nEXAMPLE CODE 1\nI thought that I have an error in …, so I tried changing … to …, but then I got a different wrong answer, 0.4285443. I think the error may be on line 3 in the code, but I do not know how to fix that.\n\nZoom: I am trying to install the R package … on my personal laptop and I am getting the following error:\nEXAMPLE ERROR\nMy operating system is …, I have version … of R installed and I am using RStudio. I tried googling the error but was not able to solve the issue. Zoom meeting link: https://aalto.zoom.us/j/XXX\n\n\n\nAcknowledgements\nTA session instructions above have been copied from Programming Parallel Computers by Jukka Suomela with CC-BY-4.0 license."
  },
  {
    "objectID": "FAQ.html",
    "href": "FAQ.html",
    "title": "Bayesian Data Analysis course - FAQ",
    "section": "",
    "text": "Instead of installing RStudio on your computer, you can use it in your web browser:\n\nInformation about Aalto JupyterHub\nGo to jupyter.cs.aalto.fi\nChoose CS-E5710 - Bayesian Data Analysis (2024)\nIn the Launcher click RStudio\nIn the RStudio Files pane (bottom right) you can create folders for your work and upload files from your computer to the server\n\nThe notebooks folder is the only persistent folder (stays there if you sign out) so save everything to that folder!\nYou may get an error when uploading a large zip file, but uploading smaller zip files work. If you can’t upload demo zip file contact the course staff via Zulip.\nYou may access your data as a network drive by SMB mounting it on your own computer - see Accessing JupyterHub data. This allows you to have total control over your data.\n\nAfter uploading files, use Files pane to open them (e.g. an RMarkdown or Quarto notebook)\nKnitting of R, Rmd, and qmd files works as well (tested 24th August)\nCmdStanR used later in the course has been tested to work 24th August.\n\nTo use CmdStanR: library(cmdstanr)\n\nIf you do not see the following output, please contact us on Zulip: This is cmdstanr version 0.6.0 - CmdStanR documentation and vignettes: mc-stan.org/cmdstanr - CmdStan path: /coursedata/cmdstan - CmdStan version: 2.33.0\n\n\nThere is a limited memory available (3Gib) and bigger models and datasets can run out of memory with cryptic error message, but the demos and assignment models should run (if not, then contact the course staff via Zulip).\nSee also Aalto JupyterHub FAQ and bugs\n\n\n\n\n\nInformation about Aalto remote desktop\nGo to vdi.aalto.fi\nDownload VMWare Horizon application or use the web portal\n\nIf using the VMWare Horizon application, click on New Server and enter vdi.aalto.fi\n\nEnter your aalto username (aalto email works too) and password in the respective fields.\nSelect Ubuntu 20.04\nClick Activities, start typing RStudio in the search bar, and click RStudio.\n\n\n\n\n\nGo to jupyter.cs.aalto.fi on your favorite web-browser. \nLog-in with your aalto username and password.\nSelect the CS-E5710 - Bayesian Data Analysis (2024) -r-ubuntu:6.2.3-bayesda2024 server. \nSelect the notebooks folder in the left hand file browser. \nSelect the git clone icon as seen in the screenshot below. \nIn the text box type https://github.com/avehtari/BDA_R_demos.git for python demos replace BDA_R_demos.git with BDA_Python_demos.git instead. Then click clone. \nWait a while, there should be BDA_R_demos folder under notebooks folder. Click on the BDA_R_demos folder. \nClick on the RStudio button on the right. \nNow you should have an R-studio like interface in your web-browser. Click on File -&gt; Open File... \nClick on notebooks and then select BDA_R_demos folder. \nSelect a demo to run. Here we open the folder demos_ch2 and then select demo2_1.R file and click open. This should open the file in the window. \nSelect the contents of the file and click Code -&gt; Run Selected Line(s) as shown in the screenshot below. \nYou should see the output of the code in the bottom right corner."
  },
  {
    "objectID": "FAQ.html#how-to-use-r-and-rstudio-remotely",
    "href": "FAQ.html#how-to-use-r-and-rstudio-remotely",
    "title": "Bayesian Data Analysis course - FAQ",
    "section": "",
    "text": "Instead of installing RStudio on your computer, you can use it in your web browser:\n\nInformation about Aalto JupyterHub\nGo to jupyter.cs.aalto.fi\nChoose CS-E5710 - Bayesian Data Analysis (2024)\nIn the Launcher click RStudio\nIn the RStudio Files pane (bottom right) you can create folders for your work and upload files from your computer to the server\n\nThe notebooks folder is the only persistent folder (stays there if you sign out) so save everything to that folder!\nYou may get an error when uploading a large zip file, but uploading smaller zip files work. If you can’t upload demo zip file contact the course staff via Zulip.\nYou may access your data as a network drive by SMB mounting it on your own computer - see Accessing JupyterHub data. This allows you to have total control over your data.\n\nAfter uploading files, use Files pane to open them (e.g. an RMarkdown or Quarto notebook)\nKnitting of R, Rmd, and qmd files works as well (tested 24th August)\nCmdStanR used later in the course has been tested to work 24th August.\n\nTo use CmdStanR: library(cmdstanr)\n\nIf you do not see the following output, please contact us on Zulip: This is cmdstanr version 0.6.0 - CmdStanR documentation and vignettes: mc-stan.org/cmdstanr - CmdStan path: /coursedata/cmdstan - CmdStan version: 2.33.0\n\n\nThere is a limited memory available (3Gib) and bigger models and datasets can run out of memory with cryptic error message, but the demos and assignment models should run (if not, then contact the course staff via Zulip).\nSee also Aalto JupyterHub FAQ and bugs\n\n\n\n\n\nInformation about Aalto remote desktop\nGo to vdi.aalto.fi\nDownload VMWare Horizon application or use the web portal\n\nIf using the VMWare Horizon application, click on New Server and enter vdi.aalto.fi\n\nEnter your aalto username (aalto email works too) and password in the respective fields.\nSelect Ubuntu 20.04\nClick Activities, start typing RStudio in the search bar, and click RStudio.\n\n\n\n\n\nGo to jupyter.cs.aalto.fi on your favorite web-browser. \nLog-in with your aalto username and password.\nSelect the CS-E5710 - Bayesian Data Analysis (2024) -r-ubuntu:6.2.3-bayesda2024 server. \nSelect the notebooks folder in the left hand file browser. \nSelect the git clone icon as seen in the screenshot below. \nIn the text box type https://github.com/avehtari/BDA_R_demos.git for python demos replace BDA_R_demos.git with BDA_Python_demos.git instead. Then click clone. \nWait a while, there should be BDA_R_demos folder under notebooks folder. Click on the BDA_R_demos folder. \nClick on the RStudio button on the right. \nNow you should have an R-studio like interface in your web-browser. Click on File -&gt; Open File... \nClick on notebooks and then select BDA_R_demos folder. \nSelect a demo to run. Here we open the folder demos_ch2 and then select demo2_1.R file and click open. This should open the file in the window. \nSelect the contents of the file and click Code -&gt; Run Selected Line(s) as shown in the screenshot below. \nYou should see the output of the code in the bottom right corner."
  },
  {
    "objectID": "FAQ.html#how-to-use-r-and-rstudio-on-your-own-computer",
    "href": "FAQ.html#how-to-use-r-and-rstudio-on-your-own-computer",
    "title": "Bayesian Data Analysis course - FAQ",
    "section": "How to use R and RStudio on your own computer",
    "text": "How to use R and RStudio on your own computer\n\nHow to access the course material in Github\nInstead of trying to download each file separately via the Github interface, it is recommended to use one of these options:\n\nThe best way is to clone the repository using git, and use pull to get the latest updates.\n\nIf you want to learn to use git, start by installing a git client. There are plenty of good git tutorials online. See for example the tutorial by Aalto Git Aalto\n\nIf you don’t want to learn to use git, download a the repository as a zip file. Click the green button “Code” at the main page of the repository and choose “Download ZIP” (direct link). Remember to download again during the course to get the latest updates.\n\n\n\nR packages used in demos\nAalto JupyterHub has all the R packages used in demos pre-installed. If you install R on your own computer, you can install all the packages used by the main demos with\ninstall.packages(c(\"MASS\", \"bayesplot\", \"brms\", \"cmdstanr \", \"dplyr\", \"gganimate\", \"ggdist\", \"ggforce\", \"ggplot2\", \"grid\", \"gridExtra\", \"latex2exp\", \"loo\", \"plyr\", \"posterior\", \"purrr\", \"rprojroot\", \"tidyr\", \"quarto\"))\n\n\nInstalling aaltobda package\nThe course has its own R package aaltobda with data and functionality to simplify coding. aaltobda has been pre-installed in Aalto JupyterHub. To install the package to your own computer just run the following:\n\ninstall.packages(\"aaltobda\", repos = c(\"https://avehtari.r-universe.dev\", getOption(\"repos\")))\n\nIf during the course there is announcement that aaltobda has been updated (e.g. some error has been fixed), you can get the latest version by repeating the second step above.\n\n\nError related to LC_CTYPE while installing aaltobda r-package.\nWhen installing the aaltobda package, you may encounter an error like this:\n\nError: (converted from warning) Setting LC_CTYPE failed, using \"C\"\nExecution halted\nError: Failed to install 'aaltobda' from GitHub:\n (converted from warning) installation of package '/var/folders/g6/bdv4dr4s6qq4zyxw2nzy26kr0000gn/T//Rtmp3uYwuD/file121f355845a3/aaltobda_0.1.tar.gz' had non-zero exit status\n\nSolution: See the following StackOverflow solution. (link)\n\n\nProblems installing R packages on Windows ?\nGetting the setup needed for the course working on Windows might involve a bit more effort than on Linux and Mac. Consequently, we recommend using either Linux or MacOS, or using R remotely. Moreover, Stan, the probabilistic programming language which we will use later on during the course requires a C++ compiler toolchain which is not available by default in Windows (blame Microsoft). However, if you want to use Windows and have a problem getting the setup working, below are two options to consider:\n\n\nInstalling knitr\nIf you just installed RStudio and R, chances are you don’t have knitr installed, the package responsible for rendering your notebook to pdf.\nSolution:\n\ninstall.packages(\"knitr\")\n\nYou can also install packages from RStudio menu Tools-&gt;Install Packages.\n\n\nIf knitr is installed but the pdf won’t compile\nIn this case it is possible that you don’t have LaTeX installed, which is the package that runs the engine to process the text and render the pdf itself.\nSolution: Tinytex is the bare minimum Latex core that you need to install in order to run the pdf compiler. If you want to go further and download a full distribution of Latex, look at TeX Live for Linux and MacTeX for Mac OS.\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\n\n\nHow to install the latest version of CmdStanR or RStan\n\nMake sure you have installed R version 4.* or newer. If you don’t, install a newer version using instructions from https://www.r-project.org/\nAt the moment CmdStanR is faster in use, and thus we recommend using CmdStanR instead of RStan, but both can be used for the assignments.\nCmdStanR is a lightweight interface to Stan for R users (see CmdStanPy for Python).\nCmdStanR avoids some installation problems as it doesn’t require matching C++ tools for R and RStan\nInstall RStan along with the necessary C++ compiler toolchain as described here\n\nInstead of RStan, you can also use new CmdStanR which maybe easier to install.\n\n\nWorkarounds for current Rstan Windows issues\n\nA list of the current RStan Windows problems and and known temporary workarounds"
  },
  {
    "objectID": "FAQ.html#how-to-render-a-single-qmd-file-to-html-or-pdf",
    "href": "FAQ.html#how-to-render-a-single-qmd-file-to-html-or-pdf",
    "title": "Bayesian Data Analysis course - FAQ",
    "section": "How to render a single qmd-file to html or pdf",
    "text": "How to render a single qmd-file to html or pdf\nClicking Render in RStudio defaults to render all qmd files in the directory (“project”) and to render both html and pdf. To save time in rendering, you can install quarto R package\ninstall.packages(\"quarto\")\nlibrary(quarto)\nand then render just one file and choose the target to be html with\nquarto_render(\"template2.qmd\", output_format = \"html\")\nand when you are ready to submit the pdf, use\nquarto_render(\"template2.qmd\", output_format = \"pdf\")\nquarto R package is available as pre-installed in JupyterHub (you may need to restart your server o get the new image).\nIf the rendering to pdf doesn’t work, you can print the html to pdf file, but do make sure to turn off “More setings -&gt; Print headers and footers” to avoid accidentally printing your identity."
  },
  {
    "objectID": "FAQ.html#what-is-tidyr-or-tidyverse-that-is-used-in-the-r-demos-what-does-mean",
    "href": "FAQ.html#what-is-tidyr-or-tidyverse-that-is-used-in-the-r-demos-what-does-mean",
    "title": "Bayesian Data Analysis course - FAQ",
    "section": "What is tidyr or tidyverse that is used in the R demos? What does %>% mean?",
    "text": "What is tidyr or tidyverse that is used in the R demos? What does %&gt;% mean?\n\nTidyverse is a collection of R packages designed for data science. The packages “share an underlying design philosophy, grammar, and data structures”.\nA clear characteristic that distinguishes tidyverse from the base R is the pipe operator %&gt;%\nRecent R versions have a new built-in pipe operator |&gt;, which in most cases can replace %&gt;%, and there are some differences only in more advanced use cases.\nIn this course you do not need to use tidyverse. However, some packages belonging to tidyverse, such as ggplot2, can be useful for visualizing results in the reports."
  },
  {
    "objectID": "FAQ.html#m1-macs-with-python-and-stan",
    "href": "FAQ.html#m1-macs-with-python-and-stan",
    "title": "Bayesian Data Analysis course - FAQ",
    "section": "M1 Macs with Python and Stan",
    "text": "M1 Macs with Python and Stan\nUnfortunately the installation of pystan will fail on an M1 Mac, as there is not a binary wheel available for the httpstan dependency. A recommended alternative here is the CmdStanPy package.\nM1 Mac users that are intent on using pystan will need to complete the following steps to build httpstan from source and then install pystan:\n\nBuild and Install httpstan\n# Download httpstan source\ngit clone https://github.com/stan-dev/httpstan\ncd httpstan\n# Build shared libraries and generate code\npython3 -m pip install poetry\n# Build httpstan source\n#   - There will be many compiler warnings, these are safe to ignore\nmake -j8\n# Build the httpstan wheel\npython3 -m poetry build\n# Install the wheel\npython3 -m pip install dist/*.whl\n\n\nInstall pystan\npython3 -m pip install pystan"
  },
  {
    "objectID": "FAQ.html#i-missed-some-deadline-or-wasnt-able-to-do-some-part-of-the-course",
    "href": "FAQ.html#i-missed-some-deadline-or-wasnt-able-to-do-some-part-of-the-course",
    "title": "Bayesian Data Analysis course - FAQ",
    "section": "I missed some deadline or wasn’t able to do some part of the course",
    "text": "I missed some deadline or wasn’t able to do some part of the course\n\nCan I combine results from assignments, project, presentation, and e-exam made in different periods / years?\n\nYes.\n\nI missed the deadline to register for the course in Sisu. Can I join the course?\n\nYes, just register in MyCourses and contact student services that they add you in Sisu, too.\n\nI missed the deadline for the assignment. Can you accept my late submission?\n\nOpen MyCourses Quizzes are automatically submitted at the deadline time \n\nI was not able to do one of the assignments because [some personal problem]. Can I do some extra work?\n\nThings happen and you don’t need to tell the course staff your personal reasons (especially you shouldn’t tell any health issue details). Everyone gets a second change in period III. In period III there is just one submission deadline, but otherwise the procedure is the same (ie. you need to return all the assignments). If you submitted the project work in autumn you don’t need to re-submit it if you re-submit assignments.\n\nI missed the deadline to register project group. Can I still register?\n\nYes. Those who registered early are allowed to choose the presentation slots first.\n\nMy group member a) disappeared, b) doesn’t do anything, c) is annoying. Can I continue with the project alone.\n\nFirst we hope you can resolve the issue, but if nothing works, then you can continue the project work alone.\n\nI was not able a) to do the project or b) to give a presentation because [some personal problem]. Can a) I submit it later, b) present later.\n\nThings happen and you don’t need to tell the course staff your personal reasons (especially you shouldn’t tell any health issue details). Everyone gets a second change in period III. In period III there is second project submission deadline and presentation slots. If you are happy with your assignment score, you don’t need to re-submit assignments if you submit the project work in period III."
  },
  {
    "objectID": "FAQ.html#recommended-courses-after-bayesian-data-analysis",
    "href": "FAQ.html#recommended-courses-after-bayesian-data-analysis",
    "title": "Bayesian Data Analysis course - FAQ",
    "section": "Recommended courses after Bayesian Data Analysis",
    "text": "Recommended courses after Bayesian Data Analysis\nHere are some great Aalto courses that are using Bayesian inference\n\nCS-E4820 - Machine Learning: Advanced Probabilistic Methods (common probabilistic models in machine learning, such as sparse Bayesian linear models, Gaussian mixture models and factor analysis models, variational inference)\nELEC-E8106 - Bayesian Filtering and Smoothing (dynamical systems, time series, tracking)\nCS-E4895 - Gaussian Processes\nCS-E5795 - Computational Methods in Stochastics (more about MC, MCMC, HMC)\nMS-E1654 - Computational Inverse Problems"
  },
  {
    "objectID": "Aalto2024.html",
    "href": "Aalto2024.html",
    "title": "Bayesian Data Analysis course - Aalto 2024",
    "section": "",
    "text": "Aalto 2024 course can be taken online except for the final project presentation. The lectures will be given on campus, but recorded and the recording will be made available online after the lecture. If you are unable to register for the course at the moment in the Sisu, there is no need to email the lecturer. You can start taking the course and register before the end of the course. Sisu shows rooms on campus for the computer exercises, and you can come to ask questions on campus, but you can also ask in Zulip during the same times. You can choose which TA session to join each week separately, without a need to register for those sessions.\nAll the course material is available in a git repo and in Panopto, (and these pages are for easier navigation). All the material can be used in other courses. Text and videos licensed under CC-BY-NC 4.0. Code licensed under BSD-3.\nThe material will be updated during the course. Exercise instructions and slides will be updated at latest on Monday of the corresponding week. The updated material will appear on the web pages, or you can clone the repo and pull before checking new material. If you don’t want to learn git, you can download the latest zip file."
  },
  {
    "objectID": "Aalto2024.html#book-bda3",
    "href": "Aalto2024.html#book-bda3",
    "title": "Bayesian Data Analysis course - Aalto 2024",
    "section": "Book: BDA3",
    "text": "Book: BDA3\n\n\n\nThe electronic version of the course book Bayesian Data Analysis, 3rd ed, by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin is available for non-commercial purposes. Hard copies are available from the publisher and many book stores. Aalto library has also copies. See also home page for the book, errata for the book, and chapter notes."
  },
  {
    "objectID": "Aalto2024.html#prerequisites",
    "href": "Aalto2024.html#prerequisites",
    "title": "Bayesian Data Analysis course - Aalto 2024",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic terms of probability theory\n\nprobability, probability density, distribution\nsum, product rule, and Bayes’ rule\nexpectation, mean, variance, median\nin Finnish, see e.g. Stokastiikka ja tilastollinen ajattelu\nin English, see e.g. Wikipedia and Introduction to probability and statistics\n\nSome algebra and calculus\nBasic visualisation techniques (R or Python)\n\nhistogram, density plot, scatter plot\nsee e.g. BDA R demos\nsee e.g. BDA Python demos\n\n\nThis course has been designed so that there is strong emphasis in computational aspects of Bayesian data analysis and using the latest computational tools. The project brings together the overall Bayesian workflow aspects.\nIf you find BDA3 too difficult to start with, I recommend\n\nFor regression models, their connection to statistical testing and causal analysis see Gelman, Hill and Vehtari: “Regression and Other Stories”.\nRichard McElreath: Statistical Rethinking, 2nd ed book is easier than BDA3 and the 2nd ed is excellent. Statistical Rethinking doesn’t go as deep in some details, math, algorithms and programming as BDA course. Richard’s lecture videos of Statistical Rethinking: A Bayesian Course Using R and Stan are highly recommended even if you are following BDA3.\nJohnson, Ott, and Dogucu: Bayes Rules! An Introduction to Applied Bayesian Modeling\nFor background prerequisites some students have found chapters 2, 4 and 5 in Kruschke, “Doing Bayesian Data Analysis” useful."
  },
  {
    "objectID": "Aalto2024.html#communication-channels",
    "href": "Aalto2024.html#communication-channels",
    "title": "Bayesian Data Analysis course - Aalto 2024",
    "section": "Communication channels",
    "text": "Communication channels\n\nMyCourses is used for some intial announcements, linking to Zulip and Peergrade, and some questionnaires.\nThe primary communication channel is the Zulip course chat (link in MyCourses, login with Aalto account)\n\nDon’t ask via email or direct messages. By asking via common streams in the course chat, more eyes will see your question, it will get answered faster and it’s likely that other students benefit from the answer.\nLogin with Aalto account to the Zulip course chat. You can adjust the notifications in the settings.\nIf you have any questions, please ask in the public streams and get answers from course staff or other students (active students helping others will get bonus points).\nIn the chat system, we will have separate streams for each assignment and the project.\nStream #general can be used for any kind of general discussions and questions related to the course.\nAll important announcements will be posted to #announcements (no discussion on this stream).\nAny kind of feedback is welcome on stream #feedback.\nWe have also streams #r, #python, and #stan for questions that are not specific to assignments or the project.\nStream #queue is used as a queue for getting help during TA sessions.\nThe lecturer and teaching assistants have names with “(staff)” or “(TA)” in the end of their names.\n\nA weekly lecture time on campus includes times for questions and answers\nIf you need one-to-one help, please take part in the TA sessions and ask there.\nIf you find errors in material, post in #feedback stream or submit an issue in github."
  },
  {
    "objectID": "Aalto2024.html#assessment",
    "href": "Aalto2024.html#assessment",
    "title": "Bayesian Data Analysis course - Aalto 2024",
    "section": "Assessment",
    "text": "Assessment\nAssignments (40%), e-exam (10%), and a project work with presentation (50%). Minimum of 50% of points must be obtained from each. You can get bonus points from chat activity (e.g. helping other students and reporting typos in the material) and answering time usage questionnaries."
  },
  {
    "objectID": "Aalto2024.html#schedule-2024",
    "href": "Aalto2024.html#schedule-2024",
    "title": "Bayesian Data Analysis course - Aalto 2024",
    "section": "Schedule 2024",
    "text": "Schedule 2024\nThe course consists of 12 lectures, 9 assignments, a project work, and a project presentation in periods I and II. It’s good start reading the material for the next lecture and assignment while making the assignment related to the previous lecture. There are 9 assignments and a project work with presentation, and thus the assignments are not in one-to-one correspondence with the lectures. The schedule below lists the lectures and how they connect to the topics, book chapters and assignments.\n\nSchedule overview\nHere is an overview of the schedule. Scroll down the page to see detailed instructions for each block. When you are working on assignment related to previous lecture, it is good to start reading the book chapters relaed to the next lecture and assignment. The schedule links to 2023 lecture videos until couple hours after the 2024 lecture has been recorded.\n\n\n\n\nReadings\nLectures\nAssignment\nLecture Date\nAssignment due date\n\n\n\n\n1. Introduction\nBDA3 Chapter 1\n2024 Lecture 1.1 Introduction,  2024 Lecture 1.2 Course practicalities, Slides 1.1, Slides 1.2\n2024 Assignment 1\n2024-09-02\n2024-09-15\n\n\n2. Basics of Bayesian inference\nBDA3 Chapter 1,  BDA3 Chapter 2\n2023 Lecture 2.1 (2024 recording failed),  2024 Lecture 2.2, Slides 2\nAssignment 2\n2024-09-16\n2024-09-22\n\n\n3. Multidimensional posterior\nBDA3 Chapter 3\n2023 Lecture 3.1, 2023 Lecture 3.2Slides 3\nAssignment 3\n2024-09-23\n2024-09-29\n\n\n4. Monte Carlo\nBDA3 Chapter 10\n2024 Lecture 4.1, 2024 Lecture 4.2, Slides 4\nAssignment 4\n2024-09-30\n2024-10-06\n\n\n5. Markov chain Monte Carlo\nBDA3 Chapter 11\n2024 Lecture 5.1,  2024 Lecture 5.2, Slides 5\nAssignment 5\n2024-10-07\n2024-10-13\n\n\n6. Stan, HMC, PPL\nBDA3 Chapter 12 + extra material on Stan\n2023 Lecture 6.1, Lecture 6.2, 2024 Slides 6\nAssignment 6\n2024-10-14\n2024-10-27\n\n\n7. Hierarchical models and exchangeability\nBDA3 Chapter 5\n2024 Lecture 7Slides 7\nAssignment 7\n2024-10-28\n2024-11-10\n\n\n8. Model checking & cross-validation\nBDA3 Chapter 6, BDA3 Chapter 7, Visualization in Bayesian workflow, Practical Bayesian cross-validation\n2024 Lecture 8.1,  2024 Lecture 8.2, Slides 8a,Slides 8b\nStart project work\n2024-11-04\nN/A\n\n\n9. Model comparison, selection, and hypothesis testing\nBDA3 Chapter 7 (not 7.2 and 7.3),  Practical Bayesian cross-validation\n2023 Lecture 9.1,  Lecture 9.2, Slides 9\nAssignment 8\n2024-11-11\n2024-11-17\n\n\n10. Decision analysis\nBDA3 Chapter 9\n2023 Lecture 10.1 (2024 recording failed, but the content is almost the same), 2024 Lecture 10.2, Slides 10a, Slides 10b\nAssignment 9\n2024-11-18\n2024-11-24\n\n\n11. Variable selectio with projpred, project presentation example\nBDA3 Chapter 4\n2023 Lecture 11.1, 2023 Lecture 11.2, 2023 Lecture 11.3, Slides 11a, Slides Project Presentation, Slides 11 extra\nProject work\n2024-11-25\nN/A\n\n\n12. TBA\n\nOptional:   \nProject work\n2024-12-02\nN/A\n\n\n13. Project evaluation\n\n\n\nProject presentations: 9.-13.12.\nEvaluation week\n\n\n\n\n\n1) Course introduction, BDA 3 Ch 1, prerequisites assignment\nCourse practicalities, material, assignments, project work, peergrading, QA sessions, TA sessions, prerequisites, chat, etc.\n\nLogin with Aalto account to the Zulip course chat with link in MyCourses\nIntroduction/practicalities lecture Monday 2024-09-02 14:15-16, hall C, Otakaari 1**\n\n2024 Lecture videos 1.1 and 1.2 in Panopto\nSlides 1.1, Slides 1.2\n\nRead BDA3 Chapter 1\n\nstart with reading instructions for Chapter 1 and afterwards read the additional comments in the same document\n\nThere are no R demos for Chapter 1\nMake and submit 2024 Assignment 1. Deadline Sunday 2024-09-15 23:59\n\nWe highly recommend to submit all assignments Friday before 3pm so that you can get TA help before submission. As the course has students who work weekdays (e.g. FiTech students), the late submission until Sunday night is allowed, but we can’t provide support during the weekends.\nthis assignment checks that you have sufficient prerequisite skills (basic probability calculus, and R or Python)\nGeneral information about assignments\n\nR markdown template for assignments\nFAQ for the assignments has solutions to commonly asked questions related RStudio setup, errors during package installations, etc.\n\n\nGet help in TA sessions 2024-09-04 14-16, Y342a, Otakaari 1 2024-09-05 12-14, Y429c-d, Otakaari 1\n\nin Sisu these are marked as exercise sessions, but we call them TA sessions\nthese are optional and you can choose which one to join\nsee more info about TA sessions\n\nHighly recommended, but optional: Make BDA3 exercises 1.1-1.4, 1.6-1.8 (model solutions available for 1.1-1.6)\nStart reading Chapters 1+2, see instructions below\n\n\n\n2) BDA3 Ch 1+2, basics of Bayesian inference\nBDA3 Chapters 1+2, basics of Bayesian inference, observation model, likelihood, posterior and binomial model, predictive distribution and benefit of integration, priors and prior information, and one parameter normal model.\n\nRead BDA3 Chapter 2\n\nsee reading instructions for Chapter 2\n\nLecture Monday 2024-09-16 14:15-16, hall T1, CS building\n\nSlides 2\nVideos: 2023 Lecture 2.1 (2024 recording failed), 2024 Lecture 2.2 on basics of Bayesian inference, observation model, likelihood, posterior and binomial model, predictive distribution and benefit of integration, priors and prior information, and one parameter normal model. BDA3 Ch 1+2.\n\nRead the additional comments for Chapter 2\nCheck R demos or Python demos for Chapter 2\nMake and submit Assignment 2. Deadline Sunday 2024-09-22 23:59\n\nTA sessions 2024-09-18 14-16, Y342a, Otakaari 1 2024-09-19 12-14, Y429c-d, Otakaari 1\n\nHighly recommended, but optional: Make BDA3 exercises 2.1-2.5, 2.8, 2.9, 2.14, 2.17, 2.22 (model solutions available for 2.1-2.5, 2.7-2.13, 2.16, 2.17, 2.20, and 2.14 is in course slides)\nStart reading Chapter 3, see instructions below\n\n\n\n3) BDA3 Ch 3, multidimensional posterior\nMultiparameter models, joint, marginal and conditional distribution, normal model, bioassay example, grid sampling and grid evaluation. BDA3 Ch 3.\n\nRead BDA3 Chapter 3\n\nsee reading instructions for Chapter 3\n\nLecture Monday 2024-09-23. 14:15-16, hall T1, CS building\n\nSlides 3\nVideos: 2023 Lecture 3.1 2023 Lecture 3.2 on multiparameter models, joint, marginal and conditional distribution, normal model, bioassay example, grid sampling and grid evaluation. BDA3 Ch 3.\n\nRead the additional comments for Chapter 3\nCheck R demos or Python demos for Chapter 3\nMake and submit Assignment 3. Deadline Sunday 2024-09-29 23:59\nTA sessions 2024-09-25 14-16, 2024-09-26 12-14,\nHighly recommended, but optional: Make BDA3 exercises 3.2, 3.3, 3.9 (model solutions available for 3.1-3.3, 3.5, 3.9, 3.10)\nStart reading Chapter 10, see instructions below\n\n\n\n4) BDA3 Ch 10, Monte Carlo\nNumerical issues, Monte Carlo, how many simulation draws are needed, how many digits to report, direct simulation, curse of dimensionality, rejection sampling, and importance sampling. BDA3 Ch 10.\n\nRead BDA3 Chapter 10\n\nsee reading instructions for Chapter 10\n\nLecture Monday 2024-09-30 14:15-16, hall T1, CS building\n\nSlides 4\nVideos: 2024 Lecture 4.1 on numerical issues, Monte Carlo, how many simulation draws are needed, how many digits to report, and 2024 Lecture 4.2 on Pareto-\\(\\hat{k}\\) diagnostic, direct simulation, rejection sampling, and importance sampling. BDA3 Ch 10.\n\nRead the additional comments for Chapter 10\nCheck R demos or Python demos for Chapter 10\nMake and submit Assignment 4. Deadline Sunday 2024-10-06 23:59\nTA sessions 2024-10-02 14-16, 2024-10-03 12-14,\nHighly recommended, but optional: Make BDA3 exercises 10.1, 10.2 (model solution available for 10.4)\nStart reading Chapter 11, see instructions below\n\n\n\n5) BDA3 Ch 11, Markov chain Monte Carlo\nMarkov chain Monte Carlo, Gibbs sampling, Metropolis algorithm, warm-up, convergence diagnostics, R-hat, and effective sample size. BDA3 Ch 11.\n\nRead BDA3 Chapter 11\n\nsee reading instructions for Chapter 11\n\nLecture Monday 2024-10-07 14:15-16, hall T1, CS building\n\nSlides 5\nVideos: 2024 Lecture 5.1 on Markov chain Monte Carlo, Gibbs sampling, Metropolis algorithm, and 2024 Lecture 5.2 on warm-up, convergence diagnostics, R-hat, and effective sample size.\n\nRead the additional comments for Chapter 11\nCheck R demos or Python demos for Chapter 11\nMake and submit Assignment 5. Deadline Sunday 2024-10-13 23:59\nTA sessions 2024-10-09 14-16, 2024-10-10 12-14,\nHighly recommended, but optional: Make BDA3 exercise 11.1 (model solution available for 11.1)\nStart reading Chapter 12 + Stan material, see instructions below\n\n\n\n6) BDA3 Ch 12 + Stan, HMC, PPL, Stan\nHMC, NUTS, dynamic HMC and HMC specific convergence diagnostics, probabilistic programming and Stan. BDA3 Ch 12 + extra material\n\nRead BDA3 Chapter 12\n\nsee reading instructions for Chapter 12\n\nLecture Monday 2024-10-14 14:15-16, hall T1, CS building\n\nSlides 6\nVideos: 2023 Lecture 6.1 on HMC, NUTS, dynamic HMC and HMC specific convergence diagnostics, and 2024 Lecture 6.2 on probabilistic programming and Stan. BDA3 Ch 12 + extra material.\nOptional: Stan Extra introduction recorded 2020 Golf putting example, main features of Stan, benefits of probabilistic programming, and comparison to some other software.\n\nRead the additional comments for Chapter 12\nRead Stan introduction article\nCheck R demos for RStan or Python demos for PyStan\nAdditional material for Stan:\n\nDocumentation\nRStan installation\nPyStan installation\nBasics of Bayesian inference and Stan, Jonah Gabry & Lauren Kennedy Part 1 and Part 2\n\nMake and submit Assignment 6. DeadlineSunday 2024-10-27 23:59 (two weeks for this assignment)\nTA sessions 2024-10-16 14-16, 2024-10-17 12-14,\nStart reading Chapter 5 + Stan material, see instructions below\n\n\n\n7) BDA3 Ch 5, hierarchical models\nHierarchical models and exchangeability. BDA3 Ch 5.\n\nRead BDA3 Chapter 5\n\nsee reading instructions for Chapter 5\n\nLecture Monday 2024-10-28 14:15-16, hall T2, CS building\n\nSlides 7\nVideos: 2023 Lecture 7 on hierarchical models and exchangeability.\n\nRead the additional comments for Chapter 5\nCheck R demos or Python demos for Chapter 5\nMake and submit Assignment 7. Deadline Sunday 2024-11-10 23:59 (two weeks for this assignment)\nTA sessions 2024-10-30 14-16, 2024-10-31 12-14,\nHighly recommended, but optional: Make BDA3 exercises 5.1 and 5.2 (model solution available for 5.3-5.5, 5.7-5.12)\nStart reading Chapters 6-7 and additional material, see instructions below.\n\n\n\n8) BDA3 Ch 6+7 + extra material, model checking, cross-validation\nModel checking and cross-validation.\n\nRead BDA3 Chapters 6 and 7 (skip 7.2 and 7.3)\n\nsee reading instructions for Chapter 6 and Chapter 7\n\nRead Visualization in Bayesian workflow\n\nmore about workflow and examples of prior predictive checking and LOO-CV probability integral transformations\n\nRead Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC (Journal link)\n\nreplaces BDA3 Sections 7.2 and 7.3 on cross-validation\n\nLecture Monday 2024-11-04 14:15-16, hall T2, CS building\n\nSlides 8a, Slides 8b\nVideos: 2024 Lecture 8.1 on model checking, and 2024 Lecture 8.2 on cross-validation part 1. BDA3 Ch 6-7 + extra material.\n\nRead the additional comments for Chapter 6 and Chapter 7\nCheck R demos or Python demos for Chapter 6\nAdditional reading material\n\nCross-validation FAQ\n\nNo new assignment in this block\nStart the project work\nTA sessions 2024-11-06 14-16, 2024-11-07 12-14,\nHighly recommended, but optional: Make BDA3 exercise 6.1 (model solution available for 6.1, 6.5-6.7)\n\n\n\n9) BDA3 Ch 7, extra material, model comparison and selection\nPSIS-LOO, K-fold-CV, model comparison and selection. Extra lecture on variable selection with projection predictive variable selection.\n\nRead Chapter 7 (no 7.2 and 7.3)\n\nsee reading instructions for Chapter 7\n\nRead Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC (Journal link)\n\nreplaces BDA3 Sections 7.2 and 7.3 on cross-validation\n\nLecture Monday 2024-11-11 14:15-16, hall T2, CS building\n\nSlides 9\nVideos: Lecture 9.1 and Lecture 9.2 on model comparison, selection, and hypothesis testing.\n\nAdditional reading material\n\nCV FAQ\n\nMake and submit Assignment 8. Sunday 2024-11-17 23:59\nTA sessions 2024-11-13 14-16, 2024-11-14 12-14,\nStart reading Chapter 9, see instructions below.\n\n\n\n10) BDA3 Ch 9, decision analysis + BDA3 Ch 4 Laplace approximation and asymptotics\nDecision analysis. BDA3 Ch 9. + Laplace approximation and asymptotics. BDA Ch 4.\n\nRead Chapter 9 and 4\n\nsee reading instructions for Chapter 9\nsee reading instructions for Chapter 4\n\nLecture Monday 2024-11-18 14:15-16, hall T2, CS building\n\nSlides 10a, Slides 10b\nVideos: 2023 Lecture 10.1 (2024 recording failed, but the content is almost the same) on decision analysis. BDA3 Ch 9, and 2024 Lecture 10.2 on Laplace approximation, and asymptotics, BDA3 Ch 4.\n\nMake and submit Assignment 9. Sunday 2024-11-24 23:59\nTA sessions 2024-11-20 14-16, 2024-11-21 12-14,\nStart reading Chapter 4, see instructions below.\n\n\n\n11) Variable selection with projpred, project presentation example, extra\n\nLecture Monday 2024-11-25 14:15-16, hall T2, CS building\n\nSlides 11a, Slides Project Presentation, Slides 11 extra\nVideos: 2023 Lecture 11.1 on variable selecion with projpred, 2023 Lecture 11.2 on project presentations, 2023 Lecture 11.3 on rest of BDA3, ROS, and Bayesian Workflow\n\nNo new assignment. Work on project. TAs help with projects.\nTA sessions 2024-11-27 14-16, 2024-11-28 12-14,\n\n\n\n12) TBA\n\nLecture Monday 2024-12-02 14:15-16, hall T2, CS building\n\nSlides 12\n\nTBA\nWork on project. TAs help with projects. Project deadline 1.12. 23:59\nTA sessions 2024-12-04 14-16, 2024-12-05 12-14,\n\n\n\n13) Project evaluation\n\nProject report deadline 1.12. 23:59 (submit to peergrade).\n\nReview project reports done by your peers before 6.12. 23:59, and reflect on your feedback.\n\nProject presentations 9.-13.12. (evaluation week)"
  },
  {
    "objectID": "Aalto2024.html#r",
    "href": "Aalto2024.html#r",
    "title": "Bayesian Data Analysis course - Aalto 2024",
    "section": "R",
    "text": "R\nR is used in the course as there are more packages for Stan and statistical analysis. Unless you are already experienced and have figured out your preferred way to work with R, we recommend\n\nuse RStudio in JupyterHub\nor install RStudio Desktop locally,\n\nSee FAQ for frequently asked questions about R problems in this course. The demo codes provide useful starting points for all the assignments.\n\nFor learning R programming basics\n\nR Bootcamp Very short interactive tutorial for ggplot2 and nadling dataframes.\nGarrett Grolemund, Hands-On Programming with R\n\nFor learning basic and advanced plotting using R\n\nAndrew Heiss, Data Visualiztion with R, Interactive lessons\nRobert Kabacoff, Modern Data Visualization with R, Online book\nKieran Healy, Data Visualization - A practical introduction\nAntony Unwin, Graphical Data Analysis with R"
  },
  {
    "objectID": "Aalto2024.html#english-finnish-english-statistics-dictionary",
    "href": "Aalto2024.html#english-finnish-english-statistics-dictionary",
    "title": "Bayesian Data Analysis course - Aalto 2024",
    "section": "English-Finnish-English statistics dictionary",
    "text": "English-Finnish-English statistics dictionary\nExcellent online English-Finnish-English statistics dictionary:\n\nJuha Alho, Elja Arjas, Esa Läärä ja Pekka Pere (2023). Tilastotieteen sanasto. Suomen Tilastoseuran julkaisuja no. 8. 2. laitos. Suomen Tilastoseura. Helsinki. https://sanasto.tilastoseura.fi/\n\nShorter English-Finnish dictionary for the terms specific for this course\n\nLyhyt englanti-suomi sanasto kurssin termeistä\n\nSanasta “bayesilainen” esiintyy Suomessa muutamaa erilaista kirjoitustapaa. Olen käyttänyt muotoa “bayesilainen”, joka on muodostettu yleisen vieraskielisten nimien taivutussääntöjen mukaan: “Jos nimi on kirjoitettuna takavokaalinen mutta äännettynä etuvokaalinen, kirjoitetaan päätteseen tavallisesti takavokaali etuvokaalin sijasta, esim. Birminghamissa, Thamesilla.” Terho Itkonen, Kieliopas, 6. painos, Kirjayhtymä, 1997.\nSuomen tilastoseura sen sijaan suosittaa muotoa “bayseiläinen”. Heidän perustelunsa löytyy Tilastotieteen sanastosta (ks. linkki yllä). Tilastotieteen sanaston verkkoversiossa on hakutoiminto, ja PDF-versio sisältää käännösten perusteluita sekä hieman tilastotieteen varhaista historiaa Suomessa."
  },
  {
    "objectID": "Aalto2023.html",
    "href": "Aalto2023.html",
    "title": "Bayesian Data Analysis course - Aalto 2023",
    "section": "",
    "text": "Aalto 2023 course can be taken online except for the final project presentation. The lectures will be given on campus, but recorded and the recording will be made available online after the lecture. If you are unable to register for the course at the moment in the Sisu, there is no need to email the lecturer. You can start taking the course and register before the end of the course. Sisu shows rooms on campus for the computer exercises, and you can come to ask questions on campus, but you can also ask in Zulip during the same times. You can choose which TA session to join each week separately, without a need to register for those sessions.\nAll the course material is available in a git repo (and these pages are for easier navigation). All the material can be used in other courses. Text and videos licensed under CC-BY-NC 4.0. Code licensed under BSD-3.\nThe material will be updated during the course. Exercise instructions and slides will be updated at latest on Monday of the corresponding week. The updated material will appear on the web pages, or you can clone the repo and pull before checking new material. If you don’t want to learn git, you can download the latest zip file."
  },
  {
    "objectID": "Aalto2023.html#book-bda3",
    "href": "Aalto2023.html#book-bda3",
    "title": "Bayesian Data Analysis course - Aalto 2023",
    "section": "Book: BDA3",
    "text": "Book: BDA3\n\n\n\nThe electronic version of the course book Bayesian Data Analysis, 3rd ed, by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin is available for non-commercial purposes. Hard copies are available from the publisher and many book stores. Aalto library has also copies. See also home page for the book, errata for the book, and chapter notes."
  },
  {
    "objectID": "Aalto2023.html#prerequisites",
    "href": "Aalto2023.html#prerequisites",
    "title": "Bayesian Data Analysis course - Aalto 2023",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic terms of probability theory\n\nprobability, probability density, distribution\nsum, product rule, and Bayes’ rule\nexpectation, mean, variance, median\nin Finnish, see e.g. Stokastiikka ja tilastollinen ajattelu\nin English, see e.g. Wikipedia and Introduction to probability and statistics\n\nSome algebra and calculus\nBasic visualisation techniques (R or Python)\n\nhistogram, density plot, scatter plot\nsee e.g. BDA R demos\nsee e.g. BDA Python demos\n\n\nThis course has been designed so that there is strong emphasis in computational aspects of Bayesian data analysis and using the latest computational tools. The project brings together the overall Bayesian workflow aspects.\nIf you find BDA3 too difficult to start with, I recommend\n\nFor regression models, their connection to statistical testing and causal analysis see Gelman, Hill and Vehtari: “Regression and Other Stories”.\nRichard McElreath: Statistical Rethinking, 2nd ed book is easier than BDA3 and the 2nd ed is excellent. Statistical Rethinking doesn’t go as deep in some details, math, algorithms and programming as BDA course. Richard’s lecture videos of Statistical Rethinking: A Bayesian Course Using R and Stan are highly recommended even if you are following BDA3.\nJohnson, Ott, and Dogucu: Bayes Rules! An Introduction to Applied Bayesian Modeling\nFor background prerequisites some students have found chapters 2, 4 and 5 in Kruschke, “Doing Bayesian Data Analysis” useful."
  },
  {
    "objectID": "Aalto2023.html#communication-channels",
    "href": "Aalto2023.html#communication-channels",
    "title": "Bayesian Data Analysis course - Aalto 2023",
    "section": "Communication channels",
    "text": "Communication channels\n\nMyCourses is used for some intial announcements, linking to Zulip and Peergrade, and some questionnaires.\nThe primary communication channel is the Zulip course chat (link in MyCourses, login with Aalto account)\n\nDon’t ask via email or direct messages. By asking via common streams in the course chat, more eyes will see your question, it will get answered faster and it’s likely that other students benefit from the answer.\nLogin with Aalto account to the Zulip course chat. You can adjust the notifications in the settings.\nIf you have any questions, please ask in the public streams and get answers from course staff or other students (active students helping others will get bonus points).\nIn the chat system, we will have separate streams for each assignment and the project.\nStream #general can be used for any kind of general discussions and questions related to the course.\nAll important announcements will be posted to #announcements (no discussion on this stream).\nAny kind of feedback is welcome on stream #feedback.\nWe have also streams #r, #python, and #stan for questions that are not specific to assignments or the project.\nStream #queue is used as a queue for getting help during TA sessions.\nThe lecturer and teaching assistants have names with “(staff)” or “(TA)” in the end of their names.\n\nA weekly lecture time on campus includes times for questions and answers\nIf you need one-to-one help, please take part in the TA sessions and ask there.\nIf you find errors in material, post in #feedback stream or submit an issue in github.\nPeergrade alerts: If you are worried that you forget the deadlines, you can set peergade to send you email when assignment opens for submission, 24 hours before assignment close for submission, assignment is open for reviewing, 24 hours before an assignment closes for reviewing if you haven’t started yet, someone likes my feedback (once a day). Click your name -&gt; User Settings to choose which alerts you want."
  },
  {
    "objectID": "Aalto2023.html#assessment",
    "href": "Aalto2023.html#assessment",
    "title": "Bayesian Data Analysis course - Aalto 2023",
    "section": "Assessment",
    "text": "Assessment\nAssignments (60%) and a project work with presentation (40%). Minimum of 50% of points must be obtained from both the assignments and project work. You can get bonus points from chat activity (e.g. helping other students and reporting typos in the material) and answering time usage questionnaries."
  },
  {
    "objectID": "Aalto2023.html#schedule-2023",
    "href": "Aalto2023.html#schedule-2023",
    "title": "Bayesian Data Analysis course - Aalto 2023",
    "section": "Schedule 2023",
    "text": "Schedule 2023\nThe course consists of 12 lectures, 9 assignments, a project work, and a project presentation in periods I and II. It’s good start reading the material for the next lecture and assignment while making the assignment related to the previous lecture. There are 9 assignments and a project work with presentation, and thus the assignments are not in one-to-one correspondence with the lectures. The schedule below lists the lectures and how they connect to the topics, book chapters and assignments.\n\nSchedule overview\nHere is an overview of the schedule. Scroll down the page to see detailed instructions for each block. When you are working on assignment related to previous lecture, it is good to start reading the book chapters relaed to the next lecture and assignment.\n\n\n\n\nReadings\nLectures\nAssignment\nLecture Date\nAssignment due date\n\n\n\n\n1. Introduction\nBDA3 Chapter 1\n2023 Lecture 1.1 Introduction,  2023 Lecture 1.2 Course practicalities, Slides 1.1, Slides 1.2\nAssignment 1\n2023-09-04\n2023-09-10\n\n\n2. Basics of Bayesian inference\nBDA3 Chapter 1,  BDA3 Chapter 2\n2023 Lecture 2.1,  2023 Lecture 2.2, Slides 2\nAssignment 2\n2023-09-11\n2023-09-17\n\n\n3. Multidimensional posterior\nBDA3 Chapter 3\n2023 Lecture 3.1, 2023 Lecture 3.2Slides 3\nAssignment 3\n2023-09-18\n2023-09-24\n\n\n4. Monte Carlo\nBDA3 Chapter 10\n2023 Lecture 4.1, 2023 Lecture 4.2, Slides 4\nAssignment 4\n2023-09-25\n2023-10-01\n\n\n5. Markov chain Monte Carlo\nBDA3 Chapter 11\n2023 Lecture 5.1,  2023 Lecture 5.2, Slides 5\nAssignment 5\n2023-10-02\n2023-10-08\n\n\n6. Stan, HMC, PPL\nBDA3 Chapter 12 + extra material on Stan\n2023 Lecture 6.1, 2023 Lecture 6.2, Slides 6\nAssignment 6\n2023-10-09\n2023-10-22\n\n\n7. Hierarchical models and exchangeability\nBDA3 Chapter 5\n2023 Lecture 7.1,  2023 Lecture 7.2, 2022 Project info, Slides 7\nAssignment 7\n2023-10-23\n2023-11-05\n\n\n8. Model checking & cross-validation\nBDA3 Chapter 6, BDA3 Chapter 7, Visualization in Bayesian workflow, Practical Bayesian cross-validation\n2023 Lecture 8.1,  2023 Lecture 8.2, Slides 8a,Slides 8b\nStart project work\n2023-10-30\nN/A\n\n\n9. Model comparison, selection, and hypothesis testing\nBDA3 Chapter 7 (not 7.2 and 7.3),  Practical Bayesian cross-validation\n2023 Lecture 9.1,  2023 Lecture 9.2, Slides 9\nAssignment 8\n2023-11-06\n2023-11-12\n\n\n10. Decision analysis\nBDA3 Chapter 9\n2023 Lecture 10.1, 2023 Lecture 10.2, Slides 10a, Slides 10b\nAssignment 9\n2023-11-13\n2023-11-19\n\n\n11. Variable selectio with projpred, project presentation example\nBDA3 Chapter 4\n2023 Lecture 11.1, 2023 Lecture 11.2, 2023 Lecture 11.3, Slides 11a, Slides Project Presentation, Slides 11 extra\nProject work\n2023-11-20\nN/A\n\n\n12. TBA\n\nOptional:   \nProject work\n2023-11-27\nN/A\n\n\n13. Project evaluation\n\n\n\nProject presentations: 11.-15.12.\nEvaluation week\n\n\n\n\n\n1) Course introduction, BDA 3 Ch 1, prerequisites assignment\nCourse practicalities, material, assignments, project work, peergrading, QA sessions, TA sessions, prerequisites, chat, etc.\n\nLogin with Aalto account to the Zulip course chat with link in MyCourses\nSignin to Peergrade with link in MyCourses.\nIntroduction/practicalities lecture Monday 2023-09-04 14:15-16, hall C, Otakaari 1**\n\n2023 Lecture videos 1.1 and 1.2 in Panopto\nSlides 1.1, Slides 1.2\n\nRead BDA3 Chapter 1\n\nstart with reading instructions for Chapter 1 and afterwards read the additional comments in the same document\n\nThere are no R/Python demos for Chapter 1\nMake and submit Assignment 1. Deadline Sunday 2023-09-10 23:59\n\nWe highly recommend to submit all assignments Friday before 3pm so that you can get TA help before submission. As the course has students who work weekdays (e.g. FiTech students), the late submission until Sunday night is allowed, but we can’t provide support during the weekends.\nthis assignment checks that you have sufficient prerequisite skills (basic probability calculus, and R or Python)\nGeneral information about assignments\n\nR markdown template for assignments\nFAQ for the assignments has solutions to commonly asked questions related RStudio setup, errors during package installations, etc.\n\n\nGet help in TA sessions 2023-09-06 14-16, 2023-09-07 12-14, 2023-09-08 10-12\n\nin Sisu these are marked as exercise sessions, but we call them TA sessions\nthese are optional and you can choose which one to join\nsee more info about TA sessions\n\nOptional: Make BDA3 exercises 1.1-1.4, 1.6-1.8 (model solutions available for 1.1-1.6)\nStart reading Chapters 1+2, see instructions below\n\n\n\n2) BDA3 Ch 1+2, basics of Bayesian inference\nBDA3 Chapters 1+2, basics of Bayesian inference, observation model, likelihood, posterior and binomial model, predictive distribution and benefit of integration, priors and prior information, and one parameter normal model.\n\nRead BDA3 Chapter 2\n\nsee reading instructions for Chapter 2\n\nLecture Monday 2023-09-11 14:15-16, hall T1, CS building\n\nSlides 2\nVideos: 2023 Lecture 2.1, 2023 Lecture 2.2 on basics of Bayesian inference, observation model, likelihood, posterior and binomial model, predictive distribution and benefit of integration, priors and prior information, and one parameter normal model. BDA3 Ch 1+2.\n\nRead the additional comments for Chapter 2\nCheck R demos or Python demos for Chapter 2\nMake and submit Assignment 2. Deadline Sunday 2023-09-17 23:59\n\nReview Assignment 1 done by your peers before 23:59 2023-09-13\nReflect on your feedback\nTA sessions 2023-09-13 12-14, 2023-09-14 12-14, 2023-09-15 12-14.\n\nOptional: Make BDA3 exercises 2.1-2.5, 2.8, 2.9, 2.14, 2.17, 2.22 (model solutions available for 2.1-2.5, 2.7-2.13, 2.16, 2.17, 2.20, and 2.14 is in course slides)\nStart reading Chapter 3, see instructions below\n\n\n\n3) BDA3 Ch 3, multidimensional posterior\nMultiparameter models, joint, marginal and conditional distribution, normal model, bioassay example, grid sampling and grid evaluation. BDA3 Ch 3.\n\nRead BDA3 Chapter 3\n\nsee reading instructions for Chapter 3\n\nLecture Monday 2023-09-18. 14:15-16, hall T1, CS building\n\nSlides 3\nVideos: 2023 Lecture 3.1 2023 Lecture 3.2 on multiparameter models, joint, marginal and conditional distribution, normal model, bioassay example, grid sampling and grid evaluation. BDA3 Ch 3.\n\nRead the additional comments for Chapter 3\nCheck R demos or Python demos for Chapter 3\nMake and submit Assignment 3. Deadline Sunday 2023-09-24 23:59\n\nReview Assignment 2 done by your peers before 23:59 2023-09-20, and reflect on your feedback.\n\nTA sessions 2023-09-20 12-14, 2023-09-21 12-14, 2023-09-22 12-14.\nOptional: Make BDA3 exercises 3.2, 3.3, 3.9 (model solutions available for 3.1-3.3, 3.5, 3.9, 3.10)\nStart reading Chapter 10, see instructions below\n\n\n\n4) BDA3 Ch 10, Monte Carlo\nNumerical issues, Monte Carlo, how many simulation draws are needed, how many digits to report, direct simulation, curse of dimensionality, rejection sampling, and importance sampling. BDA3 Ch 10.\n\nRead BDA3 Chapter 10\n\nsee reading instructions for Chapter 10\n\nLecture Monday 2023-09-25 14:15-16, hall T1, CS building\n\nSlides 4\nVideos: 2023 Lecture 4.1 on numerical issues, Monte Carlo, how many simulation draws are needed, how many digits to report, and 2023 Lecture 4.2 on direct simulation, curse of dimensionality, rejection sampling, and importance sampling. BDA3 Ch 10.\n\nRead the additional comments for Chapter 10\nCheck R demos or Python demos for Chapter 10\nMake and submit Assignment 4. Deadline Sunday 2023-10-01 23:59\n\nReview Assignment 3 done by your peers before 23:59 2023-09-27, and reflect on your feedback\n\nTA sessions 2023-09-27 12-14, 2023-09-28 12-14, 2023-09-29 12-14.\nOptional: Make BDA3 exercises 10.1, 10.2 (model solution available for 10.4)\nStart reading Chapter 11, see instructions below\n\n\n\n5) BDA3 Ch 11, Markov chain Monte Carlo\nMarkov chain Monte Carlo, Gibbs sampling, Metropolis algorithm, warm-up, convergence diagnostics, R-hat, and effective sample size. BDA3 Ch 11.\n\nRead BDA3 Chapter 11\n\nsee reading instructions for Chapter 11\n\nLecture Monday 2023-10-02 14:15-16, hall T1, CS building\n\nSlides 5\nVideos: 2023 Lecture 5.1 on Markov chain Monte Carlo, Gibbs sampling, Metropolis algorithm, and 2023 Lecture 5.2 on warm-up, convergence diagnostics, R-hat, and effective sample size.\n\nRead the additional comments for Chapter 11\nCheck R demos or Python demos for Chapter 11\nMake and submit Assignment 5. Deadline Sunday 2023-10-08 23:59\n\nReview Assignment 4 done by your peers before 23:59 2023-10-04, and reflect on your feedback\n\nTA sessions 2023-10-04 12-14, 2023-10-05 12-14, 2023-10-06 12-14.\nOptional: Make BDA3 exercise 11.1 (model solution available for 11.1)\nStart reading Chapter 12 + Stan material, see instructions below\n\n\n\n6) BDA3 Ch 12 + Stan, HMC, PPL, Stan\nHMC, NUTS, dynamic HMC and HMC specific convergence diagnostics, probabilistic programming and Stan. BDA3 Ch 12 + extra material\n\nRead BDA3 Chapter 12\n\nsee reading instructions for Chapter 12\n\nLecture Monday 2023-10-09 14:15-16, hall T1, CS building\n\nSlides 6\nVideos: 2022 Lecture 6.1 on HMC, NUTS, dynamic HMC and HMC specific convergence diagnostics, and 2022 Lecture 6.2 on probabilistic programming and Stan. BDA3 Ch 12 + extra material.\nOptional: Stan Extra introduction recorded 2020 Golf putting example, main features of Stan, benefits of probabilistic programming, and comparison to some other software.\n\nRead the additional comments for Chapter 12\nRead Stan introduction article\nCheck R demos for RStan or Python demos for PyStan\nAdditional material for Stan:\n\nDocumentation\nRStan installation\nPyStan installation\nBasics of Bayesian inference and Stan, Jonah Gabry & Lauren Kennedy Part 1 and Part 2\n\nMake and submit Assignment 6. DeadlineSunday 2023-10-22 23:59 (two weeks for this assignment)\n\nReview Assignment 5 done by your peers before 23:59 2023-10-11, and reflect on your feedback\n\nTA sessions 2023-10-11 14-16, 2023-10-12 12-14, 2023-10-13 10-12.\nStart reading Chapter 5 + Stan material, see instructions below\nNo Lecture on evaluation week.\n\n\n\n7) BDA3 Ch 5, hierarchical models\nHierarchical models and exchangeability. BDA3 Ch 5.\n\nRead BDA3 Chapter 5\n\nsee reading instructions for Chapter 5\n\nLecture Monday 2023-10-23 14:15-16, hall T2, CS building\n\nSlides 7\nVideos: 2023 Lecture 7.1 on hierarchical models, 2023 Lecture 7.2 on exchangeability.\n\nRead the additional comments for Chapter 5\nCheck R demos or Python demos for Chapter 5\nMake and submit Assignment 7. Deadline Sunday 2023-11-05 23:59 (two weeks for this assignment)\n\nReview Assignment 6 done by your peers before 23:59 2023-10-25, and reflect on your feedback\n\nTA sessions 2023-10-25 14-16, 2023-10-26 12-14, 2023-10-27 10-12.\nOptional: Make BDA3 exercises 5.1 and 5.2 (model solution available for 5.3-5.5, 5.7-5.12)\nStart reading Chapters 6-7 and additional material, see instructions below.\n\n\n\n8) BDA3 Ch 6+7 + extra material, model checking, cross-validation\nModel checking and cross-validation.\n\nRead BDA3 Chapters 6 and 7 (skip 7.2 and 7.3)\n\nsee reading instructions for Chapter 6 and Chapter 7\n\nRead Visualization in Bayesian workflow\n\nmore about workflow and examples of prior predictive checking and LOO-CV probability integral transformations\n\nRead Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC (Journal link)\n\nreplaces BDA3 Sections 7.2 and 7.3 on cross-validation\n\nLecture Monday 2023-10-30 14:15-16, hall T2, CS building\n\nSlides 8a, Slides 8b\nVideos: 2022 Lecture 8.1 on model checking, and 2023 Lecture 8.2 on cross-validation part 1. BDA3 Ch 6-7 + extra material.\n\nRead the additional comments for Chapter 6 and Chapter 7\nCheck R demos or Python demos for Chapter 6\nAdditional reading material\n\nCross-validation FAQ\n\nNo new assignment in this block\nStart the project work\nTA sessions 2023-11-01 14-16, 2023-11-02 12-14, 2023-11-03 10-12.\nOptional: Make BDA3 exercise 6.1 (model solution available for 6.1, 6.5-6.7)\n\n\n\n9) BDA3 Ch 7, extra material, model comparison and selection\nPSIS-LOO, K-fold-CV, model comparison and selection. Extra lecture on variable selection with projection predictive variable selection.\n\nRead Chapter 7 (no 7.2 and 7.3)\n\nsee reading instructions for Chapter 7\n\nRead Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC (Journal link)\n\nreplaces BDA3 Sections 7.2 and 7.3 on cross-validation\n\nLecture Monday 2023-11-06 14:15-16, hall T2, CS building\n\nSlides 9\nVideos: 2023 Lecture 9.1 and 2023 Lecture 9.2 on model comparison, selection, and hypothesis testing.\n\nAdditional reading material\n\nCV FAQ\n\nMake and submit Assignment 8. Sunday 2023-11-12 23:59\n\nReview Assignment 7 done by your peers before 23:59 2023-11-08, and reflect on your feedback\n\nTA sessions 2023-11-08 14-16, 2023-11-09 12-14, 2023-11-10 10-12.\nStart reading Chapter 9, see instructions below.\n\n\n\n10) BDA3 Ch 9, decision analysis + BDA3 Ch 4 Laplace approximation and asymptotics\nDecision analysis. BDA3 Ch 9. + Laplace approximation and asymptotics. BDA Ch 4.\n\nRead Chapter 9 and 4\n\nsee reading instructions for Chapter 9\nsee reading instructions for Chapter 4\n\nLecture Monday 2023-11-13 14:15-16, hall T2, CS building\n\nSlides 10a, Slides 10b\nVideos: 2023 Lecture 10.1 on decision analysis. BDA3 Ch 9, and 2023 Lecture 10.2 on Laplace approximation, and asymptotics, BDA3 Ch 4.\n\nMake and submit Assignment 9. Sunday 2023-11-19 23:59\n\nReview Assignment 8 done by your peers before 23:59 N/A, and reflect on your feedback\n\nTA sessions 2023-11-15 14-16, 2023-11-16 12-14, 2023-11-17 10-12.\nStart reading Chapter 4, see instructions below.\n\n\n\n11) Variable selection with projpred, project presentation example, extra\n\nLecture Monday 2023-11-20 14:15-16, hall T2, CS building\n\nSlides 11a, Slides Project Presentation, Slides 11 extra\nVideos: 2023 Lecture 11.1 on variable selecion with projpred, 2023 Lecture 11.2 on project presentations, 2023 Lecture 11.3 on rest of BDA3, ROS, and Bayesian Workflow\n\nNo new assignment. Work on project. TAs help with projects.\n\nReview Assignment 9 done by your peers before 23:59 2023-11-15, and reflect on your feedback\n\nTA sessions 2023-11-22 14-16, 2023-11-23 12-14, 2023-11-24 10-12.\n\n\n\n12) TBA\n\nLecture Monday 2023-11-27 14:15-16, hall T2, CS building\n\nSlides 12\n\nTBA\nWork on project. TAs help with projects. Project deadline 3.12. 23:59\nTA sessions 2023-11-29 14-16, 2023-11-30 12-14, 2023-12-01 10-12.\n\n\n\n13) Project evaluation\n\nProject report deadline 3.12. 23:59 (submit to peergrade).\n\nReview project reports done by your peers before 7.12. 23:59, and reflect on your feedback.\n\nProject presentations 11.-15.12. (evaluation week)"
  },
  {
    "objectID": "Aalto2023.html#r-and-python",
    "href": "Aalto2023.html#r-and-python",
    "title": "Bayesian Data Analysis course - Aalto 2023",
    "section": "R and Python",
    "text": "R and Python\nWe strongly recommend using R in the course as there are more packages for Stan and statistical analysis in R. If you are already fluent in Python, but not in R, then using Python may be easier, but it can still be more useful to learn also R. Unless you are already experienced and have figured out your preferred way to work with R, we recommend\n\nuse RStudio remotely\nor install RStudio Desktop,\n\nSee FAQ for frequently asked questions about R problems in this course. The demo codes provide useful starting points for all the assignments.\n\nFor learning R programming basics\n\nGarrett Grolemund, Hands-On Programming with R\n\nFor learning basic and advanced plotting using R\n\nKieran Healy, Data Visualization - A practical introduction\nAntony Unwin, Graphical Data Analysis with R"
  },
  {
    "objectID": "Aalto2023.html#english-finnish-english-statistics-dictionary",
    "href": "Aalto2023.html#english-finnish-english-statistics-dictionary",
    "title": "Bayesian Data Analysis course - Aalto 2023",
    "section": "English-Finnish-English statistics dictionary",
    "text": "English-Finnish-English statistics dictionary\nExcellent online English-Finnish-English statistics dictionary:\n\nJuha Alho, Elja Arjas, Esa Läärä ja Pekka Pere (2023). Tilastotieteen sanasto. Suomen Tilastoseuran julkaisuja no. 8. 2. laitos. Suomen Tilastoseura. Helsinki. https://sanasto.tilastoseura.fi/\n\nShorter English-Finnish dictionary for the terms specific for this course\n\nLyhyt englanti-suomi sanasto kurssin termeistä\n\nSanasta “bayesilainen” esiintyy Suomessa muutamaa erilaista kirjoitustapaa. Olen käyttänyt muotoa “bayesilainen”, joka on muodostettu yleisen vieraskielisten nimien taivutussääntöjen mukaan: “Jos nimi on kirjoitettuna takavokaalinen mutta äännettynä etuvokaalinen, kirjoitetaan päätteseen tavallisesti takavokaali etuvokaalin sijasta, esim. Birminghamissa, Thamesilla.” Terho Itkonen, Kieliopas, 6. painos, Kirjayhtymä, 1997.\nSuomen tilastoseura sen sijaan suosittaa muotoa “bayseiläinen”. Heidän perustelunsa löytyy Tilastotieteen sanastosta (ks. linkki yllä). Tilastotieteen sanaston verkkoversiossa on hakutoiminto, ja PDF-versio sisältää käännösten perusteluita sekä hieman tilastotieteen varhaista historiaa Suomessa."
  },
  {
    "objectID": "BDA3_notes.html",
    "href": "BDA3_notes.html",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "",
    "text": "These notes help you to focus on the most important parts of each chapter related o the Bayesian Data Analysis course. Before reading a chapter, you can check below which sections, pages, and terms are the most important. After reading the chapter or following the corresponding lecture, you can check here for additional clarifications. There also some notes for the chapters not included in the course."
  },
  {
    "objectID": "BDA3_notes.html#ch1",
    "href": "BDA3_notes.html#ch1",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 1 Probability and inference",
    "text": "Chapter 1 Probability and inference\nChapter 1 is related to the pre-requisites and Lecture 1 Introduction.\n\nOutline\n\n1.1-1.3 important terms, especially 1.3 for the notation\n1.4 an example related to the first exercise, and another practical example\n1.5 foundations\n1.6 good example related to visualization exercise\n1.7 example which can be skipped\n1.8 background material, good to read before doing the first assignment\n1.9 background material, good to read before doing the second assignment\n1.10 a point of view for using Bayesian inference\n\n\n\nThe most important terms\nFind all the terms and symbols listed below. Note that some of the terms are now only briefly introduced and will be covered later in more detail. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.\n\nfull probability model\nposterior distribution\npotentially observable quantity\nquantities that are not directly observable\nexchangeability\nindependently and identically distributed\n\\(\\theta, y, \\tilde{y}, x, X, p(\\cdot|\\cdot), p(\\cdot), \\operatorname{Pr}(\\cdot), \\sim, H\\)\nsd, E, var\nBayes rule\nprior distribution\nsampling distribution, data distribution\njoint probability distribution\nposterior density\nprobability\ndensity\ndistribution\n\\(p(y|\\theta)\\) as a function of \\(y\\) or \\(\\theta\\)\nlikelihood\nposterior predictive distribution\nprobability as measure of uncertainty\nsubjectivity and objectivity\ntransformation of variables\nsimulation\ninverse cumulative distribution function\n\n\n\nRecommended exercises\nOptional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:\n\n1.1-1.4, 1.6-1.8 (model solutions available for 1.1-1.6)\n\n\n\nDistributed as, \\(\\sim\\)\nIt is common to write statistical models using a following notation: \\[\n\\begin{aligned}\ny & \\sim \\mathrm{normal}(\\mu, \\sigma) \\\\\n\\mu & \\sim \\mathrm{normal}(0, 10) \\\\\n\\sigma & \\sim \\mathrm{normal}^+(0, 1),\n\\end{aligned}\n\\] where the symbol \\(\\sim\\) is called tilde (\\sim in LateX). In general, we can read \\(\\sim\\) as “is distributed as,” and overall this notation is used as a shorthand for defining distributions, so that the above example can be written (with also as \\[\n\\begin{aligned}\n   p(y| \\mu, \\sigma) & = \\mathrm{normal}(y |  \\mu, \\sigma)\\\\\n   p(\\mu) & = \\mathrm{normal}(\\mu |  0, 10)\\\\\n   p(\\sigma) & = \\mathrm{normal}^+(\\sigma |  0, 1).\n\\end{aligned}\n\\]\nA collection of distribution statements define a joint distribution as the product of component distributions \\[\np(y,\\mu,\\sigma) = p(y| \\mu, \\sigma )p(\\mu) p(\\sigma).\n\\]\n\n\nProportional to, \\(\\propto\\)\nThe symbol \\(\\propto\\) means proportional to, which means left hand side is equal to right hand size given a constant multiplier. For instance if \\(y=2x\\), then \\(y \\propto x\\). It’s \\ propto in LaTeX. See Proportionality in Wikipedia.\nThe Bayes rule is \\[\np(\\theta | y) = \\frac{p(y | \\theta)p(\\theta)}{p(y)},\n\\] where dividing by \\(p(y)\\) makes \\(\\int p(\\theta | y) d\\theta = 1\\). \\(p(y)\\) is often infeasible to compute, but luckily we also often don’t need to know it, and then we write the Bayes rule as \\[\np(\\theta | y) \\propto p(y | \\theta)p(\\theta).\n\\]\n\n\nModel and likelihood\nTerm \\(p(y|\\theta,M)\\) has two different names depending on the situation. Due to the short notation used, there is possibility of confusion.\n\nTerm \\(p(y|\\theta,M)\\) is called a model (sometimes more specifically observation model or statistical model) when it is used to describe uncertainty about \\(y\\) given \\(\\theta\\) and \\(M\\). Longer notation \\(p_y(y|\\theta,M)\\) shows explicitly that it is a function of \\(y\\).\nIn Bayes rule, the term \\(p(y|\\theta,M)\\) is called likelihood function. Posterior distribution describes the probability (or probability density) for different values of \\(\\theta\\) given a fixed \\(y\\), and thus when the posterior is computed the terms on the right hand side (in Bayes rule) are also evaluated as a function of \\(\\theta\\) given fixed \\(y\\). Longer notation \\(p_\\theta(y|\\theta,M)\\) shows explicitly that it is a function of \\(\\theta\\). Term has it’s own name (likelihood) to make the difference to the model. The likelihood function is unnormalized probability distribution describing uncertainty related to \\(\\theta\\) (and that’s why Bayes rule has the normalization term to get the posterior distribution).\n\n\n\nTwo types of uncertainty\nEpistemic and aleatory uncertainty are reviewed nicely in the article: Tony O’Hagan, ``Dicing with the unknown’’ Significance 1(3):132-133, 2004.\nIn that paper, there is one typo using the word aleatory instead of epistemic (if you notice this, it’s then quite obvious).\n\n\nTransformation of variables\n\nSee BDA3 p. 21\n\n\n\nAmbiguous notation in statistics\n\nIn \\(p(y|\\theta)\\)\n\n\\(y\\) can be variable or value\n\nwe could clarify by using \\(p(Y|\\theta)\\) or \\(p(y|\\theta)\\)\n\n\\(\\theta\\) can be variable or value\n\nwe could clarify by using \\(p(y|\\Theta)\\) or \\(p(y|\\theta)\\)\n\n\\(p\\) can be a discrete or continuous function of \\(y\\) or \\(\\theta\\)\n\nwe could clarify by using \\(P_Y\\), \\(P_\\Theta\\), \\(p_Y\\) or \\(p_\\Theta\\)\n\n\\(P_Y(Y|\\Theta=\\theta)\\) is a probability mass function, sampling distribution, observation model\n\\(P(Y=y|\\Theta=\\theta)\\) is a probability\n\\(P_\\Theta(Y=y|\\Theta)\\) is a likelihood function (can be discrete or continuous)\n\\(p_Y(Y|\\Theta=\\theta)\\) is a probability density function, sampling distribution, observation model\n\\(p(Y=y|\\Theta=\\theta)\\) is a density\n\\(p_\\Theta(Y=y|\\Theta)\\) is a likelihood function (can be discrete or continuous)\n\\(y\\) and \\(\\theta\\) can also be mix of continuous and discrete\nDue to the sloppiness sometimes likelihood is used to refer \\(P_{Y,\\theta}(Y|\\Theta)\\), \\(p_{Y,\\theta}(Y|\\Theta)\\)\n\n\n\n\nExchangeability\nYou don’t need to understand or use the term exchangeability before Chapter 5 and Lecture 7. At this point and until Chapter 5 and Lecture 7, it is sufficient that you know that 1) independence is stronger condition than exchangeability, 2) independence implies exchangeability, 3) exchangeability does not imply independence, 4) exchangeability is related to what information is available instead of the properties of unknown underlying data generating mechanism. If you want to know more about exchangeability right now, then read BDA Section 5.2 and notes for Chapter 5."
  },
  {
    "objectID": "BDA3_notes.html#ch2",
    "href": "BDA3_notes.html#ch2",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 2 Single-parameter models",
    "text": "Chapter 2 Single-parameter models\nChapter 2 is related to the prerequisites and Lecture 2 Basics of Bayesian inference.\n\nOutline\n\n2.1 Binomial model (e.g. biased coin flipping)\n2.2 Posterior as compromise between data and prior information\n2.3 Posterior summaries\n2.4 Informative prior distributions (skip exponential families and sufficient statistics)\n2.5 Gaussian model with known variance\n2.6 Other single parameter models\n\nin this course the normal distribution with known mean but unknown variance is the most important\nglance through Poisson and exponential\n\n2.7 glance through this example, which illustrates benefits of prior information, no need to read all the details (it’s quite long example)\n2.8 Noninformative priors\n2.9 Weakly informative priors\n\nLaplace’s approach for approximating integrals is discussed in more detail in Chapter 4.\n\n\nThe most important terms\nFind all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.\n\nbinomial model\nBernoulli trial\n\\(\\mathop{\\mathrm{Bin}}\\), \\(\\binom{n}{y}\\)\nLaplace’s law of succession\nthink which expectations in eqs. 2.7-2.8\nsummarizing posterior inference\nmode, mean, median, standard deviation, variance, quantile\ncentral posterior interval\nhighest posterior density interval / region\nuninformative / informative prior distribution\nprinciple of insufficient reason\nhyperparameter\nconjugacy, conjugate family, conjugate prior distribution, natural conjugate prior\nnonconjugate prior\nnormal distribution\nconjugate prior for mean of normal distribution with known variance\nposterior for mean of normal distribution with known variance\nprecision\nposterior predictive distribution\nnormal model with known mean but unknown variance\nproper and improper prior\nunnormalized density\ndifficulties with noninformative priors\nweakly informative priors\n\n\n\nR and Python demos\n\n2.1: Binomial model and Beta posterior. R. Python.\n2.2: Comparison of posterior distributions with different parameter values for the Beta prior distribution. R. Python.\n2.3: Use samples to plot histogram with quantiles, and the same for a transformed variable. R. Python.\n2.4: Grid sampling using inverse-cdf method. R. Python.\n\n\n\nRecommended exercises\nOptional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:\n\n2.1-2.5, 2.8, 2.9, 2.14, 2.17, 2.22 (model solutions available for 2.1-2.5, 2.7-2.13, 2.16, 2.17, 2.20, and 2.14 is in course slides)\n\n\n\nPosterior, credible, and confidence intervals\n\nConfidence interval is used in frequentist statistics and not used in Bayesian statistics, but we mention it here to make that fact explicit. Given a confidence level $ (95% and 99% are typical values), a confidence interval is a random interval which contains the parameter being estimated \\(\\gamma\\)% of the time (Wikipedia).\nCredible interval is used in Bayesian statistics (and by choice the acronym is CI as for the confidence interval). Credible interval is defined such that an unobserved parameter value has a particular probability \\(\\alpha\\) to fall within it (Wikipedia). Credible interval can be used to characterize any probability distribution.\nPosterior interval is a credible interval specifically characterizing posterior distribution.\n\n\n\nIntegration over Beta distribution\nChapter 2 has an example of analysing the ratio of girls born in Paris 1745–1770. Laplace used binomial model and uniform prior which produces Beta distribution as posterior distribution. Laplace wanted to calculate \\(p(\\theta \\geq 0.5)\\), which is obtained as \\[\n\\begin{aligned}\n  p(\\theta \\geq 0.5) &=& \\int_{0.5}^1  p(\\mathbf{\\theta}|y,n,M) d\\theta \\\\\n  &=& \\frac{493473!}{241945!251527!} \\int_{0.5}^1 \\theta^y(1-\\theta)^{n-y} d\\theta\\end{aligned}\n\\] Note that \\(\\Gamma(n)=(n-1)!\\). Integral has a form which is called incomplete Beta function. Bayes and Laplace had difficulties in computing this, but nowadays there are several series and continued fraction expressions. Furthermore usually the normalization term is computed by computing \\(\\log(\\Gamma(\\cdot))\\) directly without explicitly computing \\(\\Gamma(\\cdot)\\). Bayes was able to solve integral given small \\(n\\) and \\(y\\). In case of large \\(n\\) and \\(y\\), Laplace used Gaussian approximation of the posterior (more in Chapter 4). In this specific case, R pbeta gives the same results as Laplace’s result with at least 3 digit accuracy.\n\n\nNumerical accuracy\nLaplace calculated \\[\np(\\theta \\geq 0.5 | y, n, M) \\approx 1.15 \\times 10^{-42}.\n\\] Correspondingly Laplace could have calculated \\[\np(\\theta \\geq 0.5 | y, n, M) = 1 - p(\\theta \\leq 0.5 | y, n, M),\n\\] which in theory could be computed in R with 1-pbeta(0.5,y+1,n-y+1). In practice this fails, due to the limitation in the floating point representation used by the computers. In R the largest floating point number which is smaller than 1 is about 1-eps/4, where eps is about \\(2.22 \\times 10^{-16}\\) (the smallest floating point number larger than 1 is 1+eps). Thus the result from pbeta(0.5,y+1,n-y+1) will be rounded to 1 and \\(1-1=0\\neq 1.15\n\\times 10^{-42}\\). We can compute \\(p(\\theta \\geq 0.5 | y, n, M)\\) in R with pbeta(0.5, y+1, n-y+1, lower.tail=FALSE).\n\n\nHighest Posterior Density interval\nHPD interval is not invariant to reparametrization. Here’s an illustrative example (using R and package HDInterval):\n&gt; r &lt;- exp(rnorm(1000))\n&gt; quantile(log(r),c(.05, .95))\n       5%       95% \n-1.532931  1.655137 \n&gt; log(quantile(r,c(.05, .95)))\n       5%       95% \n-1.532925  1.655139 \n&gt; hdi(log(r), credMass = 0.9)\n    lower     upper \n-1.449125  1.739169 \nattr(,\"credMass\")\n[1] 0.9\n&gt; log(hdi(r, credMass = 0.9))\n    lower     upper \n-2.607574  1.318569 \nattr(,\"credMass\")\n[1] 0.9\n\n\nGaussian distribution in more complex models and methods\nGaussian distribution is commonly used in mixture models, hierarchical models, hierarchical prior structures, scale mixture distributions, Gaussian latent variable models, Gaussian processes, Gaussian random Markov fields, Kalman filters, proposal distribution in Monte Carlo methods, etc.\n\n\nPredictive distribution\nOften the predictive distribution is more interesting than the posterior distribution. The posterior distribution describes the uncertainty in the parameters (like the proportion of red chips in the bag), but the predictive distribution describes also the uncertainty about the future event (like which color is picked next). This difference is important, for example, if we want to what could happen if some treatment is given to a patient.\nIn case of Gaussian distribution with known variance \\(\\sigma^2\\) the model is \\[\n\\begin{aligned}\n  y\\sim \\operatorname{N}(\\theta,\\sigma^2),\n\\end{aligned}\n\\] where \\(\\sigma^2\\) describes aleatoric uncertainty. Using uniform prior the posterior is \\[\n\\begin{aligned}\n  p(\\theta|y) \\sim \\mathop{\\mathrm{N}}(\\theta|\\bar{y},\\sigma^2/n),\n\\end{aligned}\n\\] where \\(\\sigma^2/n\\) described epistemic uncertainty related to \\(\\theta\\). Using uniform prior the posterior predictive distribution for new \\(\\tilde{y}\\) is \\[\n\\begin{aligned}\n  p(\\tilde{y}|y) \\sim \\operatorname{N}(\\tilde{y}|\\bar{y},\\sigma^2+\\sigma^2/n),\n\\end{aligned}\n\\] where the uncertainty is sum of epistemic (\\(\\sigma^2/n\\)) and aleatoric uncertainty (\\(\\sigma^2\\)).\n\n\nNon-informative and weakly informative priors\nOur thinking has advanced since sections 2.8 and 2.9 were written. We’re even more strongly in favor weakly informative priors, and in favor of more information in the priors. Non-informative priors are likely to produce more unstable estimates (higher variance), and the lectures include also examples of how seemingly non-informative priors can actually be informative on some aspect. See further discussion and example in the Prior Choice Recommendations Wiki. Thus Prior Choice Recommendations Wiki will see also some further updates (we’re doing research and learning more all the time).\n\n\nShould we worry about rigged priors?\nAndrew Gelman’s blog post answering worries that data analyst would choose a too optimistic prior.\n\n\nPrior knowledge elicitation\nPrior elicitation transforms domain knowledge of various kinds into well-defined prior distributions. There are challenges in how to gather domain knowledge and hopw to transform that to mathematical form. We come back to the topic later in the course. A recent review “Prior Knowledge Elicitation: The Past, Present, and Future” by Mikkola et al. (2023) provides more information for those interested to go beoyond the material in thos course.\n\n\nExchangeability\nYou don’t need to understand or use the term exchangeability before Chapter 5 and Lecture 7. At this point and until Chapter 5 and Lecture 7, it is sufficient that you know that 1) independence is stronger condition than exchangeability, 2) independence implies exchangeability, 3) exchangeability does not imply independence, 4) exchangeability is related to what information is available instead of the properties of unknown underlying data generating mechanism. If you want to know more about exchangeability right now, then read BDA3 Section 5.2 and notes for Chapter 5.\n\n\nThe number of left-handed students in the class\n\nWhat we know and don’t know\n\n\\(N=L+R\\) is the total number of students in the lecture hall, \\(N\\) is known in the beginning\n\\(L\\) and \\(R\\) are the number of left and right handed students, not known before we start asking\n\\(n=l+r\\) is the total number of students we have asked\n\\(l\\) and \\(r\\) are the numbers of left and right handed students from the students we asked\nwe also know that \\(l \\leq L \\leq (N-r)\\) and \\(r \\leq R \\leq (N-l)\\)\n\nAfter observing \\(n\\) students with \\(l\\) left handed, what we know about \\(L\\)?\n\nWe define \\(L=l+\\tilde{l}\\), where \\(\\tilde{l}\\) is the unobserved number of left handed students among those who we did not yet ask\nPosterior distribution for \\(\\theta\\) is \\(\\operatorname{Beta}(\\alpha+l, \\beta+r)\\)\nPosterior predictive distribution for \\(\\tilde{l}\\) is \\(\\operatorname{Beta-Binomial}(\\tilde{l} | N-n, \\alpha+l, \\beta+r)=\\int_0^1\\operatorname{Bin}(\\tilde{l} | N-n, \\theta)\\operatorname{Beta}(\\theta | \\alpha+l, \\beta+r)d\\theta\\)\n\nEventually as we have asked everyone, \\(n=N\\), and there is no uncertainty on the number of left-handed students present, and \\(l=L\\) and \\(\\tilde{l}=0\\). There is still uncertainty about \\(\\theta\\), but that is relevant only if we would like to make predictions beyond the students in the lecture hall."
  },
  {
    "objectID": "BDA3_notes.html#ch3",
    "href": "BDA3_notes.html#ch3",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 3 Introduction to multiparameter models",
    "text": "Chapter 3 Introduction to multiparameter models\nChapter 3 is related to the Lecture 3 Multidimensional posterior.\n\nOutline\n\n3.1 Marginalisation\n3.2 Normal distribution with a noninformative prior (very important)\n3.3 Normal distribution with a conjugate prior (very important)\n3.4 Multinomial model (can be skipped)\n3.5 Multivariate normal with known variance (needed later)\n3.6 Multivariate normal with unknown variance (glance through)\n3.7 Bioassay example (very important, related to one of the exercises)\n3.8 Summary (summary)\n\nNormal model is used a lot as a building block of the models in the later chapters, so it is important to learn it now. Bioassay example is good example used to illustrate many important concepts and it is used in several exercises over the course.\n\n\nThe most important terms\nFind all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.\n\nmarginal distribution/density\nconditional distribution/density\njoint distribution/density\nnuisance parameter\nmixture\nnormal distribution with a noninformative prior\nnormal distribution with a conjugate prior\nsample variance\nsufficient statistics\n\\(\\mu\\), \\(\\sigma^2\\), \\(\\bar{y}\\), \\(s^2\\)\na simple normal integral\n\\(\\operatorname{Inv-\\chi^2}\\)\nfactored density\n\\(t_{n-1}\\)\ndegrees of freedom\nposterior predictive distribution\n\\(\\operatorname{N-Inv-\\chi^2}\\)\nvariance matrix \\(\\Sigma\\)\nnonconjugate model\ngeneralized linear model\nbinomial model\nlogistic transformation\ndensity at a grid\n\n\n\nR and Python demos\n\n3.1: Visualize joint density and marginal densities of posterior of normal distribution with unknown mean and variance. R. Python.\n3.2: Visualize factored sampling and corresponding marginal and conditional density. R. Python.\n3.3: Visualize marginal distribution of \\(\\mu\\) as a mixture of normals. R. Python.\n3.4: Visualize sampling from the posterior predictive distribution. R. Python.\n3.5: Visualize Newcomb’s data. R. Python.\n3.6: Visualize posterior in bioassay example. R. Python.\n\n\n\nRecommended exercises\nOptional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:\n\n3.2, 3.3, 3.9 (model solutions available for 3.1-3.3, 3.5, 3.9, 3.10)\n\n\n\nConjugate prior for normal distribution\nBDA3 p. 67 mentions that the conjugate prior for normal distribution has to have a product form \\(p(\\sigma^2)p(\\mu|\\sigma^2)\\). The book refers to (3.2) and the following discussion. As additional hint is useful to think the relation of terms \\((n-1)s^2\\) and \\(n(\\bar{y}-\\mu)^2\\) in 3.2 to equations 3.3 and 3.4.\n\n\nTrace of square matrix\nTrace of square matrix, \\(\\operatorname{trace}\\), \\(\\operatorname{tr}A\\), \\(\\operatorname{trace}(A)\\), \\(\\operatorname{tr}(A)\\), is the sum of diagonal elements. To derive equation 3.11 the following property has been used \\(\\operatorname{tr}(ABC) = \\operatorname{tr}(CAB) = \\operatorname{tr}(BCA)\\).\n\n\nHistory and naming of distributions\nSee Earliest Known Uses of Some of the Words of Mathematics.\n\n\nUsing Monte Carlo to obtain draws from the posterior of generated quantities\nChapter 3 discusses closed form posteriors for binomial and normal models given conjugate priors. These are also used as part of the assignment. The assignment also requires forming a posterior for derived quantities, and these posterior don’t have closed form (so no need to try derive them). As we know how to sample from the posterior of binomial and normal models, we can use these posterior draws to get draws from the posterior of derived quantity.\nFor example, given posteriors \\(p(\\theta_1|y_1)\\) and \\(p(\\theta_2|y_2)\\) we want to find the posterior for the difference \\(p(\\theta_1-\\theta_2|y_1,y_2)\\).\n\nSample \\(\\theta_1^s\\) from \\(p(\\theta_1|y_1)\\) and \\(\\theta_2^s\\) from \\(p(\\theta_2|y_2)\\), we can compute posterior draws for the derived quantity as \\(\\delta^s=\\theta_1^s-\\theta_2^s\\) (\\(s=1,\\ldots,S\\)).\n\\(\\delta^s\\) are then draws from \\(p(\\delta^s|y_1,y_2)\\), and they can be used to illustrate the posterior \\(p(\\delta^s|y_1,y_2)\\) with histogram, and compute posterior mean, sd, and quantiles.\n\nThis is one reason why Monte Carlo approaches are so commonly used.\n\n\nThe number of required Monte Carlo draws\nThis will discussed in Lecture 4 and Chapter 10. Meanwhile, e.g., 1000 draws is sufficient.\n\n\nBioassay\nBioassay example is is an example of very common statistical inference task typical, for example, medicine, pharmacology, health care, cognitive science, genomics, industrial processes etc.\nThe example is from Racine et al (1986) (see ref in the end of the BDA3). Swiss company makes classification of chemicals to different toxicity categories defined by authorities (like EU). Toxicity classification is based on lethal dose 50% (LD50) which tells what amount of chemical kills 50% of the subjects. Smaller the LD50 more lethal the chemical is. The original paper mentions \"1983 Swiss poison Regulation\" which defines following categories for chemicals orally given to rats (mg/ml)\n\n\n\n\nClass\nLD50\n\n\n\n\n1\n&lt;5\n\n\n2\n5-50\n\n\n3\n50-500\n\n\n4\n500-2000\n\n\n5\n2000-5000\n\n\n\nTo reduce the number of rats needed in the experiments, the company started to use Bayesian methods. The paper mentions that in those days use of just 20 rats to define the classification was very little. Book gives LD50 in log(g/ml). When the result from 3.6 is transformed to scale mg/ml, we see that the mean LD50 is about 900 and \\(p(500&lt;\\text{LD50}&lt;2000)\\approx 0.99\\). Thus, the tested chemical can be classified as category 4 toxic.\nNote that the chemical testing is moving away from using rats and other animals to using, for example, human cells grown in chips, tissue models and human blood cells. The human-cell based approaches are also more accurate to predict the effect for humans.\n\\(\\operatorname{logit}\\) transformation can be justified information theoretically when binomial likelihood is used.\nCode in demo 3.6 (R, Python) can be helpful in exercises related to Bioassay example.\n\n\nBayesian vs. frequentist statements in two group comparisons\nWhen asking to compare groups, some students get confused as the frequentist testing is quite different. The frequentist testing is often focusing on a) differently named tests for different models and b) null hypothesis testing. In Bayesian inference a) the same Bayes rule and investigation of posterior is used for all models, b) null hypothesis testing is less common. We come later to decision making given posterior and utility/ cost function (Lecture 10.1) and more about null hypothesis testing (Lecture 12.1). Now it is assumed you will report the posterior (e.g. histogram), possible summaries, and report what you can infer from that. Specifically as in this assignment the group comparisons are based on continuous model parameter, the probability of 0 difference is 0 (later lecture 12.1 covers null hypothesis testing). Instead of forcing dichotomous answer (yes/no) about whether there is difference, report the whole posterior that tells also how big that difference might be. What big means depends on the application, which brings us back to the fact of importance of domain expertise. You are not experts on the application examples used in the assignment, but you can think how would you report what you have learned to a domain expert.\nFrank Harrell’s recommendations how to state results in two group comparisons are excellent.\n\n\nUnimodal and multimodal\nFrom Wikipedia Unimodality:\n\nIn statistics, a unimodal probability distribution or unimodal distribution is a probability distribution which has a single peak. The term “mode” in this context refers to any peak of the distribution.\nIf it has more modes it is “bimodal” (2), “trimodal” (3), etc., or in general, “multimodal”.\n\nBDA3 Section 2.3 discusses posterior summaries and illustrates bimodal distribution in Figure 2.2."
  },
  {
    "objectID": "BDA3_notes.html#ch4",
    "href": "BDA3_notes.html#ch4",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 4 Asymptotics and connections to non-Bayesian approaches",
    "text": "Chapter 4 Asymptotics and connections to non-Bayesian approaches\nChapter 4 is related to the Lecture 11 Normal approximation, frequency properties.\n\nOutline\n\n4.1 Normal approximation (Laplace’s method)\n4.2 Large-sample theory\n4.3 Counter examples\n4.4 Frequency evaluation (not part of the course, but interesting)\n4.5 Other statistical methods (not part of the course, but interesting)\n\nNormal approximation is used often used as part of posterior computation (more about this in Chapter 13, which is not a part of the course).\n\n\nThe most important terms\nFind all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.\n\nsample size\nasymptotic theory\nnormal approximation\nquadratic function\nTaylor series expansion\nobserved information\npositive definite\nwhy \\(\\log \\sigma\\)?\nJacobian of the transformation\npoint estimates and standard errors- large-sample theory\nasymptotic normality\nconsistency\nunderidentified\nnonidentified\nnumber of parameters increasing with sample size\naliasing\nunbounded likelihood\nimproper posterior\nedge of parameter space\ntails of distribution\n\n\n\nR and Python demos\n\n4.1: Bioassay example R. Python.\n\n\n\nNormal approximation\nOther normal posterior approximations are discussed in Chapter 13. For example, variational and expectation propagation methods improve the approximation by global fitting instead of just the curvature at the mode. The normal approximation at the mode is often also called the Laplace method, as Laplace used it first.\nSeveral researchers have provided partial proofs that posterior converges towards normal distribution. In the mid 20th century Le Cam was first to provide a strict proof.\n\n\nObserved information\nWhen \\(n\\rightarrow\\infty\\), the posterior distribution approaches normal distribution. As the log density of the normal is a quadratic function, the higher derivatives of the log posterior approach zero. The curvature at the mode describes the information only in the case if asymptotic normality. In the case of the normal distribution, the curvature describes also the width of the normal. Information matrix is a precision matrix, which is inverse of a covariance matrix.\n\n\nAliasing\nIn Finnish: valetoisto.\nAliasing is a special case of under-identifiability, where likelihood repeats in separate points of the parameter space. That is, likelihood will get exactly same values and has same shape although possibly mirrored or otherwise projected. For example, the following mixture model \\[\n  p(y_i|\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2,\\lambda)=\\lambda\\operatorname{N}(\\mu_1,\\sigma_1^2)+(1-\\lambda)\\operatorname{N}(\\mu_2,\\sigma_2^2),\n\\] has two normals with own means and variances. With a probability \\(\\lambda\\) the observation comes from \\(\\operatorname{N}(\\mu_1,\\sigma_1^2)\\) and a probability \\(1-\\lambda\\) from \\(\\operatorname{N}(\\mu_2,\\sigma_2^2)\\). This kind of model could be used, for example, for the Newcomb’s data, so that the another normal component would model faulty measurements. Model does not state which of the components 1 or 2, would model good measurements and which would model the faulty measurements. Thus it is possible to interchange values of \\((\\mu_1,\\mu_2)\\) and \\((\\sigma_1^2,\\sigma_2^2)\\) and replace \\(\\lambda\\) with \\((1-\\lambda)\\) to get the equivalent model. Posterior distribution then has two modes which are mirror images of each other. When \\(n\\rightarrow\\infty\\) modes will get narrower, but the posterior does not converge to a single point.\nIf we can integrate over the whole posterior, the aliasing is not a problem. However aliasing makes the approximative inference more difficult.\n\n\nFrequency property vs. frequentist\nBayesians can evaluate frequency properties of Bayesian estimates without being frequentist. For Bayesians the starting point is the Bayes rule and decision theory. Bayesians care more about efficiency than unbiasedness. For frequentists the starting point is to find an estimator with desired frequency properties and quite often unbiasedness is chosen as the first restriction.\n\n\nTransformation of variables\nSee BDA3 p. 21 for the explanation how to derive densities for transformed variables. This explains, for example, why uniform prior \\(p(\\log(\\sigma^2))\\propto 1\\) for \\(\\log(\\sigma^2)\\) corresponds to prior \\(p(\\sigma^2)=\\sigma^{-2}\\) for \\(\\sigma^{2}\\).\n\n\nOn derivation\nHere’s a reminder how to integrate with respect to \\(g(\\theta)\\). For example \\[\n  \\frac{d}{d\\log\\sigma}\\sigma^{-2}=-2 \\sigma^{-2}\n\\] is easily solved by setting \\(z=\\log\\sigma\\) to get \\[\n  \\frac{d}{dz}\\exp(z)^{-2}=-2\\exp(z)^{-3}\\exp(z)=-2\\exp(z)^{-2}=-2\\sigma^{-2}.\n\\]"
  },
  {
    "objectID": "BDA3_notes.html#ch5",
    "href": "BDA3_notes.html#ch5",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 5 Hierarchical models",
    "text": "Chapter 5 Hierarchical models\nChapter 5 is related to the Lecture 7 Hierarchical models and exchangeability.\n\nOutline\n\n5.1 Lead-in to hierarchical models\n5.2 Exchangeability (a useful theoretical concept)\n5.3 Bayesian analysis of hierarchical models (discusses factorized computation which can be skipped)\n5.4 Hierarchical normal model (discusses factorized computation which can be skipped)\n5.5 Example: parallel experiments in eight schools (useful dicussion, skip the details of computation)\n5.6 Meta-analysis (can be skipped in this course)\n5.7 Weakly informative priors for hierarchical variance parameters (more recent discussion can be found in Prior Choice Recommendation Wiki)\n\nThe hierarchical models in the chapter are simple to keep computation simple. More advanced computational tools are presented in Chapters 10, 11 and 12 (part of the course), and 13 (not part of the course).\n\n\nThe most important terms\nFind all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.\n\npopulation distribution\nhyperparameter\nhierarchical model\nexchangeability\ninvariant to permutations\nindependent and identically distributed\nignorance \npartially exchangeable\nconditionally exchangeable\nconditional independence\nhyperprior\ndifferent posterior predictive distributions\nthe conditional probability formula\n\n\n\nR and Python demos\n\n5.1: Rats example. R. Python.\n5.2: SAT example. R. Python.\n\n\n\nRecommended exercises\nOptional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:\n\n5.1 and 5.2 (model solution available for 5.3-5.5, 5.7-5.12)\n\n\n\nComputation\nExamples in BDA3 Sections 5.3 and 5.4 continue computation with factorization and grid, but there is no need to go deep in to computational details as in the assignments you will use MCMC and Stan instead.\n\n\nExchangeability vs. independence\nExchangeability and independence are two separate concepts. Neither necessarily implies the other. Independent identically distributed variables/parameters are exchangeable. Exchangeability is less strict condition than independence. Often we may assume that observations or unobserved quantities are in fact dependent, but if we can’t get information about these dependencies we may assume those observations or unobserved quantities as exchangeable. ``Ignorance implies exchangeability.’’\nIn case of exchangeable observations, we may sometimes act as if observations were independent if the additional potential information gained from the dependencies is very small. This is related to de Finetti’s theorem (BDA3 p. 105), which applies formally only when \\(J\\rightarrow\\infty\\), but in practice difference may be small if \\(J\\) is finite but relatively large (see examples below).\n\nIf no other information than data \\(y\\) is available to distinguish \\(\\theta_j\\) from each other and parameters can not be ordered or grouped, we may assume symmetry between parameters in their prior distribution\nThis symmetry can be represented with exchangeability\nParameters \\(\\theta_1,\\ldots,\\theta_J\\) are exchangeable in their joint distribution if \\(p(\\theta_1,\\ldots,\\theta_J)\\) is invariant to permutation of indexes \\((1,\\ldots,J)\\)\n\nHere are some examples you may consider.\nEx 5.1. Exchangeability with known model parameters: For each of following three examples, answer: (i) Are observations \\(y_1\\) and \\(y_2\\) exchangeable? (ii) Are observations \\(y_1\\) and \\(y_2\\) independent? (iii) Can we act as if the two observations are independent?\n\nA box has one black ball and one white ball. We pick a ball \\(y_1\\) at random, put it back, and pick another ball \\(y_2\\) at random.\nA box has one black ball and one white ball. We pick a ball \\(y_1\\) at random, we do not put it back, then we pick ball \\(y_2\\).\nA box has a million black balls and a million white balls. We pick a ball \\(y_1\\) at random, we do not put it back, then we pick ball \\(y_2\\) at random.\n\nEx 5.2. Exchangeability with unknown model parameters: For each of following three examples, answer: (i) Are observations \\(y_1\\) and \\(y_2\\) exchangeable? (ii) Are observations \\(y_1\\) and \\(y_2\\) independent? (iii) Can we act as if the two observations are independent?\n\nA box has \\(n\\) black and white balls but we do not know how many of each color. We pick a ball \\(y_1\\) at random, put it back, and pick another ball \\(y_2\\) at random.\nA box has \\(n\\) black and white balls but we do not know how many of each color. We pick a ball \\(y_1\\) at random, we do not put it back, then we pick ball \\(y_2\\) at random.\nSame as (b) but we know that there are many balls of each color in the box.\n\nNote that for example in opinion polls, balls i.e. humans are not put back and there is a large but finite number of humans.\nFollowing complements the divorce example in the book by discussing the effect of the additional observations\n\nExample: divorce rate per 1000 population in 8 states of the USA in 1981\n\nwithout any other knowledge \\(y_1,\\ldots,y_8\\) are exchangeable\nit is reasonable to assume a prior independence given population density \\(p(y_i|\\theta)\\)\n\nDivorce rate in first seven are \\(5.6, 6.6, 7.8, 5.6,\n    7.0, 7.2, 5.4\\)\n\nnow we have some additional information, but still changing the indexing does not affect the joint distribution. For example, if we were told that divorce rate were not for the first seven but last seven states, it does not change the joint distribution, and thus \\(y_1,\\ldots,y_8\\) are exchangeable\nsensible assumption is a prior independence given population density \\(p(y_i|\\theta)\\)\nif \"true\" \\(\\theta_0\\) were known, \\(y_1,\\ldots,y_8\\) were independent given \"true\" \\(\\theta_0\\)\nsince \\(\\theta\\) is estimated using observations, \\(y_i\\) are a posterior dependent, which is obvious, e.g., from the predictive density \\(p(y_8|y_1,\\ldots,y_7,M)\\), i.e. e.g. if \\(y_1,\\ldots,y_7\\) are large then probably \\(y_8\\) is large\nif we were told that given rates were for the last seven states, then \\(p(y_1|y_2,\\ldots,y_8,M)\\) would be exactly same as \\(p(y_8|y_1,\\ldots,y_7,M)\\) above, i.e. changing the indexing does not have effect since \\(y_1,\\ldots,y_8\\) are exchangeable\n\nAdditionally we know that \\(y_8\\) is Nevada and rates of other states are \\(5.6, 6.6, 7.8, 5.6, 7.0, 7.2, 5.4\\)\n\nbased on what we were told about Nevada, predictive density s \\(p(y_8|y_1,\\ldots,y_7,M)\\) should take into account that probability \\(p(y_8&gt;\\max(y_1,\\ldots,y_7)|y_1,\\ldots,y_7)\\) should be large\nif we were told that, Nevada is \\(y_3\\) (not \\(y_8\\) as above), then new predictive density \\(p(y_8|y_1,\\ldots,y_7,M)\\) would be different, because \\(y_1,\\ldots,y_8\\) are not anymore exchangeable\n\n\n\n\nWhat if observations are not exchangeable\nOften observations are not fully exchangeable, but are partially or conditionally exchangeable. Two basic cases\n\nIf observations can be grouped, we may make hierarchical model, were each group has own subpart, but the group properties are unknown. If we assume that group properties are exchangeable we may use common prior for the group properties.\nIf \\(y_i\\) has additional information \\(x_i\\), then \\(y_i\\) are not exchangeable, but \\((y_i,x_i)\\) still are exchangeable, then we can be make joint model for \\((y_i,x_i)\\) or conditional model \\((y_i|x_i)\\).\n\nHere are additional examples (Bernardo & Smith, Bayesian Theory, 1994), which illustrate the above basic cases. Think of old fashioned thumb pin. This kind of pin can stay flat on it’s base or slanting so that the pin head and the edge of the base touch the table. This kind of pin represents realistic version of \"unfair\" coin.\n\nLet’s throw pin \\(n\\) times and mark \\(x_i=1\\) when pin stands on it’s base. Let’s assume, that throwing conditions stay same all the time. Most would accept throws as exchangeable.\nSame experiment, but odd numbered throws will be made with full metal pin and even numbered throws with plastic coated pin. Most would accept exchangeability for all odd and all even throws separately, but not necessarily for both series combined. Thus we have partial exchangeability.\nLaboratory experiments \\(x_1,...,x_n\\), are real valued measurements about the chemical property of some substance. If all experiments are from the same sample, in the same laboratory with same procedure, most would accept exchangeability. If experiments were made, for example, in different laboratories we could assume partial exchangeability.\n\\(x_1,...,x_n\\) are real valued measurements about the physiological reactions to certain medicine. Different test persons get different amount of medicine. Test persons are males and females of different ages. If the attributes of the test persons were known, most would not accept results as exchangeable. In a group with certain dose, sex and age, the measurements could be assumed exchangeable. We could use grouping or if the doses and attributes are continuous we could use regression, that is, assume conditional independence.\n\n\n\nWeakly informative priors for hierarchical variance parameters\nOur thinking has advanced since section 5.7 was written. Section 5.7 (BDA3 p. 128–) recommends use of half-Cauchy as weakly informative prior for hierarchical variance parameters. More recent recommendation is half-normal if you have substantial information on the high end values, or or half-\\(t_4\\) if you there might be possibility of surprise. Often we don’t have so much prior information that we would be able to well define the exact tail shape of the prior, but half-normal produces usually more sensible prior predictive distributions and is thus better justified. Half-normal leads also usually to easier inference.\nSee the Prior Choice Wiki for more recent general discussion and model specific recommendations."
  },
  {
    "objectID": "BDA3_notes.html#ch6",
    "href": "BDA3_notes.html#ch6",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 6 Model checking",
    "text": "Chapter 6 Model checking\nChapter 6 is related to the Lecture 8 Model checking & cross-validation.\n\nOutline\n\n6.1 The place of model checking in applied Bayesian statistics\n6.2 Do the inferences from the model make sense?\n6.3 Posterior predictive checking (\\(p\\)-values can be skipped)\n6.4 Graphical posterior predictive checks (this can be skimmed, see instead the paper Visualization in Bayesian workflow)\n6.5 Model checking for the educational testing example\n\n\n\nThe most important terms\nFind all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.\n\nmodel checking\nsensitivity analysis\nexternal validation\nposterior predictive checking\njoint posterior predictive distribution\nmarginal (posterior) predictive distribution\nself-consistency check\nreplicated data\n\\(y^{\\mathop{\\mathrm{\\mathrm{rep}}}}\\), \\(\\tilde{y}\\), \\(\\tilde{x}\\)\ntest quantities\ndiscrepancy measure\ntail-area probabilities\nclassical \\(p\\)-value\nposterior predictive \\(p\\)-values\nmultiple comparisons\nmarginal predictive checks\ncross-validation predictive distributions\n\n\n\nR and Python demos\n\n6.1: Posterior predictive checking - speed of light. R. Python.\n6.2: Posterior predictive checking - sequential dependence. R. Python.\n6.3: Posterior predictive checking - poor test statistic. R. Python.\n6.4: Posterior predictive checking - marginal predictive \\(p\\)-value. R. Python.\n\n\n\nRecommended exercises\nOptional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:\n\n6.1 (model solution available for 6.1, 6.5-6.7)\n\n\n\nReplicates vs. future observation\nPredictive \\(\\tilde{y}\\) is the next not yet observed possible observation. \\(y^{\\mathrm{rep}}\\) refers to replicating the whole experiment (with same values of \\(x\\)) and obtaining as many replicated observations as in the original data.\n\n\nPosterior predictive \\(p\\)-values\nSection 6.3 discusses posterior predictive \\(p\\)-values, which we don’t recommend any more especially in a form of hypothesis testing.\n\n\nPrior predictive checking\nPrior predictive checking using just the prior predictive distributions is very useful tool for assessing the sensibility of the model and priors even before observing any data or before doing the posterior inference. See additional reading below for examples.\n\n\nAdditional reading\nThe following article has some useful discussion and examples also about prior and posterior predictive checking.\n\nGabry, Simpson, Vehtari, Betancourt, and Gelman (2018). Visualization in Bayesian workflow. Journal of the Royal Statistical Society Series A, , 182(2):389-402. Online.\nVideo of the paper presentation.\n\nAnd some additional useful demos\n\nGraphical posterior predictive checks using the bayesplot package.\nAnother PPC demo."
  },
  {
    "objectID": "BDA3_notes.html#ch7",
    "href": "BDA3_notes.html#ch7",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 7 Evaluating, comparing, and expanding models",
    "text": "Chapter 7 Evaluating, comparing, and expanding models\nChapter 7 is related to the Lecture 8 Model checking & cross-validation’’ and he Lecture 9 Model comparison and selection*.\n\nOutline\n\n7.1 Measures of predictive accuracy\n7.2 Information criteria and cross-validation (read instead the article mentioned below)\n7.3 Model comparison based on predictive performance (read instead the article mentioned below)\n7.4 Model comparison using Bayes factors (not used in the course, but useful to read if you have heard about Bayes factors)\n7.5 Continuous model expansion / sensitivity analysis\n7.6 Example (may be skipped)\n\nInstead of Sections 7.2 and 7.3 it’s better to read\n\nAki Vehtari, Andrew Gelman and Jonah Gabry (2017). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing, 27(5):1413-1432, doi:10.1007/s11222-016-9696-4. arXiv preprint arXiv:1507.04544.\nLOO package glossary summarizes many important terms used in the assignments.\nCV-FAQ\n\nIn Sections 7.2 and 7.3 of BDA, for historical reasons there is a multiplier \\(-2\\) used. After the book was published, we have concluded that it causes too much confusion and recommend not to multiply by \\(-2\\). The above paper is not using \\(-2\\) anymore.\n\n\nExtra material\nThe following article provides excellent discussion about “How should I evaluate my modeling choices?” from a scientific perspective.\n\nDanielle J. Navarro (2019). Between the devil and the deep blue sea: Tensions between scientific judgment and statistical model selection. Computational Brain & Behavior 2:28–34. Online.\nExtra videos, slides, case studies, and references on model selection\nSections 1 and 5 (less than 3 pages) of “Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison” clarify how to interpret standard error in model comparison\n\n\n\nThe most important terms\nFind all the terms and symbols listed below. When reading the chapter and the above mentioned article, write down questions related to things unclear for you or things you think might be unclear for others.\n\npredictive accuracy/fit/error\nexternal validation\ncross-validation\ninformation criteria\noverfitting\nmeasures of predictive accuracy\npoint prediction\nscoring function\nmean squared error\nscoring rule\nlogarithmic score\nlog-predictive density\nout-of-sample predictive fit\nelpd, elppd, lppd\nwithin-sample predictive accuracy\nadjusted within-sample predictive accuracy\nAIC, DIC, WAIC (less important)\neffective number of parameters\nsingular model\nleave-one-out cross-validation\nevaluating predictive error comparisons\nbias induced by model selection\nBayes factors\ncontinuous model expansion\nsensitivity analysis\n\n\n\nAdditional reading\nMore theoretical details can be found in\n\nAki Vehtari and Janne Ojanen (2012). A survey of Bayesian predictive methods for model assessment, selection and comparison. In Statistics Surveys, 6:142-228. Online.\n\nMore experimental comparisons can be found in\n\nJuho Piironen and Aki Vehtari (2017). Comparison of Bayesian predictive methods for model selection. Statistics and Computing, 27(3):711-735. Online.\n\n\n\nPosterior probability of the model vs. predictive performance\nGelman: “To take a historical example, I don’t find it useful, from a statistical perspective, to say that in 1850, say, our posterior probability that Newton’s laws were true was 99%, then in 1900 it was 50%, then by 1920, it was 0.01% or whatever. I’d rather say that Newton’s laws were a good fit to the available data and prior information back in 1850, but then as more data and a clearer understanding became available, people focused on areas of lack of fit in order to improve the model.”\nNewton’s laws are still sufficient for prediction in specific contexts (non-relative speeds and differences in gravity, non-significant effects of air resistance or other friction). See more in the course video 1.1 Introduction to uncertainty and modeling."
  },
  {
    "objectID": "BDA3_notes.html#ch8",
    "href": "BDA3_notes.html#ch8",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 8 Modeling accounting for data collection",
    "text": "Chapter 8 Modeling accounting for data collection\nChapter 8 is not part of the BDA Aalto course.\nIn the earlier chapters it was assumed that the data collection is ignorable. Chapter 8 explains when data collection can be ignorable and when we need to model also the data collection. We don’t have time to go through chapter 8 in BDA course at Aalto, but it is highly recommended that you would read it in the end or after the course. Most important sections are 8.1, 8.5, pp. 220–222 of 8.6, and 8.8, and you can get back to the other sections later.\n\nOutline\n\n8.1 Bayesian inference requires a model for data collection (important)\n8.2 Data-collection models and ignorability\n8.3 Sample surveys\n8.4 Designed experiments\n8.5 Sensitivity and the role of randomization (important)\n8.6 Observational studies (pp. 220–222 important)\n8.7 Censoring and truncation (important)\n\n\n\nThe most important terms\n\nobserved data\ncomplete data\nmissing data\nstability assumption\ndata model\ninclusion model\ncomplete data likelihood\nobserved data likelihood\nfinite-population and superpopulation inference\nignorability\nignorable designs\npropensity score\nsample surveys\nrandom sampling of a finite population\nstratified sampling\ncluster sampling\ndesigned experiments\ncomplete randomization\nrandomized blocks and Latin squares\nsequential designs\nrandomization given covariates\nobservational studies\ncensoring\ntruncation\nmissing completely at random"
  },
  {
    "objectID": "BDA3_notes.html#ch9",
    "href": "BDA3_notes.html#ch9",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 9 Decision analysis",
    "text": "Chapter 9 Decision analysis\nChapter 9 is related to the Lecture 10 Decision analysis.\n\nOutline\n\n9.1 Context and basic steps (most important part)\n9.2 Example\n9.3 Multistage decision analysis (you may skip this example)\n9.4 Hierarchical decision analysis (you may skip this example)\n9.5 Personal vs. institutional decision analysis (important)\n\n\n\nThe most important terms\nFind all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.\n\ndecision analysis\nsteps of Bayesian decision analysis 1–4 (BDA3 p. 238)\ndecision\noutcome\nutility function\nexpected utility\ndecision tree\nsummarizing inference\nmodel selection\nindividual decision problem\ninstitutional decision problem\n\n\n\nSimpler examples\nThe lectures have simpler examples and also discuss some challenges in selecting utilities or costs.\n\n\nModel selection as a decision problem\nChapter 7 discusses how model selection con be considered as a decision problem."
  },
  {
    "objectID": "BDA3_notes.html#ch10",
    "href": "BDA3_notes.html#ch10",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 10 Introduction to Bayesian computation",
    "text": "Chapter 10 Introduction to Bayesian computation\nChapter 10 is related to the Lecture 4 Monte Carlo.\n\nOutline\n\n10.1 Numerical integration (overview)\n10.2 Distributional approximations (overview, more in Chapters 4 and 13)\n10.3 Direct simulation and rejection sampling (overview)\n10.4 Importance sampling (used in PSIS-LOO discussed later)\n10.5 How many simulation draws are needed? (Important!)\n10.6 Software (can be skipped)\n10.7 Debugging (can be skipped)\n\nSections 10.1–10.4 give overview of different computational methods. Some of then have been already used in the book.\nSection 10.5 is very important and related to the exercises.\n\n\nThe most important terms\nFind all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.\n\nunnormalized density\ntarget distribution\nlog density\noverflow and underflow\nnumerical integration\nquadrature\nsimulation methods\nMonte Carlo\nstochastic methods\ndeterministic methods\ndistributional approximations\ncrude estimation\ndirect simulation\ngrid sampling\nrejection sampling\nimportance sampling\nimportance ratios/weights\n\n\n\nR and Python demos\n\n10.1: Rejection sampling. R. Python.\n10.2: Importance sampling. R. Python.\n10.3: Sampling-importance resampling. R. Python.\n\n\n\nRecommended exercises\nOptional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:\n\n10.1, 10.2 (model solution available for 10.4)\n\n\n\nNumerical accuracy of computer arithmetic\nMany models use continuous real valued parameters. Computers have finite memory and thus the continuous values are also presented with finite number of bits and thus with finite accuracy. Most commonly used presentations are floating-point presentations that try to have balanced accuracy over the range of values where it mostly matters. As the the presentation has finite accuracy there are limitations, for example, with IEC 60559 floating-point (double precision) arithmetic used in current R\n\nthe smallest positive floating-point number \\(x\\) such that \\(1 + x \\neq 1\\) is \\(2.220446\\cdot 10^{-16}\\)\nthe smallest non-zero normalized floating-point number is \\(2.225074\\cdot 10^{-308}\\)\nthe largest normalized floating-point number \\(1.797693\\cdot 10^{308}\\)\nthe largest integer which can be represented is \\(2^{31} - 1 = 2147483647\\)\nsee more in R documentation: Numerical Characteristics of the Machine.\nWhat Is Floating-Point Arithmetic? blog post by Nick Higham provides nice short introduction.\nWhat Every Computer Scientist Should Know About Floating-Point Arithmetic by Goldberg (1991) provides nice overview of floating-point arithmetic and how the computations should be arranged for improved accuracy.\nStat 3701 Lecture Notes: Computer Arithmetic by Geyer (2020) provide code examples in R illustrating the most common issues in floating-point arithmetic including examples similar shown in the BDA course lecture.\nStan User Guide Chapter 15 discusses floating point arithmetic in context of Stan.\n\n\n\nTerms draw, draws and sample\nA group of draws is a sample. A sample can consist of one draw, and thus some people use the word sample for both single item and for the group. For clarity, we prefer separate words for a single item (draw) and for the group (sample). Sample can also mean a group of observations, and thus talking about posterior draws reduces ambiguity.\n\n\nMonte Carlo standard error\nMonte Carlo estimates have some error due to using a finite number of random draws. Monte Carlo standard error (MCSE) estimates this error. BDA3 Section 10.5 discuss the basics of this. Additional information:\n\nWhen we know the needed accuracy for reporting posterior summaries, we can estimate how big sample size is needed to get small enough MCSE for the quantity of interest.\nMCSE is based on central limit theorem which assumes that the distribution has finite variance.\nPareto-\\(k\\) diagnostic can be used to estimate whether the draws come from a distribution with finite variance (see below).\nBDA3 discusses cases for mean (E\\((\\theta)\\)) and probability (\\(p(\\theta &lt; \\alpha)\\)). MCSE for quantiles can be derived using MCSE for probability and inverse transform (details in Vehtari et al. (2020). Rank-normalization, folding, and localization: An improved \\(\\widehat{R}\\) for assessing convergence of MCMC. Bayesian analysis, Online).\nposterior package provides functions mcse_mean() and mcse_quantile(). MCSE for probabilities is obtained using mcse_mean() for indicator function outcome of the logical comparison.\n\n\n\nCentral limit theorem\n\n“In probability theory, the central limit theorem (CLT) states that, under appropriate conditions, the distribution of a normalized version of the sample mean converges to a standard normal distribution. This holds even if the original variables themselves are not normally distributed. There are several versions of the CLT, each applying in the context of different conditions.” Wikipedia Central limit theorem\nWikipedia Central limit theorem article has good summary of the topic and variants with links to further information\n3Blue1Brown YouTube videos with nice visualizations\n\nCLT with discrete distributions: “But what is the Central Limit Theorem?” \nCLT with continuous distributions: “Convolutions | Why X+Y in probability is a beautiful mess” \n\n\n\n\nPareto-\\(\\hat{k}\\) diagnostic\nPareto-\\(\\hat{k}\\) diagnostic estimates the tail shape of the distribution \\(p(\\theta)\\) given draws \\(\\theta^{(s)}\\) from that distribution. The tail shape indicates whether the distribution has finite variance and mean.\n\nIf \\(\\hat{k}&gt;1\\), it is likely that the distribution does not have finite mean, and trying to estimate the mean does not make sense.\nIf \\(\\hat{k}&gt;0.5\\), it is likely that the distribution does not have finite variance, and Monte Carlo standard error estimate based on the variance does not make sense.\n\\(\\hat{k}\\) estimate has it’s own variation given finite sample size, and if in doubt, get more draws to get more accurate \\(\\hat{k}\\) estimate.\nPareto-\\(\\hat{k}\\) diagnostic is pre-asymptotic based on finite sample size, and indicates the stability of empirical mean estimate given the draws so far. Increasing the number of draws, can get more draws far from the tail and may reveal that the tail shape is different further away.\nposterior package provides function pareto_khat()\nMonte Carlo estimates (including importance sampling) can be improved using Pareto-smoothing. Pareto smoothed Monte Carlo estimate has finite variance even if \\(p(\\theta)\\) does not, and the corresponding Monte Carlo standard error estimate can be useful if \\(\\hat{k}&lt;0.7\\). More about this later in the course.\nMore details in Vehtari et al. (2024). Pareto smoothed importance sampling. Journal of Machine Learning Research, 25(72):1-58. Online.\n\n\n\nHow many digits should be displayed\n\nToo many digits make reading of the results slower and give false impression of the accuracy.\nShow meaningful digits given the posterior uncertainty. You can compare posterior standard error or posterior intervals to the mean value. Posterior interval length can be used to determine also how many digits to show for the interval endpoints.\nDon’t ever show digits which are just random noise. You can use Monte Carlo standard error estimates to check how many digits are likely to stay the same if the sampling would be continued.\nThe advise considers the number of significant digits, which is different from the number of decimal digits. For example, the numbers \\(12\\), \\(1.2\\), \\(0.12\\), and \\(0.012\\) all have two significant digits. It is common that even if 2 significant digits would be sufficient, numbers like \\(123.4\\) and \\(1234.5\\) would be shown as rounded to integers \\(123\\) and \\(1234\\) (using the round to nearest even integer rule used by R).\nExample: The mean and 90% central posterior interval for temperature increase C\\(^\\circ\\)/century (see the slides for the example) based on posterior draws:\n\n\\(2.050774\\) and \\([0.7472868 3.3017524]\\) (too many digits)\n\\(2.1\\) and \\([0.7 3.3]\\) (good compared to the interval length)\n\\(2\\) and \\([1 3]\\) (sufficient accuracy in many cases)\n\nExample: The probability that temp increase is positive\n\n\\(0.9960000\\) (too many digits)\n\\(1.00\\) (depends on the context. \\(1.00\\) hints it’s not exactly \\(1\\), but larger than \\(0.99\\))\nWith 4000 draws MCSE \\(\\approx 0.002\\). We could report that probability is very likely larger than \\(0.99\\), or sample more to justify reporting three digits\nFor probabilities close to 0 or 1, consider also when the model assumption justify certain accuracy\n\nWhen reporting many numbers in table, for aesthetics reasons, it may be sometimes better for some numbers to show one extra or one too few digits compared to the ideal.\nOften it’s better to plot the whole posterior density in addition of any summaries, as summaries always loose some information content.\nFor your reports: Don’t be lazy and settle for the default number of digits in R or Python. Think for each reported value how many digits is sensible. If the Quiz asks certain number of digits, this overrules other advise.\nSee Digits case study for examples and how to use posterior package functions.\n\n\n\nQuadrature\nSometimes ‘quadrature’ is used to refer generically to any numerical integration method (including Monte Carlo), sometimes it is used to refer just to deterministic numerical integration methods.\n\n\nRejection sampling\nRejection sampling is mostly used as a part of fast methods for univariate sampling. For example, sampling from the normal distribution is often made using Ziggurat method, which uses a proposal distribution resembling stairs.\nRejection sampling is also commonly used for truncated distributions, in which case all draws from the truncated part are rejected.\n\n\nImportance sampling\nPopularity of importance sampling is increasing. It is used, for example, as part of other methods as particle filters and pseudo marginal likelihood approaches, and to improve distributional approximations (including variational inference in machine learning).\nImportance sampling is useful in importance sampling leave-one-out cross-validation. Cross-validation is discussed in Chapter 7 and importance sampling leave-one-out cross-validation is discussed in the article\n\nAki Vehtari, Andrew Gelman and Jonah Gabry (2016). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. In Statistics and Computing, 27(5):1413–1432. arXiv preprint arXiv:1507.04544.\n\nAfter the book was published, we have developed Pareto smoothed importance sampling which is more stable than plain importance sampling and has very useful Pareto-\\(\\hat{k}\\) diagnostic to check the reliability\n\nAki Vehtari, Daniel Simpson, Andrew Gelman, Yuling Yao, and Jonah Gabry (2024). Pareto smoothed importance sampling. Journal of Machine Learning Research, 25(72):1-58. Online.\n\n\n\nImportance resampling with or without replacement\nBDA3 p. 266 recommends importance resampling without replacement. At the time of writing that in 2013, we had less experience with importance sampling and there were some reasonable papers showing reduced variance doing resampling without replacement. We don’t recommend this anymore as Pareto smoothed importance sampling works better and is also applicable when the resample sample size is equal to the original sample size.\n\n\nImportance sampling effective sample size\nBDA3 1st (2013) and 2nd (2014) printing have an error for \\(\\tilde{w}(\\theta^s)\\) used in the effective sample size equation 10.4. The normalized weights equation should not have the multiplier S (the normalized weights should sum to one). The effective sample size estimate mentioned in the book is generic approximation, and more accurate effective sample size estimate would take into account also the functional. For example, importance sampling effective sample size can be different when estimating \\(\\operatorname{E}[\\theta]\\) or \\(\\operatorname{E}[\\theta]^2\\). If you are interested see more details, for example, the Pareto smoothed importance sampling paper.\nThe derivation for the effective sample size and Monte Carlo standard error (MCSE) for importance sampling can be found, for example, in Chapter 9 of Monte Carlo theory, methods and examples by Art B. Owen.\n\n\nBuffon’s needles\nBuffon’s needle is considered to be the first Monte Carlo style approach presented in 1777. It’s not known whether Buffon actually did the experiment in addition of describing the approach for estimating the value of \\(\\pi\\). Check out a nice computer simulation of Buffon’s needle dropping method."
  },
  {
    "objectID": "BDA3_notes.html#ch11",
    "href": "BDA3_notes.html#ch11",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 11 Basics of Markov chain simulation",
    "text": "Chapter 11 Basics of Markov chain simulation\nChapter 11 is related to the Lecture 5 Markov chain Monte Carlo.\n\nOutline\n\nMarkov chain simulation: before Section 11.1, pages 275-276\n11.1 Gibbs sampler (an example of simple MCMC method)\n11.2 Metropolis and Metropolis-Hastings (an example of simple MCMC method)\n11.3 Using Gibbs and Metropolis as building blocks (can be skipped)\n11.4 Inference and assessing convergence (important)\n11.5 Effective number of simulation draws (important)\n11.6 Example: hierarchical normal model (skip this)\n\n\n\nThe most important terms\nFind all the terms and symbols listed below. When reading the chapter, write down questions related to things unclear for you or things you think might be unclear for others.\n\nMarkov chain\nMarkov chain Monte Carlo\nrandom walk\nstarting point\ntransition distribution\njumping / proposal distribution\nto converge, convergence, assessing convergence\nstationary distribution, stationarity\neffective number of simulations\nGibbs sampler\nMetropolis sampling / algorithm\nMetropolis-Hastings algorithm\nacceptance / rejection rule\nacceptance / rejection rate\nwithin-sequence correlation, serial correlation\nwarm-up / burn-in\nto thin, thinned\noverdispersed starting points\nmixing\nto diagnose convergence\nbetween- and within-sequence variances\npotential scale reduction, \\(\\widehat{R}\\)\nthe variance of the average of a correlated sequence\nautocorrelation\nvariogram\n\\(n_{\\mathrm{eff}}\\) (we now prefer ESS / \\(S_{\\mathrm{eff}}\\), which is used also in slides)\n\n\n\nR and Python\n\n11.1: Gibbs sampling. R. Python.\n11.2: Metropolis sampling. R. Python.\n11.3: Convergence of Markov chain. R. Python.\n11.4: potential scale reduction \\(\\widehat{R}\\). R. Python.\n\n\n\nRecommended exercises\nOptional but recommended end of the chapter exercises in BDA3 to get a better understanding of the chapter topic:\n\n11.1 (model solution available for 11.1)\n\n\n\nBasics of Markov chains\nSlides by J. Virtamo for the course S-38.143 Queuing Theory (Finnish version) have a nice review of the fundamental terms. See specially the slides for the lecture 4. To prove that Metropolis algorithm works, it is sufficient to show that chain is irreducible, aperiodic and not transient.\n\n\nAnimations\n\nNice animations of some MCMC algorithms with discussion\nAnd just the animations with more options to experiment\n\n\n\nMetropolis algorithm\nThere is a lot of freedom in selection of proposal distribution in Metropolis algorithm. There are some restrictions, but we don’t go to the mathematical details in this course.\nDon’t confuse rejection in the rejection sampling and in Metropolis algorithm. In the rejection sampling, the rejected samples are thrown away. In Metropolis algorithm the rejected proposals are thrown away, but time moves on and the previous sample \\(x_t\\) is also the sample \\(x_{t+1}\\).\nWhen rejecting a proposal, the previous sample is repeated in the chain, they have to be included and they are valid samples from the distribution. For basic Metropolis, it can be shown that optimal rejection rate is 55%–77%, so that on even the optimal case quite many of the samples are repeated samples. However, high number of rejections is acceptable as then the accepted proposals are on average further away from the previous point. It is better to jump further away 23%–45% of time than more often to jump really close. Methods for estimating the effective sample size are useful for measuring how effective a given chain is.\n\n\nTransition distribution vs. proposal distribution\nTransition distribution is a property of Markov chain. In Metropolis algorithm the transition distribution is a mixture of a proposal distribution and a point mass in the current point. The book uses also term jumping distribution to refer to proposal distribution.\n\n\nConvergence\nTheoretical convergence in an infinite time is different than practical convergence in a finite time. There is no exact moment when chain has converged and thus it is not possible to detect when the chain has converged (except for rare perfect sampling methods not discussed in BDA3). The convergence diagnostics can help to find out if the chain is unlikely to be representative of the target distribution. Furthermore, even if would be able to start from a independent sample from the posterior so that chain starts from the convergence, the mixing can be so slow that we may require very large number of samples before the samples are representative of the target distribution.\nIf starting point is selected at or near the mode, less time is needed to reach the area of essential mass, but still the samples in the beginning of the chain are not representative of the true distribution unless the starting point was somehow samples directly from the target distribution.\n\n\n\\(\\widehat{R}\\), effective sample size (ESS, previously \\(n_\\mathrm{eff}\\))\nThere are many versions of \\(\\widehat{R}\\) and effective sample size. Beware that some software packages compute \\(\\widehat{R}\\) using old inferior approaches.\nThe \\(\\widehat{R}\\) and the approach to estimate effective sample size were updated in BDA3, and slightly updated version of this is described in Stan 2.18+ user guide. Since then we have developed even better \\(\\widehat{R}\\), ESS (effective sample size with change from \\(n_\\mathrm{eff}\\) to ESS is due to improved consistency in the notation) in\n\nAki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner (2020). Rank-normalization, folding, and localization: An improved \\(\\widehat{R}\\) for assessing convergence of MCMC. Bayesian analysis, Online.\n\nNew \\(\\widehat{R}\\), ESS, and Monte Carlo error estimates are available in RStan monitor function in R, in posterior package in R, and in ArviZ package in Python.\nDue to randomness in chains, \\(\\widehat{R}\\) may get values slightly below 1.\nBrief Guide to Stan’s Warnings provides summary of available convergence diagnostics in Stan and how to interpret them.\nSometimes people write *the number of effective samples’’ which is wrong (it is possible that notation \\(n_\\mathrm{eff}\\) is partially to blame for this misconception). All the posterior draws in autocorrelated Markov chain are effective, but their efficiency for estimating an expectation depends on the autocorrelation. The effective sample size is not property of individual draws, but joint property of all draws in a sample. Effective sample size also depends on the functional and the effective sample size for a given dependent sample is often different when estimating, for example, \\(\\operatorname{E}[\\theta]\\) or \\(\\operatorname{E}[\\theta^2]\\)."
  },
  {
    "objectID": "BDA3_notes.html#ch12",
    "href": "BDA3_notes.html#ch12",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 12 Computationally efficient Markov chain simulation",
    "text": "Chapter 12 Computationally efficient Markov chain simulation\nChapter 12 is related to the Lecture 6 Stan, HMC, PPL.\n\nOutline\n\n12.1 Efficient Gibbs samplers (not part of the course)\n12.2 Efficient Metropolis jump rules (not part of the course)\n12.3 Further extensions to Gibbs and Metropolis (not part of the course)\n12.4 Hamiltonian Monte Carlo (used in Stan)\n12.5 Hamiltonian dynamics for a simple hierarchical model (read through)\n12.6 Stan: developing a computing environment (read through)\n\nFor the BDA course, there are only 8 pages to read (Sections 12.4–12.6) what is inside Stan.\n\n\nR and Python demos\ng - 12.1: The No-U-Turn Sampler (NUTS) / Dynamic Hamiltonian Monte Carlo. R. - CmdStanR Demos - PyStan demos\n\n\nMCMC animations\nThese don’t include the specific version of dynamic HMC in Stan, but are useful illustrations anyway.\n\nThe No-U-Turn Sampler (NUTS) / Dynamic Hamiltonian Monte Carlo R demo\nMarkov Chains: Why Walk When You Can Flow?\nMCMC animation site by Chi Feng\n\n\n\nHamiltonian Monte Carlo\nAn excellent review of static HMC (the number of steps in dynamic simulation are not adaptively selected) is\n\nRadford Neal (2011). MCMC using Hamiltonian dynamics. In Brooks et al (ed), Handbook of Markov Chain Monte Carlo, Chapman & Hall / CRC Press. Preprint.\n\nStan uses a variant of dynamic Hamiltonian Monte Carlo (using adaptive number of steps in the dynamic simulation), which has been further developed since BDA3 was published. The first dynamic HMC variant was NUTS\n\nMatthew D. Hoffman, Andrew Gelman (2014). The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. JMLR, 15:1593–1623. Online.\n\nThe No-U-Turn Sampler gave the name NUTS which you can see often associated with Stan, but the current variant implemented in Stan has some further developments described (mostly) in\n\nMichael Betancourt (2018). A Conceptual Introduction to Hamiltonian Monte Carlo. arXiv preprint arXiv:1701.02434.\n\n\n\nDivergences and BFMI\n\nDivergence diagnostic checks whether the discretized dynamic simulation has problems due to fast varying density. See more in a Stan case study.\nBrief Guide to Stan’s Warnings provides summary of available convergence diagnostics in Stan,d how to interpret them, and suggestions for solving sampling problems.\n\n\n\nFurther information about Stan\n\nStan web page\nStan Documentation\nI recommend to start with these\n\nBob Carpenter, Andrew Gelman, Matt Hoffman, Daniel Lee, Ben Goodrich, Michael Betancourt, Marcus A. Brubaker, Jiqiang Guo, Peter Li, and Allen Riddell (2015) In press for Journal of Statistical Software. Stan: A Probabilistic Programming Language. Preprint.\nAndrew Gelman, Daniel Lee, and Jiqiang Guo (2015) Stan: A probabilistic programming language for Bayesian inference and optimization. Journal of Educational and Behavior Science. Preprint.\nVideo of Basics of Bayesian inference and Stan tutorial by Jonah Gabry & Lauren Kennedy\n\nMore complete information (no need to read the from the beginning to end, but use when needed)\n\nStan User’s Guide\nStan Reference Manual\nStan Functions Reference\nCmdStanR documentation\n\n\n\n\nCompiler and transpiler\nThis is a minor comment on the terminology. As a shorthand it’s common to see mentioned just Stan compiler, but sometimes the transpiler term is also mentioned as in the slides for this part.\nA Wikipedia article states: A source-to-source translator, source-to-source compiler (S2S compiler), transcompiler, or transpiler is a type of translator that takes the source code of a program written in a programming language as its input and produces an equivalent source code in the same or a different programming language. A source-to-source translator converts between programming languages that operate at approximately the same level of abstraction, while a traditional compiler translates from a higher level programming language to a lower level programming language.\nSo it is more accurate to say that the Stan model code is first transpiled to a C++ code, and then that C++ code is compiled to machine code to create an executable program. Cool thing about the new stanc3 transpiler is that it can create also, for example, LLVM IR or TensorFlow code.\nUsing transpiler and compiler allows to develop Stan language to be good for writing models, but get the benefit of speed and external libraries of C++, TensorFlow, and whatever comes in the future."
  },
  {
    "objectID": "BDA3_notes.html#chapter-13-modal-and-distributional-approximations",
    "href": "BDA3_notes.html#chapter-13-modal-and-distributional-approximations",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 13 Modal and distributional approximations",
    "text": "Chapter 13 Modal and distributional approximations\nChapter 13 is not part of the BDA Aalto course.\nChapter 4 presented normal distribution approximation at the mode (aka Laplace approximation). Chapter 13 discusses more about distributional approximations.\n\nOutline\n\nFinding posterior modes\n\nNewton’s method is very fast if the distribution is close to normal and the computation of the second derivatives is fast\nStan uses limited-memory Broyden-Fletcher-Goldfarb-Shannon (L-BFGS) which is a quasi-Newton method which needs only the first derivatives (provided by Stan autodiff). L-BFGS is known for good performance for wide variety of functions.\n\nBoundary-avoiding priors for modal summaries\n\nAlthough full integration is preferred, sometimes optimization of some parameters may be sufficient and faster, and then boundary-avoiding priors maybe useful.\n\nNormal and related mixture approximations\n\nDiscusses how the normal approximation can be used to approximate integrals of a a smooth function times the posterior.\nDiscusses mixture and \\(t\\) approximations.\n\nFinding marginal posterior modes using EM\n\nExpectation maximization is less important in the time of efficient probabilistic programming frameworks, but can be sometimes useful for extra efficiency.\n\nConditional and marginal posterior approximations\n\nEven in the time of efficient probabilistic programming, the methods discussed in this section can produce very big speedups for a big set of commonly used models. The methods discussed are important part of popular INLA software and are coming also to Stan to speedup latent Gaussian variable models.\n\nExample: hierarchical normal model\nVariational inference\n\nVariational inference (VI) is very popular in machine learning, and this section presents it in terms of BDA. Auto-diff variational inference in Stan was developed after BDA3 was published.\n\nExpectation propagation\n\nPractical efficient computation for expectation propagation (EP) is applicable for more limited set of models than post-BDA3 black-box VI, but for those models EP provides better posterior approximation. Variants of EP can be used for parallelization of any Bayesian computation for hierarchical models.\n\nOther approximations\n\nJust briefly mentions of INLA (in 13.5), CCD (deterministic adaptive quadrature approach) and ABC (inference when you can only sample from the generative model).\n\nUnknown normalizing factors\n\nOften the normalizing factor is not needed, but it can be estimated using importance, bridge or path sampling."
  },
  {
    "objectID": "BDA3_notes.html#ch14",
    "href": "BDA3_notes.html#ch14",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 14 Introduction to regression models",
    "text": "Chapter 14 Introduction to regression models\nChapter 14 is not part of the BDA Aalto course.\n\nOutline\n\nConditional modeling\n\nformal justification of conditional modeling\nif joint model factorizes \\(p(y,x|\\theta,\\phi)=p(y|x,\\theta)}p(x|\\phi)\\)\nwe can model just \\(p(y|x,\\theta)}\\)\n\nBayesian analysis of classical regression\n\nuninformative prior on \\(\\beta\\) and \\(\\sigma^2\\)\nconnection to multivariate normal (cf. Chapter 3) is useful to understand as it then reveals what would be the conjugate prior\nclosed form posterior and posterior predictive distribution\nthese properties are sometimes useful and thus good to know, but with probabilistic programming less often needed\n\nRegression for causal inference: incumbency and voting\n\nModeling example with bit of discussion on causal inference (see more in Regression and Other Stories (ROS) Chs. 18-21)\n\nGoals of regression analysis\n\ndiscussion of what we can do with regression analysis (see more in ROS)\n\nAssembling the matrix of explanatory variables\n\ntransformations, nonlinear relations, indicator variables, interactions (see more in ROS)\n\nRegularization and dimension reduction\n\na bit outdated and short (Bayesian Lasso is not a good idea), see more in Lecture 9.3.\n\nUnequal variances and correlations\n\nuseful concept, but computation is easier with probabilistic programming frameworks\n\nIncluding numerical prior information\n\nuseful conceptually, but easy computation with probabilistic programming frameworks makes it easier to define prior information as the prior doesn’t need to be conjugate\nsee more about priors in Prior Choice Recommendations Wiki"
  },
  {
    "objectID": "BDA3_notes.html#chapter-15-hierarchical-linear-models",
    "href": "BDA3_notes.html#chapter-15-hierarchical-linear-models",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 15 Hierarchical linear models",
    "text": "Chapter 15 Hierarchical linear models\nChapter 15 is not part of the BDA Aalto course.\nChapter 15 combines hierarchical models from Chapter 5 and linear models from Chapter 14. The chapter discusses some computational issues, but probabilistic programming frameworks make computation for hierarchical linear models easy.\n\nOutline\n\nRegression coefficients exchangeable in batches\n\nexchangeability of parameters\nthe discussion of fixed-, random- and mixed-effects models is incomplete\n\nwe don’t recommend using these terms, but they are so popular that it’s useful to know them\na relevant comment is The terms ‘fixed’ and ‘random’ come from the non-Bayesian statistical tradition and are somewhat confusing in a Bayesian context where all unknown parameters are treated as ‘random’ or, equivalently, as having fixed but unknown values.\noften fixed effects correspond to population level coefficients, random effects correspond to group or individual level coefficients, and mixed model has both ——————————- —————————————– y \\sim 1 + x fixed / population effect; pooled model y \\sim 1 + (0 + x | g) random / group effects y \\sim 1 + x + (1 + x | g) mixed effects; hierarchical model ——————————- —————————————–\n\n\nExample: forecasting U.S. presidential elections\n\nillustrative example\n\nInterpreting a normal prior distribution as extra data\n\nincludes very useful interpretation of hierarchical linear model as a single linear model with certain design matrix\n\nVarying intercepts and slopes\n\nextends from hierarchical model for scalar parameter to joint hierarchical model for several parameters\n\nComputation: batching and transformation\n\nGibbs sampling part is mostly outdated\ntransformations for HMC is useful if you write your own models, but the section is quite short and you can get more information from Stan user guide 21.7 Reparameterization and Divergences case study\n\nAnalysis of variance and the batching of coefficients\n\nANOVA as Bayesian hierarchical linear model\nrstanarm and brms packages make it easy to make ANOVA\n\nHierarchical models for batches of variance components\n\nmore variance components"
  },
  {
    "objectID": "BDA3_notes.html#chapter-16-generalized-linear-models",
    "href": "BDA3_notes.html#chapter-16-generalized-linear-models",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 16 Generalized linear models",
    "text": "Chapter 16 Generalized linear models\nChapter 16 is not part of the BDA Aalto course.\nChapter 16 extends linear models to have non-normal observation models. Model in Bioassay example in Chapter 3 is also generalized linear model. Chapter reviews the basics and discusses some computational issues, but probabilistic programming frameworks make computation for generalized linear models easy (especially with rstanarm and brms). See ROS) for discussion of generalized linear models from the modeling perspective more thoroughly.\n\nOutline\n\nParts of generalized linear model (GLM):\n\nThe linear predictor \\(\\eta = X\\beta\\)\nThe link function \\(g(\\cdot)\\) and \\(\\mu = g^{-1}(\\eta)\\)\nOutcome distribution model with location parameter \\(\\mu\\)\n\nthe distribution can also depend on dispersion parameter \\(\\phi\\)\noriginally just exponential family distributions (e.g. Poisson, binomial, negative-binomial), which all have natural location-dispersion parameterization\nafter MCMC made computation easy, GLM can refer to models where outcome distribution is not part of exponential family and dispersion parameter may have its own latent linear predictor\n\n\nStandard generalized linear model likelihoods\n\nsection title says “likelihoods”, but it would be better to say “observation models”\ncontinuous data: normal, gamma, Weibull mentioned, but common are also Student’s \\(t\\), log-normal, log-logistic, and various extreme value distributions like generalized Pareto distribution\nbinomial (Bernoulli as a special case) for binary and count data with upper limit\n\nBioassay model uses binomial observation model\n\nPoisson for count data with no upper limit\n\nPoisson is useful approximation of Binomial when the observed counts are much smaller than the upper limit\n\n\nWorking with generalized linear models\n\nbit of this and that information on how think about GLMs (see ROS for more)\nnormal approximation to the likelihood is good for thinking how much information non-normal observations provide, can be useful for someone thinking about computation, but easy computation with probabilistic programming frameworks means not everyone needs this\n\nWeakly informative priors for logistic regression\n\nan excellent section although the recommendation on using Cauchy has changed (see Prior Choice Recommendations Wiki)\nthe problem of separation is useful to understand\ncomputation part is outdated as probabilistic programming frameworks make the computation easy\n\nOverdispersed Poisson regression for police stops\n\nan example\n\nState-level opinions from national polls\n\nanother example\n\nModels for multivariate and multinomial responses\n\nextension to multivariate responses\npolychotomous data with multivariate binomial or Poisson\nmodels for ordered categories\n\nLoglinear models for multivariate discrete data\n\nmultinomial or Poisson as loglinear models"
  },
  {
    "objectID": "BDA3_notes.html#chapter-17-models-for-robust-inference",
    "href": "BDA3_notes.html#chapter-17-models-for-robust-inference",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 17 Models for robust inference",
    "text": "Chapter 17 Models for robust inference\nChapter 17 is not part of the BDA Aalto course.\nChapter 17 discusses over-dispersed observation models. The discussion is useful beyond generalized linear models. The computation is outdated. See ROS for more examples.\n\nOutline\n\nAspects of robustness\n\noverdispersed models are often connected to robustness of inferences to outliers, but the observed data can be overdispersed without any observation being outlier\noutlier is sensible only in the context of the model, being something not well modeled or something requiring extra model component\nswitching to generic overdispersed model can help to recognize problem in the non-robust model (sensitivity analysis), but it can also throw away useful information in the “outliers” and it would be useful to think what is the generative mechanism for observations which are not like others\n\nOverdispersed versions of standard models:\n\nnormal \\(\\rightarrow\\) \\(t\\)-distribution\nPoisson \\(\\rightarrow\\) negative-binomial\nbinomial \\(\\rightarrow\\) beta-binomial\nprobit \\(\\rightarrow\\) logistic / robit\n\nPosterior inference and computation\n\ncomputation part is outdated as probabilistic programming frameworks and MCMC make the computation easy\nposterior is more likely to be multimodal\n\nRobust inference for the eight schools\n\neight schools example is too small too see much difference\n\nRobust regression using t-distributed errors\n\ncomputation part is outdated as probabilistic programming frameworks and MCMC make the computation easy\nposterior is more likely to be multimodal"
  },
  {
    "objectID": "BDA3_notes.html#chapter-18-models-for-missing-data",
    "href": "BDA3_notes.html#chapter-18-models-for-missing-data",
    "title": "Bayesian Data Analysis course - BDA3 notes",
    "section": "Chapter 18 Models for missing data",
    "text": "Chapter 18 Models for missing data\nChapter 18 is not part of the BDA Aalto course.\nChapter 18 extends the data collection modeling from Chapter 8. See ROS for more examples.\n\nOutline\n\nNotation\n\nMissing completely at random (MCAR) missingness does not depend on missing values or other observed values (including covariates)\nMissing at random (MAR)\nmissingness does not depend on missing values but may depend on other observed values (including covariates)\nMissing not at random (MNAR)\nmissingness depends on missing values\n\nMultiple imputation\n\nmake a model predicting missing data\nsample repeatedly from the missing data model to generate multiple imputed data sets\nmake usual inference for each imputed data set\ncombine results\ndiscussion of computation is partially outdated\n\nMissing data in the multivariate normal and \\(t\\) models\n\na special continuous data case computation, which can still be useful as fast starting point\n\nExample: multiple imputation for a series of polls\n\nan example\n\nMissing values with counted data\n\ndiscussion of computation for count data (ie. computation in 18.3 is not applicable)\n\nExample: an opinion poll in Slovenia\n\nanother example"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Bayesian Data Analysis course - Assignments",
    "section": "",
    "text": "A big change 2024 in the course is the reduced use of peergrading as the previously used system was shut down, which means also substantial changes in the assignments.\nYou are free to use these assignments in self study and other courses (CC-BY-NC 4.0), but please do not publish complete answers online."
  },
  {
    "objectID": "assignments.html#weekly-assignments",
    "href": "assignments.html#weekly-assignments",
    "title": "Bayesian Data Analysis course - Assignments",
    "section": "Weekly assignments",
    "text": "Weekly assignments\nThere are 9 weekly assignments (two of them have two weeks to submit). Assignments are linked from the schedule and can also be found from git repo assignments folder. The deadline days for the assignments are given in the course schedule.\n\nThe assignments are weighted as 5%, 5%, 10%, 10%, 15%, 20%, 15%, 10%, 10% (total 100%)\nThe assignments are introduced on Mondays.\nMost of the assignments are submitted via MyCourses Quiz questions with links in MyCourses.\nEach assignment has also a Quarto template with some R code, but in early assignment rounds, you don’t need to submit any report. Just answer the questions in MyCourses Quiz.\nThere are chat streams #assignment1 etc. you can ask questions about the assignments. Other students and TAs can answer these. There is no guaranteed response time. These streams are best for questions that are likely to have relatively simple answer.\nThere are TA sessions for getting help. These sessions are not obligatory. These sessions are useful if you think you need help that requires a bit more discussion. The questions are answered during the TA session time (if there are too many questions, they may be answered in the chat or next TA session).\nWe highly recommend to submit all assignments Friday before 3pm so that you can get TA help before submission. As the course has students who work weekdays (e.g. FiTech students), the late submission until Sunday night is allowed, but we can’t provide support during the weekends. Sisu and MyCourses list Firday TA sessions, which are not organized unless otherwise announced.\nYour Quiz answers are autosubmitted by MyCourses at the deadline time, also if you have answered only some of the questions. \n\nNOTE: The assignment instructions will be updated during the course, and individual assignments are not guaranteed to be up to date until Monday 8am of the hand-in period of the corresponding assignment week (See the deadlines below, e.g. the first assignment is not guaranteed to be up to date until 2024-09-02 at 8am).\n\nThe assignments are mostly solved using computer (R and Stan). Related demos for each assignment are available in the course web pages (links in Materials section). See TA sessions for getting help.\n\n\nBonus points\nIn addition to the assignment score, one can get bonus points from course chat activity (e.g. helping other students and reporting typos in the material) and by answering a time usage questionnaire on the course page in MyCourses. Course chat activity and the questionnaire are not included in the maximum assignment score and thus are not required to receive a full 100% score.\nOther students’ course chat activity will not affect one’s resulting grade, i.e. there is no need to try to perform in the course chat or to take any part in it at all."
  },
  {
    "objectID": "assignments.html#ta-sessions",
    "href": "assignments.html#ta-sessions",
    "title": "Bayesian Data Analysis course - Assignments",
    "section": "TA sessions",
    "text": "TA sessions\nYou can get help for the assignments by asking in the course chat from other students or in weekly TA sessions by asking TAs. The sessions are voluntary.\n\nThere are chat streams #assignment1 etc. you can ask questions about the assignments. Other students and TAs can answer these. There is no guaranteed response time. These streams are best for questions that are likely to have relatively simple answer and thus are likely to be answered before the next TA session.\nThere are TA sessions for getting one-on-one help. These sessions are not obligatory. These sessions are useful if you think you need help that requires a bit more discussion. The questions are answered during the TA session time (if there are two many questions, they may be answered in the chat or next TA session).\n\nThere are two TA sessions each week; please see the course schedule for more details on the meeting times and rooms. In the TA sessions, you can get one-to-one help with your assignments and project work.\nDuring the TA session you can get help in the following forms:\n\nWritten communication on the course chat: you will chat with a TA using the “direct messages” feature on the course chat. You can also, for example, share code snippets and equations through chat direct messages if it helps.\nOral communication on Zoom: you will chat with a TA using a video conference on Zoom. You can also use, for example, screen sharing on Zoom if it helps.\nOral communication on campus: you will chat with a TA live in computer class room\n\nWe will use the stream #queue in the course chat to coordinate everything. We announce there when the TA session starts. Then you can write your help request there, describing in sufficient detail exactly what is the problem with which you would need help (see below).\nOnce a TA is free and your question is the first request in the queue, a TA will mark it with a check mark reaction. Then the TA will contact you and help with your problem. Finally, once the problem is solved, the TA who helped you will delete your request from the queue.\n\nGetting help on campus\n\nLogin with Aalto account to Zulip course chat (link in MyCourses)\nGo to the chat stream #queue.\nWrite a help request (see below), starting with the keyword “Live”.\nWhen it’s your turn in queue, TA will send you a direct message on Chat, and you will coninue the discussion live.\n\nIf you have follow-up questions later, please put a new request to the queue.\n\n\nGetting help via the course chat\n\nLogin with Aalto account to Zulip course chat (link in MyCourses)\nGo to the chat stream #queue.\nWrite a help request (see below), starting with the keyword “Chat”.\nA TA will send you a direct message on Chat.\nYou will discuss through direct messages until your problem is solved, and then the TA will close the discussion and delete your help request.\n\nPlease do not send direct messages to TAs without going through the above protocol. If you have follow-up questions later, please put a new request to the queue.\n\n\nGetting help via Zoom\n\nFollow these instructions to install Zoom and to sign in to Zoom.\nOpen Zoom, and make sure your video and audio are configured correctly. Create a new Zoom conference call, and copy the meeting URL.\nGo to the Chat stream #queue.\nWrite a help request (see below), starting with the keyword “Zoom”, and end it with the Zoom meeting URL.\nA TA will click on the meeting URL to join the Zoom conference that you created.\nYou will get help until your problem is solved, and then the TA will close the call and delete your help request.\n\n\n\nWhat to write in the help request?\nYour help request should preferably contain a concise summary of exactly what kind of help you would need. Ideally, after reading the help request, a TA should be able to already have an answer for you, or point to FAQ.\nTry to describe what is the problem, what you have tried, what you already know, and exactly what is the relevant part of the code. Please highlight the important parts. Here are some fictional examples of good help requests:\n\nZoom: I am trying to install the R package … on my personal laptop and I am getting the following error:\nEXAMPLE ERROR\nMy operating system is …, I have version … of R installed and I am using RStudio. I tried googling the error but was not able to solve the issue. Zoom meeting link: https://aalto.zoom.us/j/XXX\n\n\n\nAcknowledgements\nTA session instructions above have been copied from Programming Parallel Computers by Jukka Suomela with CC-BY-4.0 license."
  },
  {
    "objectID": "demos.html",
    "href": "demos.html",
    "title": "Bayesian Data Analysis course - Demos",
    "section": "",
    "text": "The BDA_R_demos repository contains some R demos and additional notes for the book Bayesian Data Analysis, 3rd ed by Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin (BDA3).\nCurrently there are demos for BDA3 Chapters 2, 3, 4, 5, 6, 10, 11 and 12. Furthermore there are demos for RStan and RStanARM.\nThe initial demos were originally written for Matlab/Octave by Aki Vehtari and translated to R by Markus Paasiniemi. Recently more demos have been added for RStan, CmdStanR and RStanARM. Unless otherwise specified in specific files all code licensed under BSD-3 and all text, slides and figures licensed under CC-BY-NC 4.0.\nSee also Model Selection tutorial.\n\nZIP file for all R demos\nChapter 2\n\n2.1: Probability of a girl birth given placenta previa (BDA3 p. 37)\n2.2: Illustrate the effect of prior in binomial model\n2.3: Illustrate simulation based inference\n2.4: Illustrate grid and inverse-cdf sampling\n\nChapter 3\n\n3.1_4: Normal model with unknown mean and variance (BDA3 section 3.2 on p. 64)\n3.5: Estimating the speed of light using normal model BDA3 p. 66\n3.6: Binomial regression and grid sampling with bioassay data (BDA3 p. 74-)\n\nChapter 4\n\n4.1: Normal approximation for binomial regression model and Bioassay data\n\nChapter 5\n\n5.1: Hierarchical model for Rats experiment (BDA3, p. 102)\n5.2: Hierarchical model for SAT-example data (BDA3, p. 102)\n\nChapter 6\n\n6.1: Posterior predictive checking of normal model for light data\n6.2: Posterior predictive checking for independence in binomial trials\n6.3: Posterior predictive checking of normal model with poor test statistic\n6.4: Marginal posterior predictive checking with PIT test\n\nChapter 7\n\nSee model selection tutorial\n\nChapter 10\n\n10.1: Rejection sampling\n10.2: Importance sampling\n10.3: Importance sampling with normal distribution as a proposal for Bioassay model\n\nChapter 11\n\n11.1: Gibbs sampling illustration\n11.2: Metropolis sampling + convergence illustration\n11.3_4: Metropolis sampling + convergence illustration\n11.5: Diagnostics with posterior and bayesplot packages\n\nChapter 12\n\n12.1: Static Hamiltonian Monte Carlo illustration\n12.2: NUTS / Dynamic Hamiltonian Monte Carlo illustration\n\nCmdStanR, RStan and RStanARM\n\nCmdStanR demos\nRStan demos\nRStanARM demos\nPosterior predictive checking\nDoes brain mass predict how much mammals sleep in a day?\nNon-linear model for traffic deaths in Finland\nExtreme value analysis and user defined probability functions in Stan"
  },
  {
    "objectID": "demos.html#bda-r-demos",
    "href": "demos.html#bda-r-demos",
    "title": "Bayesian Data Analysis course - Demos",
    "section": "",
    "text": "The BDA_R_demos repository contains some R demos and additional notes for the book Bayesian Data Analysis, 3rd ed by Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin (BDA3).\nCurrently there are demos for BDA3 Chapters 2, 3, 4, 5, 6, 10, 11 and 12. Furthermore there are demos for RStan and RStanARM.\nThe initial demos were originally written for Matlab/Octave by Aki Vehtari and translated to R by Markus Paasiniemi. Recently more demos have been added for RStan, CmdStanR and RStanARM. Unless otherwise specified in specific files all code licensed under BSD-3 and all text, slides and figures licensed under CC-BY-NC 4.0.\nSee also Model Selection tutorial.\n\nZIP file for all R demos\nChapter 2\n\n2.1: Probability of a girl birth given placenta previa (BDA3 p. 37)\n2.2: Illustrate the effect of prior in binomial model\n2.3: Illustrate simulation based inference\n2.4: Illustrate grid and inverse-cdf sampling\n\nChapter 3\n\n3.1_4: Normal model with unknown mean and variance (BDA3 section 3.2 on p. 64)\n3.5: Estimating the speed of light using normal model BDA3 p. 66\n3.6: Binomial regression and grid sampling with bioassay data (BDA3 p. 74-)\n\nChapter 4\n\n4.1: Normal approximation for binomial regression model and Bioassay data\n\nChapter 5\n\n5.1: Hierarchical model for Rats experiment (BDA3, p. 102)\n5.2: Hierarchical model for SAT-example data (BDA3, p. 102)\n\nChapter 6\n\n6.1: Posterior predictive checking of normal model for light data\n6.2: Posterior predictive checking for independence in binomial trials\n6.3: Posterior predictive checking of normal model with poor test statistic\n6.4: Marginal posterior predictive checking with PIT test\n\nChapter 7\n\nSee model selection tutorial\n\nChapter 10\n\n10.1: Rejection sampling\n10.2: Importance sampling\n10.3: Importance sampling with normal distribution as a proposal for Bioassay model\n\nChapter 11\n\n11.1: Gibbs sampling illustration\n11.2: Metropolis sampling + convergence illustration\n11.3_4: Metropolis sampling + convergence illustration\n11.5: Diagnostics with posterior and bayesplot packages\n\nChapter 12\n\n12.1: Static Hamiltonian Monte Carlo illustration\n12.2: NUTS / Dynamic Hamiltonian Monte Carlo illustration\n\nCmdStanR, RStan and RStanARM\n\nCmdStanR demos\nRStan demos\nRStanARM demos\nPosterior predictive checking\nDoes brain mass predict how much mammals sleep in a day?\nNon-linear model for traffic deaths in Finland\nExtreme value analysis and user defined probability functions in Stan"
  },
  {
    "objectID": "demos.html#bda-python-demos",
    "href": "demos.html#bda-python-demos",
    "title": "Bayesian Data Analysis course - Demos",
    "section": "BDA Python demos",
    "text": "BDA Python demos\nBDA_py_demos repository some Python demos for the book Bayesian Data Analysis, 3rd ed by Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin (BDA3).\nCurrently there are demos for BDA3 Chapters 2, 3, 4, 5, 6, 10 and 11. Furthermore, there are demos for PyStan.\nDemos are in jupyter notebook (.ipynb) format. These can be directly previewed in GitHub without need to install or run anything.\nCorresponding demos were originally written for Matlab/Octave by Aki Vehtari and translated to Python by Tuomas Sivula. Some improvements were contributed by Pellervo Ruponen and Lassi Meronen.\n\nZIP file for all Python demos\nChapter 2\n\n2.1: Probability of a girl birth given placenta previa (BDA3 p. 37)\n2.2: Illustrate the effect of prior in binomial model\n2.3: Illustrate simulation based inference\n2.4: Illustrate grid and inverse-cdf sampling\n\nChapter 3\n\n3.1_4: Normal model with unknown mean and variance (BDA3 section 3.2 on p. 64)\n3.5: Estimating the speed of light using normal model BDA3 p. 66\n3.6: Binomial regression and grid sampling with bioassay data (BDA3 p. 74-)\n\nChapter 4\n\n4.1: Normal approximation for binomial regression model and Bioassay data\n\nChapter 5\n\n5.1: Hierarchical model for Rats experiment (BDA3, p. 102)\n5.2: Hierarchical model for SAT-example data (BDA3, p. 102)\n\nChapter 6\n\n6.1: Posterior predictive checking of normal model for light data\n6.2: Posterior predictive checking for independence in binomial trials\n6.3: Posterior predictive checking of normal model with poor test statistic\n6.4: Marginal posterior predictive checking with PIT test\n\nChapter 10\n\n10.1: Rejection sampling\n10.2: Importance sampling\n10.3: Importance sampling with normal distribution as a proposal for Bioassay model\n\nChapter 11\n\n11.1: Gibbs sampling illustration\n11.2: Metropolis sampling + convergence illustration\n\nPyStan\n\nPyStan test installation\nPyStan demos\n\n\n\nPython requirements\n\npython 3\nipython\nnumpy\nscipy\nmatplotlib 2\npandas (for some demos)\npystan (for some demos)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Data Analysis course",
    "section": "",
    "text": "This is the web page for the Bayesian Data Analysis course at Aalto (CS-E5710) by Aki Vehtari.\nAalto students should check the 2024 specific course web page and MyCourses. In 2024 Aalto course can be taken online except for the final project presentation. The lectures will be given on campus, but recorded and the recording will be made available online after the course. The registration for the course lectures will be used to estimate the need for the resources. If you are unable to register for the course at the moment in the Sisu, there is no need to email the lecturer. You can start taking the course and register before the end of the course. Sisu shows rooms on campus for the computer exercises, but you can join the TA sessions also online via Zulip and Zoom. You can choose which TA session to join each week separately, without a need to register for those sessions.\nAll the course material is available in a git repo (and these pages are for easier navigation). All the material can be used in other courses. Text (except the BDA3 book) and videos licensed under CC-BY-NC 4.0. Code licensed under BSD-3.\nThe electronic version of the course book Bayesian Data Analysis, 3rd ed, by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin is available for non-commercial purposes. Hard copies are available from the publisher and many book stores. See also home page for the book, errata for the book, and chapter notes."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Bayesian Data Analysis course",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nBasic terms of probability theory\n\nprobability, probability density, distribution\nsum, product rule, and Bayes’ rule\nexpectation, mean, variance, median\nin Finnish, see e.g. Stokastiikka ja tilastollinen ajattelu\nin English, see e.g. Wikipedia and Introduction to probability and statistics\n\nSome algebra and calculus\nBasic visualisation techniques (R or Python)\n\nhistogram, density plot, scatter plot\nsee e.g. BDA R demos\nsee e.g. BDA Python demos\n\n\nThis course has been designed so that there is strong emphasis in computational aspects of Bayesian data analysis and using the latest computational tools.\nIf you find BDA3 too difficult to start with, I recommend\n\nFor regression models, their connection to statistical testing and causal analysis see Gelman, Hill and Vehtari, “Regression and Other Stories”.\nRichard McElreath’s Statistical Rethinking, 2nd ed book is easier than BDA3 and the 2nd ed is excellent. Statistical Rethinking doesn’t go as deep in some details, math, algorithms and programming as BDA course. Richard’s lecture videos of Statistical Rethinking: A Bayesian Course Using R and Stan are highly recommended even if you are following BDA3.\nFor background prerequisites some students have found chapters 2, 4 and 5 in Kruschke, “Doing Bayesian Data Analysis” useful."
  },
  {
    "objectID": "index.html#course-contents-following-bda3",
    "href": "index.html#course-contents-following-bda3",
    "title": "Bayesian Data Analysis course",
    "section": "Course contents following BDA3",
    "text": "Course contents following BDA3\nBayesian Data Analysis, 3rd ed, by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin. Home page for the book. Errata for the book. Electronic edition for non-commercial purposes only.\n\nBackground (Ch 1, Lecture 1)\nSingle-parameter models (Ch 2, Lecture 2)\nMultiparameter models (Ch 3, Lecture 3)\nComputational methods (Ch 10 , Lecture 4)\nMarkov chain Monte Carlo (Chs 11-12, Lectures 5-6)\nExtra material for Stan and probabilistic programming (see below, Lecture 6)\nHierarchical models (Ch 5, Lecture 7)\nModel checking (Ch 6, Lectures 8-9)\n\n+ Visualization in Bayesian workflow\n\nEvaluating and comparing models (Ch 7)\n\n+ Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC (Journal link)\n+ Case studies\n+ Videos\n+ Cross-validation FAQ\n\nDecision analysis (Ch 9, Lecture 10)\nLarge sample properties and Laplace approximation (Ch 4, Lecture 11-12)\nIn addition you learn workflow for Bayesian data analysis"
  },
  {
    "objectID": "index.html#how-to-study",
    "href": "index.html#how-to-study",
    "title": "Bayesian Data Analysis course",
    "section": "How to study",
    "text": "How to study\nRecommended way to go through the material is\n\nRead the reading instructions for a chapter in chapter notes.\nRead the chapter in BDA3 and check that you find the terms listed in the reading instructions.\nWatch the corresponding lecture video to get explanations for most important parts.\nRead corresponding additional information in the chapter notes.\nRun the corresponding demos in R demos or Python demos.\nRead the exercise instructions and make the corresponding assignments. Demo codes in R demos and Python demos have a lot of useful examples for handling data and plotting figures. If you have problems, visit TA sessions or ask in course slack channel.\nIf you want to learn more, make also self study exercises listed below"
  },
  {
    "objectID": "index.html#slides-and-chapter-notes",
    "href": "index.html#slides-and-chapter-notes",
    "title": "Bayesian Data Analysis course",
    "section": "Slides and chapter notes",
    "text": "Slides and chapter notes\n\nSlides\n\nincluding code for reproducing some of the figures\n\nChapter notes\n\nincluding reading instructions highlighting most important parts and terms"
  },
  {
    "objectID": "index.html#videos",
    "href": "index.html#videos",
    "title": "Bayesian Data Analysis course",
    "section": "Videos",
    "text": "Videos\nThe following video motivates why computational probabilistic methods and probabilistic programming are important part of modern Bayesian data analysis.\n\nComputational probabilistic modeling in 15mins\n\nShort video clips on selected introductory topics are available in a Panopto folder.\nThe 2022 lecture videos are in a Panopto folder. The 2023 lecture videos will be uploaded in another folder after each lecture."
  },
  {
    "objectID": "index.html#r-and-python",
    "href": "index.html#r-and-python",
    "title": "Bayesian Data Analysis course",
    "section": "R and Python",
    "text": "R and Python\nWe strongly recommend using R in the course as there are more packages for Stan and statistical analysis in R. If you are already fluent in Python, but not in R, then using Python may be easier, but it can still be more useful to learn also R. Unless you are already experienced and have figured out your preferred way to work with R, we recommend installing RStudio Desktop or using Aalto teaching JupyterHub. See FAQ for frequently asked questions about R problems in this course. The demo codes provide useful starting points for all the assignments.\n\nFor learning R programming basics we recommend\n\nGarrett Grolemund, Hands-On Programming with R\n\nFor learning basic and advanced plotting using R we recommend\n\nKieran Healy, Data Visualization - A practical introduction\nAntony Unwin, Graphical Data Analysis with R"
  },
  {
    "objectID": "index.html#demos",
    "href": "index.html#demos",
    "title": "Bayesian Data Analysis course",
    "section": "Demos",
    "text": "Demos\nThese demos include a lot of useful code for making the assignments.\n\nR demos\nPython demos"
  },
  {
    "objectID": "index.html#self-study-exercises",
    "href": "index.html#self-study-exercises",
    "title": "Bayesian Data Analysis course",
    "section": "Self study exercises",
    "text": "Self study exercises\nGreat self study BDA3 exercises for this course are listed below. Most of these have also model solutions available.\n\n1.1-1.4, 1.6-1.8 (model solutions for 1.1-1.6)\n2.1-2.5, 2.8, 2.9, 2.14, 2.17, 2.22 (model solutions for 2.1-2.5, 2.7-2.13, 2.16, 2.17, 2.20, and 2.14 is in slides)\n3.2, 3.3, 3.9 (model solutions for 3.1-3.3, 3.5, 3.9, 3.10)\n4.2, 4.4, 4.6 (model solutions for 3.2-3.4, 3.6, 3.7, 3.9, 3.10)\n5.1, 5.2 (model solutions for 5.3-5.5, 5.7-5.12)\n6.1 (model solutions for 6.1, 6.5-6.7)\n9.1\n10.1, 10.2 (model solution for 10.4)\n11.1 (model solution for 11.1)"
  },
  {
    "objectID": "index.html#stan",
    "href": "index.html#stan",
    "title": "Bayesian Data Analysis course",
    "section": "Stan",
    "text": "Stan\n\nStan home page\nIntroductory article in Journal of Statistical Software\nDocumentation\nRStan installation\nPyStan installation\nBasics of Bayesian inference and Stan, Jonah Gabry & Lauren Kennedy Part 1 and Part 2"
  },
  {
    "objectID": "index.html#extra-reading",
    "href": "index.html#extra-reading",
    "title": "Bayesian Data Analysis course",
    "section": "Extra reading",
    "text": "Extra reading\n\nDicing with the unknown\nOrigin of word Bayesian\nModel selection\nCross-validation FAQ"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Bayesian Data Analysis course",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe course material has been greatly improved by the previous and current course assistants (in alphabetical order): Michael Riis Andersen, Paul Bürkner, Akash Daka, Alejandro Catalina, Kunal Ghosh, Meenal Jhajharia, Andrew Johnson, Noa Kallioinen, Joona Karjalainen, David Kohns, Juho Kokkala, Leevi Lindgren, Yann McLatchie, Måns Magnusson, Anton Mallasto, Janne Ojanen, Topi Paananen, Markus Paasiniemi, Juho Piironen, Anna Riha, Jaakko Riihimäki, Niko Siccha, Eero Siivola, Tuomas Sivula, Teemu Säilynoja, Jarno Vanhatalo.\nThe web page has been made with rmarkdown’s site generator."
  },
  {
    "objectID": "project_gsu.html",
    "href": "project_gsu.html",
    "title": "Bayesian Data Analysis course - Project work (GSU)",
    "section": "",
    "text": "Bayesian Data Analysis Global South (GSU) 2023"
  },
  {
    "objectID": "project_gsu.html#project-work-details",
    "href": "project_gsu.html#project-work-details",
    "title": "Bayesian Data Analysis course - Project work (GSU)",
    "section": "Project work details",
    "text": "Project work details\nProject work involves choosing a data set and performing a whole analysis according to all the parts of Bayesian workflow studied along the course. In this course instance there are no project presentations, but you will get feedback from your peers. You can do the project work in groups if you like."
  },
  {
    "objectID": "project_gsu.html#project-schedule",
    "href": "project_gsu.html#project-schedule",
    "title": "Bayesian Data Analysis course - Project work (GSU)",
    "section": "Project schedule",
    "text": "Project schedule\n\nSee the overall schedule of the GSU 2023 course\nStart working on the project in midway of the course.\nProject report deadline 15th May Submit in peergrade (separate “class”, the class code will be posted in the course chat announcements).\nProject report peer grading 18-20th May."
  },
  {
    "objectID": "project_gsu.html#groups",
    "href": "project_gsu.html#groups",
    "title": "Bayesian Data Analysis course - Project work (GSU)",
    "section": "Groups",
    "text": "Groups\nIn this course instance the project work can be done in groups of 1-4 persons, but you don’t need to find a group.\nIf you don’t have a group, you can ask other students in the group chat channel #project. Tell what kind of data you are interested in (e.g. medicine, health, biological, engineering, political, business), whether you prefer R or Python, and whether you have already more concrete idea for the topic."
  },
  {
    "objectID": "project_gsu.html#evaluation",
    "href": "project_gsu.html#evaluation",
    "title": "Bayesian Data Analysis course - Project work (GSU)",
    "section": "Evaluation",
    "text": "Evaluation\nIn this course instance the project work’s evaluation consists of only from\n\npeergraded project report (40%) (within peergrade submission 80% and feedack 20%)"
  },
  {
    "objectID": "project_gsu.html#project-report",
    "href": "project_gsu.html#project-report",
    "title": "Bayesian Data Analysis course - Project work (GSU)",
    "section": "Project report",
    "text": "Project report\nIn the project report you practice presenting the problem and data analysis results, which means that minimal listing of code and figures is not a good report. There are different levels for how data analysis project could be reported. This report should be more than a summary of results without workflow steps. While describing the steps and decisions made during the workflow, to keep the report readable some of the diagnostic outputs and code can be put in the appendix. If you are uncertain you can ask TAs in TA sessions whether you are on a good level of amount of details.\nThe report should not be over 20 pages and should include\n\nIntroduction describing\n\nthe motivation\nthe problem\nand the main modeling idea.\nShowing some illustrative figure is recommended.\n\nDescription of the data and the analysis problem. Provide information where the data was obtained, and if it has been previously used in some online case study and how your analysis differs from the existing analyses.\nDescription of at least two models, for example:\n\nnon hierarchical and hierarchical,\nlinear and non linear,\nvariable selection with many models.\n\nInformative or weakly informative priors, and justification of their choices.\nStan, rstanarm or brms code.\nHow the Stan model was run, that is, what options were used. This is also more clear as combination of textual explanation and the actual code line.\nConvergence diagnostics (\\(\\widehat{R}\\), ESS, divergences) and what was done if the convergence was not good with the first try. This should be reported for all models.\nPosterior predictive checks and what was done to improve the model. This should be reported for all models.\nOptional/Bonus: Predictive performance assessment if applicable (e.g. classification accuracy) and evaluation of practical usefulness of the accuracy. This should be reported for all models as well.\nSensitivity analysis with respect to prior choices (i.e. checking whether the result changes a lot if prior is changed). This should be reported for all models.\nModel comparison (e.g. with LOO-CV).\nDiscussion of issues and potential improvements.\nConclusion what was learned from the data analysis.\nSelf-reflection of what the you/group learned while making the project."
  },
  {
    "objectID": "project_gsu.html#data-sets",
    "href": "project_gsu.html#data-sets",
    "title": "Bayesian Data Analysis course - Project work (GSU)",
    "section": "Data sets",
    "text": "Data sets\nAs some data sets have been overused for these particular goals, note that the following ones are forbidden in this work (more can be added to this list so make sure to check it regularly):\n\nextremely common data sets like titanic, mtcars, iris\nBaseball batting (used by Bob Carpenter’s StanCon case study).\nData sets used in the course demos\n\nIt’s best to use a dataset for which there is no ready made analysis in internet, but if you choose a dataset used already in some online case study, provide the link to previous studies and report how your analysis differs from those (for example if someone has made non-Bayesian analysis and you do the full Bayesian analysis).\nDepending on the model and the structure of the data, a good data set would have more than 100 observations but less than 1 million. If you know an interesting big data set, you can use a smaller subset of the data to keep the computation times feasible. It would be good that the data has some structure, so that it is sensible to use multilevel/hierarchical models.\nIf you’re looking for inspiration or you’re not sure where to begin, take a browse over this list of datasets arranged by topic, the datasets mentioned in the lecture slides (see slide 6), or else look at some of these publically accessible databases:\n\nEU data\nICPSR\nThe World Bank"
  },
  {
    "objectID": "project_gsu.html#model-requirements",
    "href": "project_gsu.html#model-requirements",
    "title": "Bayesian Data Analysis course - Project work (GSU)",
    "section": "Model requirements",
    "text": "Model requirements\n\nEvery parameter needs to have an explicit proper prior. Improper flat priors are not allowed.\nA hierarchical model is a model where the prior of certain parameter contain other parameters that are also estimated in the model. For instance, b ~ normal(mu, sigma), mu ~ normal(0, 1), sigma ~ exponential(1).\nDo not impose hard constrains on a parameter unless they are natural to them. uniform(a, b) should not be used unless the boundaries are really logical boundaries and values beyond the boundaries are completely impossible.\nAt least some models should include covariates. Modelling the outcome without predictors is likely too simple for the project.\nbrms and rstanarm can be used, but you need to report the priors used (including reporting the priors brms and rstanamr assign by default)."
  },
  {
    "objectID": "project_gsu.html#some-examples",
    "href": "project_gsu.html#some-examples",
    "title": "Bayesian Data Analysis course - Project work (GSU)",
    "section": "Some examples",
    "text": "Some examples\nThe following case study examples demonstrate how text, equations, figures, and code, and inference results can be included in one report. These examples don’t necessarily have all the workflow steps required in your report, but different steps are illustrated in different case studies and you can get good ideas for your report just by browsing through them.\n\nBDA R and Python demos are quite minimal in description of the data and discussion of the results, but show many diagnostics and basic plots.\nSome Stan case studies focus on some specific methods, but there are many case studies that are excellent examples for this course. They don’t include all the steps required in this course, but are good examples of writing. Some of them are longer or use more advanced models than required in this course.\n\nBayesian workflow for disease transmission modeling in Stan\nModel-based Inference for Causal Effects in Completely Randomized Experiments\nTagging Basketball Events with HMM in Stan\nModel building and expansion for golf putting\nA Dyadic Item Response Theory Model\nPredator-Prey Population Dynamics: the Lotka-Volterra model in Stan\nHierarchical model for motivational shifts in aging monkeys\n\nSome StanCon case studies (scroll down) can also provide good project ideas."
  }
]