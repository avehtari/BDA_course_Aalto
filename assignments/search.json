[
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "The exercises here refer to the lecture 3/BDA chapters 2-3 content.\nThe exercises constitute 90% of the Quiz 3 grade.\nWe prepared a quarto notebook specific to this assignment to help you get started. You still need to fill in your answers on Mycourses! You can inspect this and future templates\n\nas a rendered html file (to access the qmd file click the “&lt;/&gt; Code” button at the top right hand corner of the template)\n\n\n\n\n\n\n\nGeneral Instructions for Answering the Assignment Questions\n\n\n\n\n\n\nQuestions below are exact copies of the text found in the MyCourses quiz and should serve as a notebook where you can store notes and code.\nWe recommend opening these notebooks in the Aalto JupyterHub, see how to use R and RStudio remotely.\nFor inspiration for code, have a look at the BDA R Demos and the specific Assignment code notebooks\nRecommended additional self study exercises for each chapter in BDA3 are listed in the course web page. These will help to gain deeper understanding of the topic.\nCommon questions and answers regarding installation and technical problems can be found in Frequently Asked Questions (FAQ).\nDeadlines for all assignments can be found on the course web page and in MyCourses.\nYou are allowed to discuss assignments with your friends, but it is not allowed to copy solutions directly from other students or from internet.\nDo not share your answers publicly.\nDo not copy answers from the internet or from previous years. We compare the answers to the answers from previous years and to the answers from other students this year.\nUse of AI is allowed on the course, but the most of the work needs to by the student, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching).\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof.\nIf you have any suggestions or improvements to the course material, please post in the course chat feedback channel, create an issue, or submit a pull request to the public repository!\n\n\n\n\n\n\nFor convenience the assignment questions are copied below. Answer the questions in MyCourses.\n\nA factory has a production line for manufacturing car windshields. A sample of windshields has been taken for testing hardness. The observed hardness values \\( y_1 \\) can be found in the dataset windshieldy1 in the aaltobda package.We may assume that the observations follow a normal distribution with an unknown standard deviation \\( \\sigma \\). We wish to obtain information about the unknown average hardness \\( \\mu \\). For simplicity we assume standard uninformative prior discussed in the book, that is, \\( p(\\mu, \\sigma^2) \\propto (\\sigma^2)^{-1} \\). It is not necessary to derive the posterior distribution in this quiz, as it has already been done in the book (see section 3.2). As in the lecture, define \\(n\\) as the number of observations, \\(\\bar{y} \\) as the arithmatic mean estimate and \\(s^2 = \\frac{1}{n-1}\\sum^n_{i=1} (y_i-\\bar{y})^2\\).1.1 The likelihood \\( p(y|\\mu,\\sigma) \\) can be expressed as: 1.2 The resulting joint posterior for \\(\\mu\\) and \\(\\sigma^2\\) may be expressed as: 1.3 The resulting marginal posterior for \\(\\mu \\) may be expressed as: 1.4 The resulting marginal posterior for \\(\\sigma^2 \\) may be expressed as: What can you say about the unknown \\( \\mu \\)?1.5 Compute and report the point estimate \\( E(\\mu|y) \\) (report your answer with 3 decimal digits): 1.6 Compute and report the central 95% posterior interval (report your answer with 3 decimal digits). R does not have a built in quantile function for t-distributions with non-zero mean and scale different from 1. The code template gives instructions how you can make your own function:  NB: Posterior intervals are also called credible intervals and are different from confidence intervals. 1.7 Using the code in the template, what can you say about the joint posterior of \\(\\mu,\\sigma\\)? What can you say about the hardness of the next windshield coming from the production line before actually measuring the hardness?1.7 Compute and report the point estimate \\(E(\\tilde{y}|y)\\)(report your answer with 3 decimal digits) :  1.8 Compute and report a posterior predictive 95%-interval (report your answer with 3 decimal digits):  NB: Posterior predictive intervals are different from posterior intervals of parameters of the model.\n\n\nAn experiment was performed to estimate the effect of beta-blockers on mortality of cardiac patients. A group of patients was randomly assigned to treatment and control groups: out of 674 patients receiving the control, 39 died, and out of 680 receiving the treatment, 22 died. Assume that the outcomes are independent and binomially distributed, with probabilities of death of \\( p_0 \\) and \\( p_1 \\) under the control and treatment, respectively. Set up a noninformative or weakly informative prior distribution on \\( (p_0,p_1) \\). In the below, n refers to the number of trials, y to the number of successes and \\(\\theta\\) to the probability within a binomial model.Formulate model below.2.1 Take \\(y_C\\) as the number of deaths in the control group and \\(y_T\\) as the number of deaths in the treatment group. The data model can be written as: 2.2 In the context of the data and model, the prior:  2.3 The resulting posterior with independent \\(Beta(1,1) \\) priors for \\(p_1\\) and \\(p_2\\) can be expressed as: Using the \\(Beta(1,1)\\) prior for \\(p_0\\) and \\(p_1\\) independently, summarize the posterior distribution for the odds ratio, \\( \\mathrm{OR} = (p_1/(1-p_1))/(p_0/(1-p_0)) \\). With this (and any Beta prior), a the posteriors for \\((p_0,p_1)\\) are also Beta distributions, respectively (see equations in the book). First, use rbeta() to obtain posterior draws for the posterior distributions of \\(p_0\\) and \\(p_1\\). Then use these draws and odds ratio equation above to get the posterior of the odds ratio. This is called a push-forward distribution.  Obtain at least 1000 draws. If you are unsure how to get started, check out the code template.2.4 Compute a point estimate for \\(E(\\mathrm{OR}|y_0,y_1)\\).Report the result in decimals with two decimal digits:   2.5 Compute an estimate for the posterior 95% central interval. Report the result in decimal with three decimal digits:  2.6 Now use what might be deemed a weakly informative \\(Beta(2,2)\\) prior on both probabilities \\(p_0\\) and \\(p_1\\) and discuss the sensitivity of your inference to your choice of prior density: \n2.7 Use Frank Harrell's recommendations on how to state results in Bayesian two group comparison: \n\n\nConsider a case where the same factory has two production lines for manufacturing car windshields. Independent samples from the two production lines were tested for hardness. The hardness measurements for the two samples \\( \\mathbf{y}_1 \\) and \\( \\mathbf{y}_2 \\) be found in  the datasets windshieldy1 and windshieldy2 in the aaltobda package and in this exercise we will model the measurements with two independent normal distributions respectively.For the model, we assume that standard deviations \\(\\sigma_1\\) and \\(\\sigma_2\\) of the normal models are unknown. Let \\(\\bar{y_1}\\) and \\(\\bar{y_2}\\) denote averages for  \\( \\mathbf{y}_1 \\) and \\( \\mathbf{y}_2 \\), respectively and \\(s_1^2\\), \\(s_2^2\\) denote corresponding sample standard deviations for  \\( \\mathbf{y}_1 \\) and \\( \\mathbf{y}_2 \\), computed in a same manner as in exercise 1. Also, \\(n\\) denotes number of samples in each dataset(same number for both). Use the uninformative prior and answer the following questions:3.1 The data model may be expressed as: 3.2 The prior can be expressed as: \n3.3 The resulting marginal posterior for \\(\\mu_1\\) can be expressed as: \n3.4 The resulting joint posterior for \\((\\mu_1,{\\sigma_1}^2)\\) can be expressed as: \n3.5 The resulting joint posterior for \\((\\mu_2,{\\sigma_2}^2)\\) can be expressed as: \nWhat can you say about \\( \\mu_d = \\mu_1 - \\mu_2 \\)?You can use the rtnew() function to sample from the posterior distributions of \\(\\mu_1\\) and \\(\\mu_2\\), and use these samples to get a sample from the distribution of the difference \\(\\mu_d = \\mu_1 - \\mu_2\\). Draw at least 10000 samples.3.6 Compute the point estimate \\( E(\\mu_d|y_1, y_2) \\). Report the result in decimals with three decimal digits: \n3.7 Compute a posterior 95%-interval. Report the result in decimals with three decimal digits: 3.8 Given this specific model, what is the probability that the means are exactly the same (\\(\\mu_1 = \\mu_2\\)) (Use Frank Harrell's recommendations)? Explain your reasoning. \n3.9 Compute the probability that \\(\\mu_1 &lt; \\mu_2\\) (same as the probability that \\(\\mu_1 - \\mu_2 &lt; 0\\)). Report the result in decimals with two decimal digits:",
    "crumbs": [
      "Assignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignment3.html#assignment-questions",
    "href": "assignment3.html#assignment-questions",
    "title": "Assignment 3",
    "section": "",
    "text": "For convenience the assignment questions are copied below. Answer the questions in MyCourses.\n\nA factory has a production line for manufacturing car windshields. A sample of windshields has been taken for testing hardness. The observed hardness values \\( y_1 \\) can be found in the dataset windshieldy1 in the aaltobda package.We may assume that the observations follow a normal distribution with an unknown standard deviation \\( \\sigma \\). We wish to obtain information about the unknown average hardness \\( \\mu \\). For simplicity we assume standard uninformative prior discussed in the book, that is, \\( p(\\mu, \\sigma^2) \\propto (\\sigma^2)^{-1} \\). It is not necessary to derive the posterior distribution in this quiz, as it has already been done in the book (see section 3.2). As in the lecture, define \\(n\\) as the number of observations, \\(\\bar{y} \\) as the arithmatic mean estimate and \\(s^2 = \\frac{1}{n-1}\\sum^n_{i=1} (y_i-\\bar{y})^2\\).1.1 The likelihood \\( p(y|\\mu,\\sigma) \\) can be expressed as: 1.2 The resulting joint posterior for \\(\\mu\\) and \\(\\sigma^2\\) may be expressed as: 1.3 The resulting marginal posterior for \\(\\mu \\) may be expressed as: 1.4 The resulting marginal posterior for \\(\\sigma^2 \\) may be expressed as: What can you say about the unknown \\( \\mu \\)?1.5 Compute and report the point estimate \\( E(\\mu|y) \\) (report your answer with 3 decimal digits): 1.6 Compute and report the central 95% posterior interval (report your answer with 3 decimal digits). R does not have a built in quantile function for t-distributions with non-zero mean and scale different from 1. The code template gives instructions how you can make your own function:  NB: Posterior intervals are also called credible intervals and are different from confidence intervals. 1.7 Using the code in the template, what can you say about the joint posterior of \\(\\mu,\\sigma\\)? What can you say about the hardness of the next windshield coming from the production line before actually measuring the hardness?1.7 Compute and report the point estimate \\(E(\\tilde{y}|y)\\)(report your answer with 3 decimal digits) :  1.8 Compute and report a posterior predictive 95%-interval (report your answer with 3 decimal digits):  NB: Posterior predictive intervals are different from posterior intervals of parameters of the model.\n\n\nAn experiment was performed to estimate the effect of beta-blockers on mortality of cardiac patients. A group of patients was randomly assigned to treatment and control groups: out of 674 patients receiving the control, 39 died, and out of 680 receiving the treatment, 22 died. Assume that the outcomes are independent and binomially distributed, with probabilities of death of \\( p_0 \\) and \\( p_1 \\) under the control and treatment, respectively. Set up a noninformative or weakly informative prior distribution on \\( (p_0,p_1) \\). In the below, n refers to the number of trials, y to the number of successes and \\(\\theta\\) to the probability within a binomial model.Formulate model below.2.1 Take \\(y_C\\) as the number of deaths in the control group and \\(y_T\\) as the number of deaths in the treatment group. The data model can be written as: 2.2 In the context of the data and model, the prior:  2.3 The resulting posterior with independent \\(Beta(1,1) \\) priors for \\(p_1\\) and \\(p_2\\) can be expressed as: Using the \\(Beta(1,1)\\) prior for \\(p_0\\) and \\(p_1\\) independently, summarize the posterior distribution for the odds ratio, \\( \\mathrm{OR} = (p_1/(1-p_1))/(p_0/(1-p_0)) \\). With this (and any Beta prior), a the posteriors for \\((p_0,p_1)\\) are also Beta distributions, respectively (see equations in the book). First, use rbeta() to obtain posterior draws for the posterior distributions of \\(p_0\\) and \\(p_1\\). Then use these draws and odds ratio equation above to get the posterior of the odds ratio. This is called a push-forward distribution.  Obtain at least 1000 draws. If you are unsure how to get started, check out the code template.2.4 Compute a point estimate for \\(E(\\mathrm{OR}|y_0,y_1)\\).Report the result in decimals with two decimal digits:   2.5 Compute an estimate for the posterior 95% central interval. Report the result in decimal with three decimal digits:  2.6 Now use what might be deemed a weakly informative \\(Beta(2,2)\\) prior on both probabilities \\(p_0\\) and \\(p_1\\) and discuss the sensitivity of your inference to your choice of prior density: \n2.7 Use Frank Harrell's recommendations on how to state results in Bayesian two group comparison: \n\n\nConsider a case where the same factory has two production lines for manufacturing car windshields. Independent samples from the two production lines were tested for hardness. The hardness measurements for the two samples \\( \\mathbf{y}_1 \\) and \\( \\mathbf{y}_2 \\) be found in  the datasets windshieldy1 and windshieldy2 in the aaltobda package and in this exercise we will model the measurements with two independent normal distributions respectively.For the model, we assume that standard deviations \\(\\sigma_1\\) and \\(\\sigma_2\\) of the normal models are unknown. Let \\(\\bar{y_1}\\) and \\(\\bar{y_2}\\) denote averages for  \\( \\mathbf{y}_1 \\) and \\( \\mathbf{y}_2 \\), respectively and \\(s_1^2\\), \\(s_2^2\\) denote corresponding sample standard deviations for  \\( \\mathbf{y}_1 \\) and \\( \\mathbf{y}_2 \\), computed in a same manner as in exercise 1. Also, \\(n\\) denotes number of samples in each dataset(same number for both). Use the uninformative prior and answer the following questions:3.1 The data model may be expressed as: 3.2 The prior can be expressed as: \n3.3 The resulting marginal posterior for \\(\\mu_1\\) can be expressed as: \n3.4 The resulting joint posterior for \\((\\mu_1,{\\sigma_1}^2)\\) can be expressed as: \n3.5 The resulting joint posterior for \\((\\mu_2,{\\sigma_2}^2)\\) can be expressed as: \nWhat can you say about \\( \\mu_d = \\mu_1 - \\mu_2 \\)?You can use the rtnew() function to sample from the posterior distributions of \\(\\mu_1\\) and \\(\\mu_2\\), and use these samples to get a sample from the distribution of the difference \\(\\mu_d = \\mu_1 - \\mu_2\\). Draw at least 10000 samples.3.6 Compute the point estimate \\( E(\\mu_d|y_1, y_2) \\). Report the result in decimals with three decimal digits: \n3.7 Compute a posterior 95%-interval. Report the result in decimals with three decimal digits: 3.8 Given this specific model, what is the probability that the means are exactly the same (\\(\\mu_1 = \\mu_2\\)) (Use Frank Harrell's recommendations)? Explain your reasoning. \n3.9 Compute the probability that \\(\\mu_1 &lt; \\mu_2\\) (same as the probability that \\(\\mu_1 - \\mu_2 &lt; 0\\)). Report the result in decimals with two decimal digits:",
    "crumbs": [
      "Assignments",
      "Assignment 3"
    ]
  },
  {
    "objectID": "assignment9.html",
    "href": "assignment9.html",
    "title": "Assignment 9",
    "section": "",
    "text": "1 General information\nThis assignment will be published later!",
    "crumbs": [
      "Assignments",
      "Assignment 9"
    ]
  },
  {
    "objectID": "template7.html",
    "href": "template7.html",
    "title": "Notebook for Assignment 7",
    "section": "",
    "text": "1 General information\nThis notebook will be published later!",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 7"
    ]
  },
  {
    "objectID": "assignment8.html",
    "href": "assignment8.html",
    "title": "Assignment 8",
    "section": "",
    "text": "1 General information\nThis assignment will be published later!",
    "crumbs": [
      "Assignments",
      "Assignment 8"
    ]
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "The exercises here refer to the lecture 4/BDA chapters 3 and 10 content. The main topics for this assignment are the MCSE and importance sampling.\nThe exercises constitute 96% of the Quiz 4 grade.\nWe prepared a quarto notebook specific to this assignment to help you get started. You still need to fill in your answers on Mycourses! You can inspect this and future templates\n\nas a rendered html file (to access the qmd file click the “&lt;/&gt; Code” button at the top right hand corner of the template)\n\n\n\n\n\n\n\nGeneral Instructions for Answering the Assignment Questions\n\n\n\n\n\n\nQuestions below are exact copies of the text found in the MyCourses quiz and should serve as a notebook where you can store notes and code.\nWe recommend opening these notebooks in the Aalto JupyterHub, see how to use R and RStudio remotely.\nFor inspiration for code, have a look at the BDA R Demos and the specific Assignment code notebooks\nRecommended additional self study exercises for each chapter in BDA3 are listed in the course web page. These will help to gain deeper understanding of the topic.\nCommon questions and answers regarding installation and technical problems can be found in Frequently Asked Questions (FAQ).\nDeadlines for all assignments can be found on the course web page and in MyCourses.\nYou are allowed to discuss assignments with your friends, but it is not allowed to copy solutions directly from other students or from internet.\nDo not share your answers publicly.\nDo not copy answers from the internet or from previous years. We compare the answers to the answers from previous years and to the answers from other students this year.\nUse of AI is allowed on the course, but the most of the work needs to by the student, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching).\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof.\nIf you have any suggestions or improvements to the course material, please post in the course chat feedback channel, create an issue, or submit a pull request to the public repository!\n\n\n\n\n\n\nFor convenience the assignment questions are copied below. Answer the questions in MyCourses.\n\n\n    \n    \n\n\n\n\n    \n\n\n    \n    \n        \n            \n                \n                \n            \n        \n        \n            \n            In this exercise, you will use a dose-response relation model that is used in BDA3 Section 3.7 and in the chapter reading notes.The likelihood is the same as in the book, but instead of uniform priors, we will use a bivariate normal distribution as the joint prior distribution of the parameters \\(\\alpha\\) and \\(\\beta\\).\n            In the prior distribution for \\((\\alpha,\\beta)\\), the marginal distributions are \\(\\alpha \\sim N(0,2^2)\\) and \\(\\beta \\sim N(10,10^2)\\), and the correlation between them is \\(\\mathrm{corr}(\\alpha, \\beta)=0.6\\).\n\n            2.1 The mean of the prior distribution for \\((\\alpha, \\beta)\\) is: \n            2.2 The covariance of the prior distribution (two by two matrix) is: \n            \n        \n        \n            \n        \n        For this section, you will estimate the posterior mean by weighting prior draws with the appropriate importance weights. To do this you will need to calculate the unnormalized importance ratios \\(\\tilde{w}\\) (also known as weights) when the importance sampling target distribution \\(q(\\theta)\\) is the posterior distribution, and the proposal distribution \\(g(\\theta)\\) is the prior distribution as defined above. You will then need to use this function to calculate the importance sampling estimate of the posterior mean, given draws from the prior.\n        \n        \n            3.1 What is the general formula for the importance ratios (also called importance weights)?\n            \n            \n            \n            3.2 What is the correct formula for the importance ratios in the case where the prior is the proposal and the posterior is the target?\n            \n                \n                \n            \n            3.3 What is the correct formula for the unnormalized importance ratios in the case where the prior is the proposal and the posterior is the target?\n            \n                \n                \n                    \n                \n            \n        \n        3.4 The unnormalized log importance ratios in the case where the prior is the proposal and the posterior is the target simplify to:\n        \n        \n        \n        3.5 Why is it better to compute the importance rations on the log scale?\n        \n        \n        \n        The function `bioassaylp` from the aaltobda package calculates the unnormalized log posterior density assuming a uniform prior.\n        \n        3.6 What is the relationship between the posterior, likelihood and prior when the prior is uniform?\n        \n        \n        \n            3.7 What is the correct formula or formulas for the self-normalized importance sampling estimate of the mean? \\(w\\) are the unnormalized importance weights, and \\(\\tilde{w}\\) are the self-normalized weights.\n            \n            \n            \n            Use the function rmvnorm to draw 4000 draws from the prior specified in question 2, calculate the importance weights and then the importance sampling estimate of the posterior mean.\n            \n            3.8 The importance sampling estimate of the posterior mean is: \n            \n            alpha: , beta: .\n            \n            3.9 What is the equation for the generic effective sample size (ESS) estimate for importance sampling?\n            \n            \n                \n                3.10 Importance sampling ESS: \n                \n                3.11 What is the MCSE of the estimates? Make sure to use the ESS in the denominator instead of the sample size.\n            \n            \n            \n                alpha: , beta: .\n                \n                3.12 What is the Pareto-k diagnostic of the importance ratios (enter value with one decimal)? .\n                \n                3.13 Based on the value, should you trust the importance sampling estimate of the mean?\n            \n            \n            \n                \n                    \n                \n            \n            \n                \n                    You are given 4000 independent draws from the posterior distributionof the model in the dataset `bioassay_posterior` in the `aaltobda` package.\n                    4.1 The posterior can be summarised by the means and 90% posterior interval.What is the mean, 5% and 95% quantiles of the posterior draws for the parameters?mean of alpha: , 5% quantile of alpha: , 95% quantile of alpha: \n                    mean of beta: , 5% quantile of beta: , 95% quantile of beta: \n                \n                \n                4.2 What is the MCSE for the mean and quantiles (remember that they are independent draws)?For alpha: MCSE for mean: , MCSE for 5% quantile: , MCSE for 95% quantile: \n                \n                For beta: MCSE for mean: , MCSE for 5% quantile: , MCSE for 95% quantile: \n            \n            \n            Compare these posterior summaries to the importance sampling estimate.\n            \n            4.3 How does the importance sampling estimate of the posterior mean compare to the estimate from independent posterior draws?\n            \n            \n            Remember that the LD50 (median lethal dose) is equal to \\( exp( -\\alpha / \\beta) \\). \n            Calculate the posterior probability that the LD50 is less than 0.85ml/g. Also calculate the MCSE for this probability.\n            \n            4.4 Pr(LD50 &lt; 0.85 ml/g) = , MCSE =",
    "crumbs": [
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "assignment4.html#assignment-questions",
    "href": "assignment4.html#assignment-questions",
    "title": "Assignment 4",
    "section": "",
    "text": "For convenience the assignment questions are copied below. Answer the questions in MyCourses.\n\n\n    \n    \n\n\n\n\n    \n\n\n    \n    \n        \n            \n                \n                \n            \n        \n        \n            \n            In this exercise, you will use a dose-response relation model that is used in BDA3 Section 3.7 and in the chapter reading notes.The likelihood is the same as in the book, but instead of uniform priors, we will use a bivariate normal distribution as the joint prior distribution of the parameters \\(\\alpha\\) and \\(\\beta\\).\n            In the prior distribution for \\((\\alpha,\\beta)\\), the marginal distributions are \\(\\alpha \\sim N(0,2^2)\\) and \\(\\beta \\sim N(10,10^2)\\), and the correlation between them is \\(\\mathrm{corr}(\\alpha, \\beta)=0.6\\).\n\n            2.1 The mean of the prior distribution for \\((\\alpha, \\beta)\\) is: \n            2.2 The covariance of the prior distribution (two by two matrix) is: \n            \n        \n        \n            \n        \n        For this section, you will estimate the posterior mean by weighting prior draws with the appropriate importance weights. To do this you will need to calculate the unnormalized importance ratios \\(\\tilde{w}\\) (also known as weights) when the importance sampling target distribution \\(q(\\theta)\\) is the posterior distribution, and the proposal distribution \\(g(\\theta)\\) is the prior distribution as defined above. You will then need to use this function to calculate the importance sampling estimate of the posterior mean, given draws from the prior.\n        \n        \n            3.1 What is the general formula for the importance ratios (also called importance weights)?\n            \n            \n            \n            3.2 What is the correct formula for the importance ratios in the case where the prior is the proposal and the posterior is the target?\n            \n                \n                \n            \n            3.3 What is the correct formula for the unnormalized importance ratios in the case where the prior is the proposal and the posterior is the target?\n            \n                \n                \n                    \n                \n            \n        \n        3.4 The unnormalized log importance ratios in the case where the prior is the proposal and the posterior is the target simplify to:\n        \n        \n        \n        3.5 Why is it better to compute the importance rations on the log scale?\n        \n        \n        \n        The function `bioassaylp` from the aaltobda package calculates the unnormalized log posterior density assuming a uniform prior.\n        \n        3.6 What is the relationship between the posterior, likelihood and prior when the prior is uniform?\n        \n        \n        \n            3.7 What is the correct formula or formulas for the self-normalized importance sampling estimate of the mean? \\(w\\) are the unnormalized importance weights, and \\(\\tilde{w}\\) are the self-normalized weights.\n            \n            \n            \n            Use the function rmvnorm to draw 4000 draws from the prior specified in question 2, calculate the importance weights and then the importance sampling estimate of the posterior mean.\n            \n            3.8 The importance sampling estimate of the posterior mean is: \n            \n            alpha: , beta: .\n            \n            3.9 What is the equation for the generic effective sample size (ESS) estimate for importance sampling?\n            \n            \n                \n                3.10 Importance sampling ESS: \n                \n                3.11 What is the MCSE of the estimates? Make sure to use the ESS in the denominator instead of the sample size.\n            \n            \n            \n                alpha: , beta: .\n                \n                3.12 What is the Pareto-k diagnostic of the importance ratios (enter value with one decimal)? .\n                \n                3.13 Based on the value, should you trust the importance sampling estimate of the mean?\n            \n            \n            \n                \n                    \n                \n            \n            \n                \n                    You are given 4000 independent draws from the posterior distributionof the model in the dataset `bioassay_posterior` in the `aaltobda` package.\n                    4.1 The posterior can be summarised by the means and 90% posterior interval.What is the mean, 5% and 95% quantiles of the posterior draws for the parameters?mean of alpha: , 5% quantile of alpha: , 95% quantile of alpha: \n                    mean of beta: , 5% quantile of beta: , 95% quantile of beta: \n                \n                \n                4.2 What is the MCSE for the mean and quantiles (remember that they are independent draws)?For alpha: MCSE for mean: , MCSE for 5% quantile: , MCSE for 95% quantile: \n                \n                For beta: MCSE for mean: , MCSE for 5% quantile: , MCSE for 95% quantile: \n            \n            \n            Compare these posterior summaries to the importance sampling estimate.\n            \n            4.3 How does the importance sampling estimate of the posterior mean compare to the estimate from independent posterior draws?\n            \n            \n            Remember that the LD50 (median lethal dose) is equal to \\( exp( -\\alpha / \\beta) \\). \n            Calculate the posterior probability that the LD50 is less than 0.85ml/g. Also calculate the MCSE for this probability.\n            \n            4.4 Pr(LD50 &lt; 0.85 ml/g) = , MCSE =",
    "crumbs": [
      "Assignments",
      "Assignment 4"
    ]
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "The exercises here refer to the lecture 2/BDA chapters 1-2 content. All questions check your understanding of a simple posterior analysis using a binomial model for the observations and a beta prior.\nThe exercises constitute 90% of the Quiz 2 grade.\nWe prepared a quarto notebook specific to this assignment to help you get started. You still need to fill in your answers on Mycourses! You can inspect this and future templates\n\nas a qmd file,\nas a rendered html file\n\n\n\n\n\n\n\nGeneral Instructions for Answering the Assignment Questions\n\n\n\n\n\n\nQuestions below are exact copies of the text found in the MyCourses quiz and should serve as a notebook where you can store notes and code.\nWe recommend opening these notebooks in the Aalto JupyterHub, see how to use R and RStudio remotely.\nFor inspiration for code, have a look at the BDA R Demos and the specific Assignment code notebooks\nRecommended additional self study exercises for each chapter in BDA3 are listed in the course web page. These will help to gain deeper understanding of the topic.\nCommon questions and answers regarding installation and technical problems can be found in Frequently Asked Questions (FAQ).\nDeadlines for all assignments can be found on the course web page and in MyCourses.\nYou are allowed to discuss assignments with your friends, but it is not allowed to copy solutions directly from other students or from internet.\nDo not share your answers publicly.\nDo not copy answers from the internet or from previous years. We compare the answers to the answers from previous years and to the answers from other students this year.\nUse of AI is allowed on the course, but the most of the work needs to by the student, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching).\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof.\nIf you have any suggestions or improvements to the course material, please post in the course chat feedback channel, create an issue, or submit a pull request to the public repository!\n\n\n\n\n\n\nFor convenience the assignment questions are copied below. Answer the questions in MyCourses.\n\n\n\nAlgae\n    status is monitored in 274 sites at Finnish lakes and rivers. The\n    observations for the 2008 algae status at each site are presented in the\n    dataset algae in the aaltobda package ('0': no algae, '1': algae\n    present).\nLet \\( \\theta \\) be the probability of a monitoring site having\n    detectable blue-green algae levels, \\( y \\) the number of observed sites with algae detected, and \\( n \\) be total number sites surveyed.\n    Use a binomial model for the observations and a \\( \\text{Beta}(2,10) \\) prior for binomial model parameter \\( \\theta \\) to formulate a Bayesian model.\n    Here we will not test you on the individual mathematical operations needed in order to derive the posterior distribution for \\( \\theta \\) as it has already been done in the book (and lecture) so make sure to look that up.\nYour task is to perform Bayesian\n    inference for a binomial model and fill in the quiz below based on it. \nFor questions with checkboxes, more than one answer may be\n    correct.\n\nThe algae dataset contains the results of 274 measurements from Finnish lakes, with the following results:\n\n    No Algae: 230 sites\n    Algae: 44 sites\n\nOur goal for the following set of questions is to find the formulation of the posterior using a binomial likeliood and a beta prior on the unknown probability parameter \\( \\theta \\)\n\n\n    1.1 The prior \\(p(\\theta)\\) can be expressed as: \n\n\n    1.2 The likelihood \\( p(y = 44 | \\theta, n = 274) \\) as a function of \\( \\theta \\) can be expressed as: \n\n\n\n\n    1.3 The resulting posterior \\( p(\\theta|y = 44, n = 274) \\) can be expressed as : \n\n\n\n\nThe posterior distribution \\( p(\\theta|y) \\) is analytically available as \\( \\text{Beta}(\\alpha, \\beta) \\), so we can use the properties of that distribution to summarise what we know about \\( \\theta \\).  And in particular, we can make probability statements about ranges of values for \\( \\theta \\). Let's however start with the average value of \\( \\theta \\) you expect after having conditioned on the data. \n\n    2.1 Which of the following is the correct formula for the mean (\\( E[\\cdot] \\)) of a \\( \\text{Beta}(\\alpha, \\beta) \\) distribution: \n\n\n    2.2 Using your answer above, what is the mean of our posterior (i.e.,  \\( E(\\theta|y) \\))? Report the result in decimals with two decimal digits. \n\n\nPosterior intervals are sometimes called credible intervals and are different from confidence intervals (for more on this, see here). These are computed using the quantile function of the posterior distribution. As the quantiles of a \\( \\text{Beta}(\\alpha, \\beta) \\) distribution do not have a simple analytical form like the expectation, you can use R to compute the posterior intervals.\n\n\n    2.3 What R function would you use here to compute posterior intervals? \n\nUsing your answer above, calculate (report the results in decimals with two decimal digits): \n\n\n\n    2.4 90% posterior interval lower bound: \n\n\n    2.5 90% posterior interval upper bound: \n\nWe are interested in using our posterior distribution to estimate the probability that the proportion of detected algae samples (\\( \\theta \\)) is smaller than the historical detection rate \\( \\theta_0 = 0.2 \\), i.e. \\( p(\\theta \\leq \\theta_0 \\mid y) \\). \n\n    3.1 Which of the following approaches would we take? \n\n\n    3.2 What statistical function computes this probability for us? \n\n\n    3.3 Which R function does this for you? \n\n\n    3.4 Using your answers above, report this probability (report the result in decimals with two decimal digits): \n\n\n\nRedo the analysis using a uniform prior, \\( \\text{Beta}( 1,1 \\)). \n\n    4.1 What is the mean of our posterior (i.e.,  \\( E(\\theta|y) \\))? Report the result in decimals with two decimal digits. \n    4.2 90% posterior interval lower bound. Report the result in decimals with two decimal digits. \n    4.3 90% posterior interval upper bound. Report the result in decimals with two decimal digits. \n    4.4 Probability \\( p(\\theta \\leq \\theta_0 \\mid y) \\). Report the result in decimals with two decimal digits. \n\nRedo the analysis using as prior \\( \\text{Beta}(0.5,0.5) \\).\n\n\n    4.5 What is the mean of our posterior (i.e.,  \\( E(\\theta|y) \\))? Report the result in decimals with two decimal digits. \n    4.6 90% posterior interval lower bound. Report the result in decimals with two decimal digits. \n    4.7 90% posterior interval upper bound. Report the result in decimals with two decimal digits. \n    4.8 Probability \\( p(\\theta \\leq \\theta_0 \\mid y) \\). Report the result in decimals with two decimal digits. \n\n\nRedo the analysis using as prior \\( \\text{Beta}(100,2) \\).\n\n\n    4.9 What is the mean of our posterior (i.e.,  \\( E(\\theta|y) \\))? Report the result in decimals with two decimal digits. \n    4.10 90% posterior interval lower bound. Report the result in decimals with two decimal digits. \n    4.11 90% posterior interval upper bound. Report the result in decimals with two decimal digits. \n    4.12 Probability \\( p(\\theta \\leq \\theta_0 \\mid y) \\). Report the result in decimals with two decimal digits. \n\n4.13 Based on testing different priors, would you consider the posterior results believe and defensible (w.r.t. to this data set). In order to help your reasoning you can plot the prior and posteriors used with the code template for Assignment 2?",
    "crumbs": [
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "assignment2.html#assignment-questions",
    "href": "assignment2.html#assignment-questions",
    "title": "Assignment 2",
    "section": "",
    "text": "For convenience the assignment questions are copied below. Answer the questions in MyCourses.\n\n\n\nAlgae\n    status is monitored in 274 sites at Finnish lakes and rivers. The\n    observations for the 2008 algae status at each site are presented in the\n    dataset algae in the aaltobda package ('0': no algae, '1': algae\n    present).\nLet \\( \\theta \\) be the probability of a monitoring site having\n    detectable blue-green algae levels, \\( y \\) the number of observed sites with algae detected, and \\( n \\) be total number sites surveyed.\n    Use a binomial model for the observations and a \\( \\text{Beta}(2,10) \\) prior for binomial model parameter \\( \\theta \\) to formulate a Bayesian model.\n    Here we will not test you on the individual mathematical operations needed in order to derive the posterior distribution for \\( \\theta \\) as it has already been done in the book (and lecture) so make sure to look that up.\nYour task is to perform Bayesian\n    inference for a binomial model and fill in the quiz below based on it. \nFor questions with checkboxes, more than one answer may be\n    correct.\n\nThe algae dataset contains the results of 274 measurements from Finnish lakes, with the following results:\n\n    No Algae: 230 sites\n    Algae: 44 sites\n\nOur goal for the following set of questions is to find the formulation of the posterior using a binomial likeliood and a beta prior on the unknown probability parameter \\( \\theta \\)\n\n\n    1.1 The prior \\(p(\\theta)\\) can be expressed as: \n\n\n    1.2 The likelihood \\( p(y = 44 | \\theta, n = 274) \\) as a function of \\( \\theta \\) can be expressed as: \n\n\n\n\n    1.3 The resulting posterior \\( p(\\theta|y = 44, n = 274) \\) can be expressed as : \n\n\n\n\nThe posterior distribution \\( p(\\theta|y) \\) is analytically available as \\( \\text{Beta}(\\alpha, \\beta) \\), so we can use the properties of that distribution to summarise what we know about \\( \\theta \\).  And in particular, we can make probability statements about ranges of values for \\( \\theta \\). Let's however start with the average value of \\( \\theta \\) you expect after having conditioned on the data. \n\n    2.1 Which of the following is the correct formula for the mean (\\( E[\\cdot] \\)) of a \\( \\text{Beta}(\\alpha, \\beta) \\) distribution: \n\n\n    2.2 Using your answer above, what is the mean of our posterior (i.e.,  \\( E(\\theta|y) \\))? Report the result in decimals with two decimal digits. \n\n\nPosterior intervals are sometimes called credible intervals and are different from confidence intervals (for more on this, see here). These are computed using the quantile function of the posterior distribution. As the quantiles of a \\( \\text{Beta}(\\alpha, \\beta) \\) distribution do not have a simple analytical form like the expectation, you can use R to compute the posterior intervals.\n\n\n    2.3 What R function would you use here to compute posterior intervals? \n\nUsing your answer above, calculate (report the results in decimals with two decimal digits): \n\n\n\n    2.4 90% posterior interval lower bound: \n\n\n    2.5 90% posterior interval upper bound: \n\nWe are interested in using our posterior distribution to estimate the probability that the proportion of detected algae samples (\\( \\theta \\)) is smaller than the historical detection rate \\( \\theta_0 = 0.2 \\), i.e. \\( p(\\theta \\leq \\theta_0 \\mid y) \\). \n\n    3.1 Which of the following approaches would we take? \n\n\n    3.2 What statistical function computes this probability for us? \n\n\n    3.3 Which R function does this for you? \n\n\n    3.4 Using your answers above, report this probability (report the result in decimals with two decimal digits): \n\n\n\nRedo the analysis using a uniform prior, \\( \\text{Beta}( 1,1 \\)). \n\n    4.1 What is the mean of our posterior (i.e.,  \\( E(\\theta|y) \\))? Report the result in decimals with two decimal digits. \n    4.2 90% posterior interval lower bound. Report the result in decimals with two decimal digits. \n    4.3 90% posterior interval upper bound. Report the result in decimals with two decimal digits. \n    4.4 Probability \\( p(\\theta \\leq \\theta_0 \\mid y) \\). Report the result in decimals with two decimal digits. \n\nRedo the analysis using as prior \\( \\text{Beta}(0.5,0.5) \\).\n\n\n    4.5 What is the mean of our posterior (i.e.,  \\( E(\\theta|y) \\))? Report the result in decimals with two decimal digits. \n    4.6 90% posterior interval lower bound. Report the result in decimals with two decimal digits. \n    4.7 90% posterior interval upper bound. Report the result in decimals with two decimal digits. \n    4.8 Probability \\( p(\\theta \\leq \\theta_0 \\mid y) \\). Report the result in decimals with two decimal digits. \n\n\nRedo the analysis using as prior \\( \\text{Beta}(100,2) \\).\n\n\n    4.9 What is the mean of our posterior (i.e.,  \\( E(\\theta|y) \\))? Report the result in decimals with two decimal digits. \n    4.10 90% posterior interval lower bound. Report the result in decimals with two decimal digits. \n    4.11 90% posterior interval upper bound. Report the result in decimals with two decimal digits. \n    4.12 Probability \\( p(\\theta \\leq \\theta_0 \\mid y) \\). Report the result in decimals with two decimal digits. \n\n4.13 Based on testing different priors, would you consider the posterior results believe and defensible (w.r.t. to this data set). In order to help your reasoning you can plot the prior and posteriors used with the code template for Assignment 2?",
    "crumbs": [
      "Assignments",
      "Assignment 2"
    ]
  },
  {
    "objectID": "template9.html",
    "href": "template9.html",
    "title": "Notebook for Assignment 9",
    "section": "",
    "text": "1 General information\nThis notebook will be published later!",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 9"
    ]
  },
  {
    "objectID": "template2.html",
    "href": "template2.html",
    "title": "Notebook for Assignment 2",
    "section": "",
    "text": "1 General information\nThis assignment is related to Lecture 2 and BDA3 Chapters 1 and 2. You may find an additional discussion about choosing priors in a blog post by Andrew Gelman.\n\n\n\n\n\n\nTip\n\n\n\n\n\nReading instructions:\n\nThe reading instructions for BDA3 Chapter 1.\nThe reading instructions for BDA3 Chapter 2.\n\n\n\n\n\n\n\n\n\n\nGeneral Instructions for Answering the Assignment Questions\n\n\n\n\n\n\nQuestions below are exact copies of the text found in the MyCourses quiz and should serve as a notebook where you can store notes and code.\nWe recommend opening these notebooks in the Aalto JupyterHub, see how to use R and RStudio remotely.\nFor inspiration for code, have a look at the BDA R Demos and the specific Assignment code notebooks\nRecommended additional self study exercises for each chapter in BDA3 are listed in the course web page. These will help to gain deeper understanding of the topic.\nCommon questions and answers regarding installation and technical problems can be found in Frequently Asked Questions (FAQ).\nDeadlines for all assignments can be found on the course web page and in MyCourses.\nYou are allowed to discuss assignments with your friends, but it is not allowed to copy solutions directly from other students or from internet.\nDo not share your answers publicly.\nDo not copy answers from the internet or from previous years. We compare the answers to the answers from previous years and to the answers from other students this year.\nUse of AI is allowed on the course, but the most of the work needs to by the student, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching).\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof.\nIf you have any suggestions or improvements to the course material, please post in the course chat feedback channel, create an issue, or submit a pull request to the public repository!\n\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\n\nThe following installs the aaltobda package:\n#| cache: true\n# Caching should be fine here\ninstall.packages(\"aaltobda\", repos = c(\"https://avehtari.github.io/BDA_course_Aalto/\", getOption(\"repos\")))\n\n\n\n\n\n2 Inference for binomial proportion\nAlgae status is monitored in 274 sites at Finnish lakes and rivers. The observations for the 2008 algae status at each site are presented in the dataset algae in the aaltobda package (‘0’: no algae, ‘1’: algae present).\nLoading the library and the data.\n\nlibrary(aaltobda)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(latex2exp)\ndata(\"algae\")\n# The data are now stored in the variable `algae`.\n# These are the values for the prior required in the assignment\nprior_alpha = 2\nprior_beta = 10\n\nLet \\(\\pi\\) be the probability of a monitoring site having detectable blue-green algae levels and \\(y\\) the observations in algae. Use a binomial model for the observations \\(y\\) and a \\(Beta(2,10)\\) prior for binomial model parameter \\(\\pi\\) to formulate a Bayesian model. Here it is not necessary to derive the posterior distribution for \\(\\pi\\) as it has already been done in the book and it suffices to refer to that derivation. Also, it is not necessary to write out the distributions; it is sufficient to use label-parameter format, e.g. \\(Beta(\\alpha,\\beta)\\).\nYour task is to perform Bayesian inference for a binomial model and answer questions based on it.\n\n\n3 Formulating probabilities\n\n\n4 Summary of the posterior distribution of \\(\\theta\\)\nKeep the below name and format for the functions to work with markmyassignment:\n\n# Useful function: qbeta()\n\nbeta_point_est &lt;- function(prior_alpha, prior_beta, data) {\n    # Do computation here, and return as below.\n    \n}\nbeta_interval &lt;- function(prior_alpha, prior_beta, data, prob=0.9) {\n    # Do computation here, and return as below.\n\n\n}\n\n\n\n5 Comparison to historical records\nKeep the below name and format for the function to work with markmyassignment:\n\n# Useful function: pbeta()\n\nbeta_low &lt;- function(prior_alpha, prior_beta, data, pi_0=0.2) {\n    # Do computation here, and return as below.\n\n}\n\n\n\n6 Prior sensitivity analysis\nMake prior sensitivity analysis by testing a couple of different reasonable priors and plot the different posteriors. Try to summarise the results by one or two sentences.\n# Useful function: dbeta()\nAmend the code below to help you.\n\n# Beta distribution density function\nbeta_density &lt;- function(x, shape1, shape2) {\n  dbeta(x, shape1, shape2)\n}\n\n# Binomial likelihood density function\nbinomial_likelihood &lt;- function(x, n, k) {\n  choose(n, k) * x^k * (1 - x)^(n - k)\n}\n\n# Parameters for the Beta prior distributions\npriors &lt;- list(c(#alpha, #beta), c(#alpha, #beta), c(#alpha, #beta)) # Define three different priors here priors, (alpha, beta)\n\n# Parameters for the binomial likelihood (number of trials and successes)\nn &lt;-  # number of trials\nk &lt;-  # number of successes\n\n# Create a sequence of x values from 0 to 1\nx &lt;- seq(0, 1, length.out = 100)\n\n\n# Create a data frame with densities\ndensities_list &lt;- lapply(priors, function(prior) {\n  alpha_prior &lt;- prior[1]\n  beta_prior &lt;- prior[2]\n  alpha_posterior &lt;- alpha_prior + k\n  beta_posterior &lt;- beta_prior + n - k\n  \n  data.frame(\n    x = rep(x, 2),\n    density = c(beta_density(x, alpha_prior, beta_prior),\n                beta_density(x, alpha_posterior, beta_posterior)),\n    distribution = factor(rep(c(\"Prior\", \"Posterior\"), each = length(x))),\n    prior = paste(\"Prior: Alpha =\", alpha_prior, \"Beta =\", beta_prior)\n  )\n})\n\ndf &lt;- bind_rows(densities_list)\n\n# Plot the densities using ggplot2\nggplot(df, aes(x = x, y = density, color = distribution)) +\n  geom_line(size = 1) +\n  facet_wrap(~ prior, scales = \"fixed\", nrow = 3) +\n  labs(title = \"Prior and Posterior Densities for Different Priors\", \n       x = TeX(\"$\\\\theta$\"), \n       y = \"Density\", \n       color = \"Distribution\") +\n  theme_minimal()",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 2"
    ]
  },
  {
    "objectID": "template8.html",
    "href": "template8.html",
    "title": "Notebook for Assignment 8",
    "section": "",
    "text": "1 General information\nThis notebook will be published later!",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 8"
    ]
  },
  {
    "objectID": "assignment5.html",
    "href": "assignment5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "The exercises here refer to the lecture 5/BDA chapter 11 content. The main topics for this assignment are the MCSE and importance sampling.\nThe exercises constitute 96% of the Quiz 5 grade.\nWe prepared a quarto notebook specific to this assignment to help you get started. You still need to fill in your answers on Mycourses! You can inspect this and future templates\n\nas a rendered html file (to access the qmd file click the “&lt;/&gt; Code” button at the top right hand corner of the template)\n\n\n\n\n\n\n\nGeneral Instructions for Answering the Assignment Questions\n\n\n\n\n\n\nQuestions below are exact copies of the text found in the MyCourses quiz and should serve as a notebook where you can store notes and code.\nWe recommend opening these notebooks in the Aalto JupyterHub, see how to use R and RStudio remotely.\nFor inspiration for code, have a look at the BDA R Demos and the specific Assignment code notebooks\nRecommended additional self study exercises for each chapter in BDA3 are listed in the course web page. These will help to gain deeper understanding of the topic.\nCommon questions and answers regarding installation and technical problems can be found in Frequently Asked Questions (FAQ).\nDeadlines for all assignments can be found on the course web page and in MyCourses.\nYou are allowed to discuss assignments with your friends, but it is not allowed to copy solutions directly from other students or from internet.\nDo not share your answers publicly.\nDo not copy answers from the internet or from previous years. We compare the answers to the answers from previous years and to the answers from other students this year.\nUse of AI is allowed on the course, but the most of the work needs to by the student, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching).\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof.\nIf you have any suggestions or improvements to the course material, please post in the course chat feedback channel, create an issue, or submit a pull request to the public repository!\n\n\n\n\n\n\nFor convenience the assignment questions are copied below. Answer the questions in MyCourses.\n\n\n\n\nAs you've learned in the previous assignments, in some cases we have closed form solution for useful posterior summaries for parameter vector \\( \\theta \\). 1.1 Why are Monte Carlo and other sampling based methods for posterior evaluation of functions \\( f(\\theta) \\) convenient?\n\n    \n        \n            \n        \n    \nWe now briefly review some Monte Carlo methods. Assume we want to obtain draws from \\(p(\\theta|y)\\). Possibly the simplest way, is to apply grid sampling (see lecture 4). Briefly, the general idea of this method is to identify range of values for \\(\\theta\\) which captures almost all the mass of the posterior distribution, divide this range into grid (small intervals for 1D case, small squares for 2D case and so on), compute the unnormalised posterior density at a grid of values, normalize them by approximating the distribution as a step function over the grid and setting the total probability in the grid to 1. Then we can sample from the discrete distribution over values of a grid (more information can be found on page 76 of BDA3). However, this approach poses a certain challenge for the case of multivariate \\(\\theta\\).\n\n    \n        \n            1.2 What is the main issue of grid sampling:\n        \n    \nWe may try to apply more elaborated methods such as rejection sampling (page 264 of BDA3). Rejection sampling is based on defining a positive function \\(g(\\theta)\\), so that we can obtain draws from the probability density proportional to \\(p(\\theta)\\) and accept those draws according to a certain rule (check lecture lecture 4).1.3 : What is a problem with this rejection sampling:\n\n    \nThe most popular way to obtain posterior draws nowadays is MCMC. 2.1 What is a Markov chain?2.2 Which of the following is not a Markov chain?2.3 In question 2.2 we assumed discrete sequences. For the continuous case, we can use conditional transition distribution \\( T_t(\\theta^t |\\theta^{t-1}) \\) for a set of random variables \\( \\theta^1,\\theta^2,\\dotsc \\) to describe the probability density \\( p(\\theta^t | \\theta^1, \\dotsc , \\theta^{t-1}) = p(\\theta^t | \\theta^{t-1}) \\), given some initial point \\( \\theta^0 \\). What are the two steps we need to prove for the Markov chain to eventually produce valid series of draws from the the posterior \\( p(\\theta |y) \\)?2.4 What are the necessary and sufficient conditions to prove that a Markov chain has a unique stationary distribution?2.5 Why is it often necessary to discard the first draws of the MCMC chain?2.6 Why might MCMC be preferrable to grid and rejection sampling? \n\nThe Metropolis-Hastings algorithm is a relatively simple MCMC algorithm, yet it is foundational for more advanced algorithms, such as the Hamiltonian Monte Carlo algorithm you will encounter later in the course. Later in this assignment, you will implement the Metropolis algorithm (page 278 of BDA3). Denote the proposal distribution at time \\( t \\) for parameter vector \\( \\theta^t \\) contitional on \\( \\theta^{t-1} \\) as \\( J_t(\\theta^t|\\theta^{t-1}) \\). For simplicity, we consider here symmetric proposal distributions (otherwise the algorithm would be known as the Metropolis-Hastings algorithm). \n3.1: What is a main idea behind the Metropolis algorithm:3.2: Denote the current proposal by \\( \\theta^*\\) at time t from the proposal distribution \\( J_t(\\theta^* | \\theta^{t-1}) \\). Which one is the correct acceptance ratio, \\( r \\):3.3: Which one is the correct rule for accepting the proposal? Denote the acceptance ratio you've determined in 3.2 by \\(r\\). Set \\( \\theta^t \\) equal to:3.4 Assume the posterior surface has a unique mode. What would eventually happen to the MCMC chain if adopting the rule \\(\\theta^*\\) iff \\( r &gt; 1 \\) and \\( \\theta^{t-1} \\) otherwise?3.5 Retain the assumpion made in question 3.4. Why is the posterior mode alone not generally useful for posterior expectations?\n\n\nSuppose you've successfully obtained multiple chains of MCMC draws from the Metropolis algorithm. Remember that the goal for the MCMC chains is to firstly converge to some unique distribution, and that all chains converge to the same distribution (with the hope of that being \\(p(\\theta|y)\\)). You would now like to check if your chains can be trusted to have done so, after having discarded an appropriate amount of draws you used for warm-up.  There are multiple convergence diagnostics and we will discuss some of them here. \nOne way is to investigate the trace plot: check visually whether the chains converged to the same distribution. If they have, we say the chains have mixed. To do this, we plot them in the same figure and observe if something went wrong. Below you are given an example with two chains. Look at them carefully and answer questions below.\n\n\nFigure 1: \n\n    4.1: Can you say that your chains converged to the same distribution based on Figure 1:\n    \n        \n\n    Figure 2: \n        4.2: Can you say that your chains are mixing well based on Figure 2:\n        \n            \n        \n        \n        Trace plot inspection can be useful when parameter space is small, however, when dealing with large spaces, it becomes inconvenient to analyse trace plots of each parameter separately, therefore we would like to also have some quantitative metrics for investigating convergence. One of them is called Rhat. There are different versions of it, for instance, version from BDA3 book(page 285). The much improved version you will use throughout the course is based on Vehtari et al. (2021). Let's look at the book version to understand why it is preferred to use the modified version.5.1: What is the definition of Rhat from BDA3 book:\n        \n            5.2: Which range of values for Rhat would are a good indicator for convergence (see lecture 5)?Imagine a situation where one chain converged to \\(N(0,1)\\) distribution and the other one to Student t with 3 degrees of freedom. The trace plot then looks like:\n        Figure 3: 5.3: What do you observe:\n        \n            5.4: However, despite of this issue, the Rhat value from the book equals 0.999859, indicating good convergence of the chains. What can be the explanation for this:\n        \n            On the contrary, Rhat value from Vehtari et al paper equals 1.39025,  much bigger than recommended 1.01, resolving the above problem. You can find more examples about new Rhat in the paper but also this demo.\n        \n        Another useful quantity is the effective sample size. \n        6.1: Why can we not rely on total sample size?6.2: Suppose you have obtained S draws from your posterior with MCMC. We define the effective sample size as \\( S_{eff} = S/\\tau \\). What does \\( \\tau \\) refer to in this context?6.3: Look at the trace plots below. For which sequences do you expect HIGHER effective sample size:\n        \n                    \n        6.4: Select the autocorrelation function below corresponding to the chains in Figure 4 of the previous question.\n        \n                    \n\n        \n        In this exercise, you will implement the Metropolis algorithm. Here you are provided code to get you started fro this part of the assignment. You are provided with the code for a Metropolis algorithm, however, it contains three errors. Your task is to find these errors, correct them and answer the following questions below. \n        7.1: Lets denote \"bioassaylp(alpha_propose, beta_propose, x, y, n) - bioassaylp(alpha_previous, beta_previous, x, y, n) + dmvnorm(c(alpha_propose, beta_propose), prior_mean, prior_sigma, TRUE) - dmvnorm(c(alpha_previous, beta_previous), prior_mean, prior_sigma, TRUE)\" as expression 1. How should we correct error 1:\n        \n        \n            7.2: Lets denote \"if(runif(1) &gt; density_ratio(alpha_propose, alpha_previous, beta_propose, beta_previous, x, y, n))\" as expression 2. How should we correct error 2:\n        \n            7.3: How should we correct error 3:\n        \n            After you corrected the errors, you may run sampling. Investigate the code in the notebook and answer the questions below.7.4: How many chains did we run:  .7.5: How many draws including warm-up have we obtained for each individual chain:  .7.6: What is the warm-up length: .7.7: After performing sampling, let's check the convergence of the chains. The next code chunk will produce trace plots. Which one is the correct one:\n        \n            \n            \n            \n        }After looking at trace plots, compute Rhat values for both alpha and beta:7.8: Rhat for alpha=, Rhat for beta=\n        In addition to Rhat, compute ESS for for the mean and 0.25 quantile of alpha and beta:7.9: ESS mean for alpha=, ESS mean for beta=.7.10: ESS q0.25 for alpha=, ESS q0.25 for beta=.Let us compare ESS of independent draws and MCMC draws. Use 4000 independent draws from the posterior distribution of the model in the dataset bioassay_posterior in the aaltobda package from Assignment 4.7.11: ESS mean of alpha for bioassay_posterior =, ESS mean of beta for bioassay_posterior =7.12: ESS q0.25 of alpha for bioassay_posterior =, ESS q0.25 of beta for bioassay_posterior =7.13: Visualise scatter plot of posterior draws. Which one is the correct figure:",
    "crumbs": [
      "Assignments",
      "Assignment 5"
    ]
  },
  {
    "objectID": "assignment5.html#assignment-questions",
    "href": "assignment5.html#assignment-questions",
    "title": "Assignment 5",
    "section": "",
    "text": "For convenience the assignment questions are copied below. Answer the questions in MyCourses.\n\n\n\n\nAs you've learned in the previous assignments, in some cases we have closed form solution for useful posterior summaries for parameter vector \\( \\theta \\). 1.1 Why are Monte Carlo and other sampling based methods for posterior evaluation of functions \\( f(\\theta) \\) convenient?\n\n    \n        \n            \n        \n    \nWe now briefly review some Monte Carlo methods. Assume we want to obtain draws from \\(p(\\theta|y)\\). Possibly the simplest way, is to apply grid sampling (see lecture 4). Briefly, the general idea of this method is to identify range of values for \\(\\theta\\) which captures almost all the mass of the posterior distribution, divide this range into grid (small intervals for 1D case, small squares for 2D case and so on), compute the unnormalised posterior density at a grid of values, normalize them by approximating the distribution as a step function over the grid and setting the total probability in the grid to 1. Then we can sample from the discrete distribution over values of a grid (more information can be found on page 76 of BDA3). However, this approach poses a certain challenge for the case of multivariate \\(\\theta\\).\n\n    \n        \n            1.2 What is the main issue of grid sampling:\n        \n    \nWe may try to apply more elaborated methods such as rejection sampling (page 264 of BDA3). Rejection sampling is based on defining a positive function \\(g(\\theta)\\), so that we can obtain draws from the probability density proportional to \\(p(\\theta)\\) and accept those draws according to a certain rule (check lecture lecture 4).1.3 : What is a problem with this rejection sampling:\n\n    \nThe most popular way to obtain posterior draws nowadays is MCMC. 2.1 What is a Markov chain?2.2 Which of the following is not a Markov chain?2.3 In question 2.2 we assumed discrete sequences. For the continuous case, we can use conditional transition distribution \\( T_t(\\theta^t |\\theta^{t-1}) \\) for a set of random variables \\( \\theta^1,\\theta^2,\\dotsc \\) to describe the probability density \\( p(\\theta^t | \\theta^1, \\dotsc , \\theta^{t-1}) = p(\\theta^t | \\theta^{t-1}) \\), given some initial point \\( \\theta^0 \\). What are the two steps we need to prove for the Markov chain to eventually produce valid series of draws from the the posterior \\( p(\\theta |y) \\)?2.4 What are the necessary and sufficient conditions to prove that a Markov chain has a unique stationary distribution?2.5 Why is it often necessary to discard the first draws of the MCMC chain?2.6 Why might MCMC be preferrable to grid and rejection sampling? \n\nThe Metropolis-Hastings algorithm is a relatively simple MCMC algorithm, yet it is foundational for more advanced algorithms, such as the Hamiltonian Monte Carlo algorithm you will encounter later in the course. Later in this assignment, you will implement the Metropolis algorithm (page 278 of BDA3). Denote the proposal distribution at time \\( t \\) for parameter vector \\( \\theta^t \\) contitional on \\( \\theta^{t-1} \\) as \\( J_t(\\theta^t|\\theta^{t-1}) \\). For simplicity, we consider here symmetric proposal distributions (otherwise the algorithm would be known as the Metropolis-Hastings algorithm). \n3.1: What is a main idea behind the Metropolis algorithm:3.2: Denote the current proposal by \\( \\theta^*\\) at time t from the proposal distribution \\( J_t(\\theta^* | \\theta^{t-1}) \\). Which one is the correct acceptance ratio, \\( r \\):3.3: Which one is the correct rule for accepting the proposal? Denote the acceptance ratio you've determined in 3.2 by \\(r\\). Set \\( \\theta^t \\) equal to:3.4 Assume the posterior surface has a unique mode. What would eventually happen to the MCMC chain if adopting the rule \\(\\theta^*\\) iff \\( r &gt; 1 \\) and \\( \\theta^{t-1} \\) otherwise?3.5 Retain the assumpion made in question 3.4. Why is the posterior mode alone not generally useful for posterior expectations?\n\n\nSuppose you've successfully obtained multiple chains of MCMC draws from the Metropolis algorithm. Remember that the goal for the MCMC chains is to firstly converge to some unique distribution, and that all chains converge to the same distribution (with the hope of that being \\(p(\\theta|y)\\)). You would now like to check if your chains can be trusted to have done so, after having discarded an appropriate amount of draws you used for warm-up.  There are multiple convergence diagnostics and we will discuss some of them here. \nOne way is to investigate the trace plot: check visually whether the chains converged to the same distribution. If they have, we say the chains have mixed. To do this, we plot them in the same figure and observe if something went wrong. Below you are given an example with two chains. Look at them carefully and answer questions below.\n\n\nFigure 1: \n\n    4.1: Can you say that your chains converged to the same distribution based on Figure 1:\n    \n        \n\n    Figure 2: \n        4.2: Can you say that your chains are mixing well based on Figure 2:\n        \n            \n        \n        \n        Trace plot inspection can be useful when parameter space is small, however, when dealing with large spaces, it becomes inconvenient to analyse trace plots of each parameter separately, therefore we would like to also have some quantitative metrics for investigating convergence. One of them is called Rhat. There are different versions of it, for instance, version from BDA3 book(page 285). The much improved version you will use throughout the course is based on Vehtari et al. (2021). Let's look at the book version to understand why it is preferred to use the modified version.5.1: What is the definition of Rhat from BDA3 book:\n        \n            5.2: Which range of values for Rhat would are a good indicator for convergence (see lecture 5)?Imagine a situation where one chain converged to \\(N(0,1)\\) distribution and the other one to Student t with 3 degrees of freedom. The trace plot then looks like:\n        Figure 3: 5.3: What do you observe:\n        \n            5.4: However, despite of this issue, the Rhat value from the book equals 0.999859, indicating good convergence of the chains. What can be the explanation for this:\n        \n            On the contrary, Rhat value from Vehtari et al paper equals 1.39025,  much bigger than recommended 1.01, resolving the above problem. You can find more examples about new Rhat in the paper but also this demo.\n        \n        Another useful quantity is the effective sample size. \n        6.1: Why can we not rely on total sample size?6.2: Suppose you have obtained S draws from your posterior with MCMC. We define the effective sample size as \\( S_{eff} = S/\\tau \\). What does \\( \\tau \\) refer to in this context?6.3: Look at the trace plots below. For which sequences do you expect HIGHER effective sample size:\n        \n                    \n        6.4: Select the autocorrelation function below corresponding to the chains in Figure 4 of the previous question.\n        \n                    \n\n        \n        In this exercise, you will implement the Metropolis algorithm. Here you are provided code to get you started fro this part of the assignment. You are provided with the code for a Metropolis algorithm, however, it contains three errors. Your task is to find these errors, correct them and answer the following questions below. \n        7.1: Lets denote \"bioassaylp(alpha_propose, beta_propose, x, y, n) - bioassaylp(alpha_previous, beta_previous, x, y, n) + dmvnorm(c(alpha_propose, beta_propose), prior_mean, prior_sigma, TRUE) - dmvnorm(c(alpha_previous, beta_previous), prior_mean, prior_sigma, TRUE)\" as expression 1. How should we correct error 1:\n        \n        \n            7.2: Lets denote \"if(runif(1) &gt; density_ratio(alpha_propose, alpha_previous, beta_propose, beta_previous, x, y, n))\" as expression 2. How should we correct error 2:\n        \n            7.3: How should we correct error 3:\n        \n            After you corrected the errors, you may run sampling. Investigate the code in the notebook and answer the questions below.7.4: How many chains did we run:  .7.5: How many draws including warm-up have we obtained for each individual chain:  .7.6: What is the warm-up length: .7.7: After performing sampling, let's check the convergence of the chains. The next code chunk will produce trace plots. Which one is the correct one:\n        \n            \n            \n            \n        }After looking at trace plots, compute Rhat values for both alpha and beta:7.8: Rhat for alpha=, Rhat for beta=\n        In addition to Rhat, compute ESS for for the mean and 0.25 quantile of alpha and beta:7.9: ESS mean for alpha=, ESS mean for beta=.7.10: ESS q0.25 for alpha=, ESS q0.25 for beta=.Let us compare ESS of independent draws and MCMC draws. Use 4000 independent draws from the posterior distribution of the model in the dataset bioassay_posterior in the aaltobda package from Assignment 4.7.11: ESS mean of alpha for bioassay_posterior =, ESS mean of beta for bioassay_posterior =7.12: ESS q0.25 of alpha for bioassay_posterior =, ESS q0.25 of beta for bioassay_posterior =7.13: Visualise scatter plot of posterior draws. Which one is the correct figure:",
    "crumbs": [
      "Assignments",
      "Assignment 5"
    ]
  },
  {
    "objectID": "assignment_instructions.html",
    "href": "assignment_instructions.html",
    "title": "Assignment instructions",
    "section": "",
    "text": "In addition to R-markdown, Quarto can be used to write the assignment reports. This template contains essentially the same information as the old R-markdown template but we illustrate how you can use Quarto for the assignments.\nSome useful resources to get started with Quarto (also an example of a list):\n\nGetting started with Quarto and Rstudio from the official webpage\nA comprehensive user guide from the official webpage\nMarkdown basics\nQuarto FAQ for R-markdown users\nAwesome Quarto - list by Mickaël Canouil\n\nTo create your assignment, you can use the assignment-specific templates (recommended, see e.g. the links at the top of assignment 1) or remove the formatting instructions and use this file as a template. Keep the header (the first lines of this file between two lines of —) as it sets the author name to be anonymous, and you can set the title to match the assignment number.\nAs with R-markdown, you can use the text editor of your choice, but RStudio’s editor is probably the easiest and you can choose the formatting (e.g. section headings, bolding, lists, figures, etc.) from the toolbar. Switching between the source and visual mode allows the quick preview of your formatting.\nNote The report should be anonymous and submitted to peergrade.io as assignmentX.pdf. Aalto JupyterHub has everything installed and you should be able to render the templates to pdf without any further set-up, but if there are problems contact the TAs or get more information on this from the Quarto documentation. Alternatively, if you have problem with creating a PDF file, start by creating an HTML file and the just print the HTML to a PDF. You may also use other software to create the report PDF, but follow the general instructions in this file (see the pdf version of the template file).",
    "crumbs": [
      "General instructions",
      "Assignment instructions"
    ]
  },
  {
    "objectID": "assignment_instructions.html#a",
    "href": "assignment_instructions.html#a",
    "title": "Assignment instructions",
    "section": "5.1 a)",
    "text": "5.1 a)\nFor each subtask include necessary textual explanation, equations, code and figures so that the answer to the question flows naturally. You can think what kind of report would you like to review, and what kind of information would make it easier where there is error (if there are errors).",
    "crumbs": [
      "General instructions",
      "Assignment instructions"
    ]
  },
  {
    "objectID": "template3.html",
    "href": "template3.html",
    "title": "Notebook for Assignment 3",
    "section": "",
    "text": "This assignment is related to Lecture 3 and BDA3 Chapters 2 and 3. Use Frank Harrell’s recommendations on how to state results in Bayesian two group comparisons (and note that there is no point null hypothesis testing in this assignment).\n\n\n\n\n\n\nTip\n\n\n\n\n\nReading instructions:\n\nThe reading instructions for BDA3 Chapter 2.\nThe reading instructions for BDA3 Chapter 3.\n\n\n\n\n\n\n\n\n\n\nGeneral Instructions for Answering the Assignment Questions\n\n\n\n\n\n\nQuestions below are exact copies of the text found in the MyCourses quiz and should serve as a notebook where you can store notes and code.\nWe recommend opening these notebooks in the Aalto JupyterHub, see how to use R and RStudio remotely.\nFor inspiration for code, have a look at the BDA R Demos and the specific Assignment code notebooks\nRecommended additional self study exercises for each chapter in BDA3 are listed in the course web page. These will help to gain deeper understanding of the topic.\nCommon questions and answers regarding installation and technical problems can be found in Frequently Asked Questions (FAQ).\nDeadlines for all assignments can be found on the course web page and in MyCourses.\nYou are allowed to discuss assignments with your friends, but it is not allowed to copy solutions directly from other students or from internet.\nDo not share your answers publicly.\nDo not copy answers from the internet or from previous years. We compare the answers to the answers from previous years and to the answers from other students this year.\nUse of AI is allowed on the course, but the most of the work needs to by the student, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching).\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof.\nIf you have any suggestions or improvements to the course material, please post in the course chat feedback channel, create an issue, or submit a pull request to the public repository!\n\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\n\nThe following installs and loads the aaltobda package:\n\nif(!require(aaltobda)){\n    install.packages(\"remotes\")\n    remotes::install_github(\"avehtari/BDA_course_Aalto\", subdir = \"rpackage\", upgrade=\"never\")\n    library(aaltobda)\n}\n\nLoading required package: aaltobda\n\n\nThe following will set-up markmyassignment to check your functions at the end of the notebook:\n\nif(!require(markmyassignment)){\n    install.packages(\"markmyassignment\")\n    library(markmyassignment)\n}\n\nLoading required package: markmyassignment\n\nassignment_path = paste(\"https://github.com/avehtari/BDA_course_Aalto/blob/master/tests/assignment3.yml\")\nset_assignment(assignment_path)    \n\nAssignment set:\nassignment3: Bayesian Data Analysis: Assignment 3\nThe assignment contain the following (6) tasks:\n- mu_point_est\n- mu_interval\n- mu_pred_interval\n- mu_pred_point_est\n- posterior_odds_ratio_point_est\n- posterior_odds_ratio_interval\n\n\nThe following installs and loads the latex2exp package, which allows us to use LaTeX in plots:\n\nif(!require(latex2exp)){\n    install.packages(\"latex2exp\")\n    library(latex2exp)\n}\n\nLoading required package: latex2exp\n\n\n\n\n\n\n\nThe following installs and loads the posterior package, which allows us to use its rvar Random Variable Datatype:\n\nif(!require(posterior)){\n    install.packages(\"posterior\")\n    library(posterior)\n}\n\nLoading required package: posterior\n\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following object is masked from 'package:aaltobda':\n\n    mcse_quantile\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nThe following installs and loads the ggdist package along with other packages for advanced plotting functions:\n\nif(!require(ggplot2)){\n    install.packages(\"ggplot2\")\n    library(ggplot2)\n}\n\nLoading required package: ggplot2\n\nggplot2::theme_set(theme_minimal(base_size = 14))\nif(!require(ggdist)){\n    install.packages(\"ggdist\")\n    library(ggdist)\n}\n\nLoading required package: ggdist\n\nif(!require(grid)){\n  install.packages(\"grid\")\n  library(grid)\n}\n\nLoading required package: grid\n\nif(!require(gridExtra)){\n  install.packages(\"gridExtra\")\n  library(gridExtra)\n}\n\nLoading required package: gridExtra\n\nif(!require(tidyr)){\n  install.packages(\"tidyr\")\n  library(tidyr)\n}\n\nLoading required package: tidyr",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 3"
    ]
  },
  {
    "objectID": "template3.html#setting-up-advanced-packages-posterior-and-ggdist",
    "href": "template3.html#setting-up-advanced-packages-posterior-and-ggdist",
    "title": "Notebook for Assignment 3",
    "section": "",
    "text": "The following installs and loads the posterior package, which allows us to use its rvar Random Variable Datatype:\n\nif(!require(posterior)){\n    install.packages(\"posterior\")\n    library(posterior)\n}\n\nLoading required package: posterior\n\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following object is masked from 'package:aaltobda':\n\n    mcse_quantile\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nThe following installs and loads the ggdist package along with other packages for advanced plotting functions:\n\nif(!require(ggplot2)){\n    install.packages(\"ggplot2\")\n    library(ggplot2)\n}\n\nLoading required package: ggplot2\n\nggplot2::theme_set(theme_minimal(base_size = 14))\nif(!require(ggdist)){\n    install.packages(\"ggdist\")\n    library(ggdist)\n}\n\nLoading required package: ggdist\n\nif(!require(grid)){\n  install.packages(\"grid\")\n  library(grid)\n}\n\nLoading required package: grid\n\nif(!require(gridExtra)){\n  install.packages(\"gridExtra\")\n  library(gridExtra)\n}\n\nLoading required package: gridExtra\n\nif(!require(tidyr)){\n  install.packages(\"tidyr\")\n  library(tidyr)\n}\n\nLoading required package: tidyr",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 3"
    ]
  },
  {
    "objectID": "template3.html#marginal-and-probabilities-for-mu",
    "href": "template3.html#marginal-and-probabilities-for-mu",
    "title": "Notebook for Assignment 3",
    "section": "2.1 Marginal and probabilities for \\(\\mu\\)",
    "text": "2.1 Marginal and probabilities for \\(\\mu\\)\nWrite your answers and code here!\nKeep the below name and format for the functions to work with markmyassignment:\n\n# Useful functions: mean(), length(), sqrt(), sum()\n# and qtnew(), dtnew() (from aaltobda)\n\nmu_point_est &lt;- function(data) {\n    # Do computation here, and return as below.\n    # This is the correct return value for the test data provided above.\n    14.5\n    \n}\nmu_interval &lt;- function(data, prob = 0.95) {\n    # Do computation here, and return as below.\n    # This is the correct return value for the test data provided above.\n    c(13.3, 15.7)\n    \n}\n\nYou can plot the density as below if you implement mu_pdf to compute the PDF of the posterior \\(p(\\mu|y)\\) of the average hardness \\(\\mu\\).\n\nmu_pdf &lt;- function(data, x){\n    # Compute necessary parameters here.\n    # These are the correct parameters for `windshieldy_test` \n    # with the provided uninformative prior.\n    df = 3\n    location = 14.5\n    scale = 0.3817557\n    # Use the computed parameters as below to compute the PDF:\n    \n    dtnew(x, df, location, scale)\n}\n\nx_interval = mu_interval(windshieldy1, .999)\nlower_x = x_interval[1]\nupper_x = x_interval[2]\nx = seq(lower_x, upper_x, length.out=1000)\nplot(\n    x, mu_pdf(windshieldy1, x), type=\"l\", \n    xlab=TeX(r'(average hardness $\\mu$)'), \n    ylab=TeX(r'(PDF of the posterior $p(\\mu|y)$)')\n)\n\n\n\n\n\n\n\nFigure 1: PDF of the posterior \\(p(\\mu|y)\\) of the average hardness \\(\\mu\\)",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 3"
    ]
  },
  {
    "objectID": "template3.html#joint-posterior-distribution-for-mu-and-sigma",
    "href": "template3.html#joint-posterior-distribution-for-mu-and-sigma",
    "title": "Notebook for Assignment 3",
    "section": "2.2 Joint posterior distribution for \\(\\mu\\) and \\(\\sigma\\)",
    "text": "2.2 Joint posterior distribution for \\(\\mu\\) and \\(\\sigma\\)\n\n# Define the inputs for the below: Sufficient Statistics\ny &lt;- windshieldy1\nn &lt;- length(y)\ns2 &lt;- var(y)\nmy &lt;- mean(y)\n\n# helper functions to sample from and evaluate\n# scaled inverse chi-squared distribution\nrsinvchisq &lt;- function(n, nu, s2, ...) nu*s2 / rchisq(n , nu, ...)\ndsinvchisq &lt;- function(x, nu, s2){\n  exp(log(nu/2)*nu/2 - lgamma(nu/2) + log(s2)/2*nu - log(x)*(nu/2+1) - (nu*s2/2)/x)\n}\n\n\n# Sample 1000 draws from marginal posteriors\nns &lt;- 1000\nsigma2  &lt;- rsinvchisq(ns, n-1, s2)\nmu &lt;- my + sqrt(sigma2/n)*rnorm(length(sigma2))\nsigma &lt;- sqrt(sigma2)\n\n# Compute the density in a grid of ranged for the grids\nt1l &lt;- c(10, 20)\nt2l &lt;- c(0.5, 7)\nnl &lt;- c(0.001, 50)\nt1 &lt;- seq(t1l[1], t1l[2], length.out = ns)\nt2 &lt;- seq(t2l[1], t2l[2], length.out = ns)\n\n\n# Compute the exact marginal density of mu:\n# multiplication by 1./sqrt(s2/n) is due to the transformation of\n# variable z=(x-mean(y))/sqrt(s2/n), see BDA3 p. 21\npm &lt;- dt((t1-my) / sqrt(s2/n), n-1) / sqrt(s2/n)\n\n# Estimate the marginal density using samples and ad hoc Gaussian kernel approximation\npmk &lt;- density(mu, adjust = 2, n = ns, from = t1l[1], to = t1l[2])$y\n\n#Compute the exact marginal density of sigma:\n# the multiplication by 2*t2 is due to the transformation of\n# variable z=t2^2, see BDA3 p. 21\nps &lt;- dsinvchisq(t2^2, n-1, s2) * 2*t2\n\n# Estimate the marginal density using samples and ad hoc Gaussian kernel approximation\npsk &lt;- density(sigma, n = ns, from = t2l[1], to = t2l[2])$y\n\n\n# Evaluate the joint density in a grid. Note that the following is not normalized, but for plotting contours it does not matter :\n# Combine grid points into another data frame\n# with all pairwise combinations\ndfj &lt;- data.frame(t1 = rep(t1, each = length(t2)),\n                  t2 = rep(t2, length(t1)))\ndfj$z &lt;- dsinvchisq(dfj$t2^2, n-1, s2) * 2*dfj$t2 * dnorm(dfj$t1, my, dfj$t2/sqrt(n))\n# breaks for plotting the contours\ncl &lt;- seq(1e-5, max(dfj$z), length.out = 6)\n\n\n\n# Now, visualise the joint and marginal densities\ndfm &lt;- data.frame(t1, Exact = pm, Empirical = pmk) |&gt;\n  pivot_longer(cols = !t1, names_to=\"grp\", values_to=\"p\")\nmargmu &lt;- ggplot(dfm) +\n  geom_line(aes(t1, p, color = grp)) +\n  coord_cartesian(xlim = t1l) +\n  labs(title = 'Marginal of mu', x = '', y = '') +\n  scale_y_continuous(breaks = NULL) +\n  theme(legend.background = element_blank(),\n        legend.position.inside = c(0.75, 0.8),\n        legend.title = element_blank())\n\n\ndfs &lt;- data.frame(t2, Exact = ps, Empirical = psk) |&gt; \n  pivot_longer(cols = !t2, names_to=\"grp\", values_to=\"p\")\nmargsig &lt;- ggplot(dfs) +\n  geom_line(aes(t2, p, color = grp)) +\n  coord_cartesian(xlim = t2l) +\n  coord_flip() +\n  labs(title = 'Marginal of sigma', x = '', y = '') +\n  scale_y_continuous(breaks = NULL) +\n  theme(legend.background = element_blank(),\n        legend.position.inside = c(0.75, 0.8),\n        legend.title = element_blank())\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n\njoint1labs &lt;- c('Samples','Exact contour')\njoint1 &lt;- ggplot() +\n  geom_point(data = data.frame(mu,sigma), aes(mu, sigma, col = '1'), size = 0.1) +\n  geom_contour(data = dfj, aes(t1, t2, z = z, col = '2'), breaks = cl) +\n  coord_cartesian(xlim = t1l,ylim = t2l) +\n  labs(title = 'Joint posterior', x = '', y = '') +\n  scale_y_continuous(labels = NULL) +\n  scale_x_continuous(labels = NULL) +\n  scale_color_manual(values=c('blue', 'black'), labels = joint1labs) +\n  guides(color = guide_legend(nrow  = 1, override.aes = list(\n    shape = c(16, NA), linetype = c(0, 1), size = c(2, 1)))) +\n  theme(legend.background = element_blank(),\n        legend.position.inside = c(0.5, 0.9),\n        legend.title = element_blank())\n\n\n# blank plot for combining the plots\nbp &lt;- grid.rect(gp = gpar(col = 'white'))\n\n\n\n\n\n\n\ngrid.arrange(joint1, margsig, margmu, bp, nrow = 2)",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 3"
    ]
  },
  {
    "objectID": "template3.html#predictive-distribution",
    "href": "template3.html#predictive-distribution",
    "title": "Notebook for Assignment 3",
    "section": "2.3 Predictive distribution",
    "text": "2.3 Predictive distribution\nWrite your answers and code here!\nKeep the below name and format for the functions to work with markmyassignment:\n\n# Useful functions: mean(), length(), sqrt(), sum()\n# and qtnew(), dtnew() (from aaltobda)\n\nmu_pred_point_est &lt;- function(data) {\n    # Do computation here, and return as below.\n    # This is the correct return value for the test data provided above.\n    14.5\n    \n}\nmu_pred_interval &lt;- function(data, prob = 0.95) {\n    # Do computation here, and return as below.\n    # This is the correct return value for the test data provided above.\n    c(11.8, 17.2)\n    \n}\n\nYou can plot the density as below if you implement mu_pred_pdf to compute the PDF of the posterior predictive \\(p(\\tilde{y}|y)\\) of a new hardness observation \\(\\tilde{y}\\).\n\nmu_pred_pdf &lt;- function(data, x){\n    # Compute necessary parameters here.\n    # These are the correct parameters for `windshieldy_test` \n    # with the provided uninformative prior.\n    df = 3\n    location = 14.5\n    scale = 0.8536316\n    # Use the computed parameters as below to compute the PDF:\n    \n    dtnew(x, df, location, scale)\n}\n\nx_interval = mu_pred_interval(windshieldy1, .999)\nlower_x = x_interval[1]\nupper_x = x_interval[2]\nx = seq(lower_x, upper_x, length.out=1000)\nplot(\n    x, mu_pred_pdf(windshieldy1, x), type=\"l\", \n    xlab=TeX(r'(new hardness observation $\\tilde{y}$)'), \n    ylab=TeX(r'(PDF of the posterior predictive $p(\\tilde{y}|y)$)')\n)\n\n\n\n\n\n\n\nFigure 2: PDF of the posterior predictive \\(p(\\tilde{y}|y)\\) of a new hardness observation \\(\\tilde{y}\\)",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 3"
    ]
  },
  {
    "objectID": "template3.html#advanced-tools-posteriors-rvar-ggdists-stat_dotsinterval",
    "href": "template3.html#advanced-tools-posteriors-rvar-ggdists-stat_dotsinterval",
    "title": "Notebook for Assignment 3",
    "section": "3.1 advanced tools (posterior’s rvar, ggdist’s stat_dotsinterval)",
    "text": "3.1 advanced tools (posterior’s rvar, ggdist’s stat_dotsinterval)\nThe posterior package’s random variable datatype rvar is a “sample-based representation of random variables” which makes handling of random samples (of draws) such as the ones contained in the above variables p0 and p1 easier. By default, it prints as the mean and standard deviation of the draws, such that rvar(p0) prints as 0.05 ± 0.021 and rvar(p1) prints as 0.1 ± 0.029.\nThe datatype is “designed to […] be able to be used inside data.frame()s and tibble()s, and to be used with distribution visualizations in the ggdist package.” The code below sets up an R data.frame() with the draws in p0 and p1 wrapped in an rvar, and uses that data frame to visualize the draws using ggdist, an R package building on ggplot2 and “designed for both frequentist and Bayesian uncertainty visualization”.\nThe below plot, Figure 3 uses ggdist’s stat_dotsinterval(), which by default visualizes\n\nan rvar’s median and central 66% and 95% intervals using a black dot and lines of varying thicknesses as when using ggdist’s stat_pointinterval() and\nan rvar’s draws using grey dots as when using ggdist’s stat_dots():\n\n\nr0 = rvar(p0)\nr1 = rvar(p1)\nggplot(data.frame(\n    rv_name=c(\"control\", \"treatment\"), rv=c(r0, r1)\n)) +\n    aes(xdist=rv, y=rv_name) + \n    labs(x=\"probabilities of death\", y=\"patient group\") + \n    stat_dotsinterval()\n\n\n\n\n\n\n\nFigure 3: Probabilities of death for the two patient groups.\n\n\n\n\n\nrvars make it easy to compute functions of random variables, such as\n\ndifferences, e.g. \\(p_0 - p_1\\): r0 - r1 computes an rvar which prints as -0.05 ± 0.037, indicating the sample mean and the sample standard deviation of the difference of the probabilities of death,\nproducts, e.g. \\(p_0 \\, p_1\\): r0 * r1 computes an rvar which prints as 0.005 ± 0.0026 which in this case has no great interpretation\n\nBelow, in Figure 4, we compute the odds ratios using the rvars and visualize its median, central intervals and draws, as above in Figure 3:\n\nrodds_ratio = (r1/(1-r1))/(r0/(1-r0))\nggplot(data.frame(\n    rv=c(rodds_ratio)\n)) +\n    aes(xdist=rv) + \n    labs(x=\"odds ratio\", y=\"relative amount of draws\") + \n    stat_dotsinterval()\n\n\n\n\n\n\n\nFigure 4: Odds ratios of the two patient groups.\n\n\n\n\n\nYou can use Figure 4 to visually check whether the answers you computed make sense.",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 3"
    ]
  },
  {
    "objectID": "assignment6.html",
    "href": "assignment6.html",
    "title": "Assignment 6",
    "section": "",
    "text": "The exercises here refer to the lecture 6/BDA chapter 12 content. The main topics for this assignment are the MCSE and importance sampling.\nThe exercises constitute 96% of the Quiz 6 grade.\nWe prepared a quarto notebook specific to this assignment to help you get started. You still need to fill in your answers on Mycourses! You can inspect this and future templates\n\nas a rendered html file (to access the qmd file click the “&lt;/&gt; Code” button at the top right hand corner of the template)\n\n\n\n\n\n\n\nGeneral Instructions for Answering the Assignment Questions\n\n\n\n\n\n\nQuestions below are exact copies of the text found in the MyCourses quiz and should serve as a notebook where you can store notes and code.\nWe recommend opening these notebooks in the Aalto JupyterHub, see how to use R and RStudio remotely.\nFor inspiration for code, have a look at the BDA R Demos and the specific Assignment code notebooks\nRecommended additional self study exercises for each chapter in BDA3 are listed in the course web page. These will help to gain deeper understanding of the topic.\nCommon questions and answers regarding installation and technical problems can be found in Frequently Asked Questions (FAQ).\nDeadlines for all assignments can be found on the course web page and in MyCourses.\nYou are allowed to discuss assignments with your friends, but it is not allowed to copy solutions directly from other students or from internet.\nDo not share your answers publicly.\nDo not copy answers from the internet or from previous years. We compare the answers to the answers from previous years and to the answers from other students this year.\nUse of AI is allowed on the course, but the most of the work needs to by the student, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching).\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof.\nIf you have any suggestions or improvements to the course material, please post in the course chat feedback channel, create an issue, or submit a pull request to the public repository!\n\n\n\n\n\n\nFor convenience the assignment questions are copied below. Answer the questions in MyCourses.\n\n\n\nThis week's assignment focuses on building the foundations for using frontier MCMC methods and the probabilistic programming language Stan. We will directly code in Stan, but later assignments will use the brms package for estimating Stan models. \n\n\n1.1: Based on the lecture slides for this week, what is the intuition behind HMC?\n\nIn order to generate good proposals, HMC uses the Hamiltonian function and it's partial derivatives to generate a path along the surface of the log posterior. As in the lecture, define \\( \\phi \\) as the momentum variable which is of the same dimension as the parameter vector \\( \\theta \\). The Hamiltonian function has two terms \\( U(\\theta) \\) and \\( K(\\phi) \\).  \n1.2: What is the intuition behind the term \\( U( \\theta ) \\)?  \n1.3: What is the intuition behind the term  \\( K( \\phi ) \\)?\n1.4: The partial derivatives of the Hamiltonian, also known as Hamilton's equations, determine how \\( \\theta \\) and \\( \\phi \\) change during MCMC. What problem occurs when implementing Hamilton's equations computationally?\n1.5: Therefore, all HMC implementations on the computer need to discretise the simulated trajectory dictated by Hamilton's equations. Which computational method does Stan (an most other HMC based algorithms) use for discretisation?\n1.6: It is not necessary in this course to know the computational details behind the leapfrog integrator, only that it is applied for L steps along the Hamiltonian trajectory with step size, \\(\\epsilon\\). The figure below by Neal (2012) shows dynamic simulation in the joint position-momentum space using leapfrog methd. Based on these figures, which of the following statements is false=\n\nFigure 1HMC has two general steps at each iteration of the MCMC chain. Denote the current state of the MCMC chain as t, then1) we draw a new momentum variable \\( \\phi \\) (often assumed to distributed marginally Gaussian) and2) perform a Metropolis update with \\( r=\\exp\\left(-H(\\theta^*,\\phi^*)+H(\\theta^{(t-1)},\\phi^{(t-1)})\\right) \\), where Hamiltonian dynamics are used to produce the proposal.The two parameters of the algorithm, number of steps L and step size \\(\\epsilon\\), need to be tuned. 1.7: What can happen eventually, when allowing the trajectory length, defined as step size times number of steps, \\(\\epsilon L\\), to be long enough? Check out this interactive demo (algorithm=HamiltonianMC, target=standard) and set Leapfrog steps equal to 75 for visual intuition (if the demo freezes, close the demo, restart and instead of sliders, make the control changes by editing the values in the numeric fields). Do not adjust anything else in the demo. To avoid this behaviour that static HMC with fixed integration time (number of steps times the step size) may have, the No-U-Turn (NUTS) algorithm by Hoffman and Gelman (2014) performs automatic tuning: neither the step size nor number of steps need be specified by the user. NUTS uses a tree-building algorithm (see the slides) to adaptively determine the number of steps, L,  while the step size is adapted during the warm-up phase according to a target average acceptance ratio. To gain some more intuition on the behaviour of the adaptivity of NUTS, open the interactive demo again with his link (algorithm=EfficientNUTS, target=banana). Set Autoplay delay to around 500, and set Leapfrog \\(\\delta\\) t to 0.03. This algorithm corresponds to Algorithm 3 in Hoffman and Gelman (2014), where for a fixed \\( \\epsilon \\), the algorithm adaptively determines the number of steps, L.1.8: What do you observe?1.9: On the other hand, set Leapfrog \\(\\delta\\) t to 0.6, all else equal. What do you observe?The user does not need to select the stepsize directly. Stan includes also adaptation of the step size in the warmup phase using stochastic optimization called dual averaging. The user specifies a target acceptance ratio (in Stan actually target for expected discretization error), with argument adapt_delta, and a number of iterations during which adaptation of \\( \\epsilon L \\) occurs (warm-up draws). Open, the interactive demo again with this link (algorithm=DualAveragingNUTS, target=banana), and adjust the Autoplay delay to around 70, but otherwise keep the default options, particularly, keep the target acceptance ratio (here \\(\\delta\\)) at 0.65. On the top left-hand corner m/M_adapt tells you how many draws have been taken compared to number of warm-up draws. Wait until m/M_adapt is at least 50/200.1.10: What do you observe with the first couple of iterations? 1.11: What do you observe with sufficient warm-up? 1.12: Now set the target acceptance ratio to 0.95. What do you observe?1.13: Now set the target acceptance ratio to 0.05. What do you observe?The current Stan HMC-NUTS implementation has some further enchantments, but we will not go to details of those. The main algorithm paramaters are adapt_delta and max_treedepth options.adapt_delta specifies the target expected discretization error (in the same scale as the average proposal acceptance ratio), during the warm-up phase and max_treedepth determines the maximum number of tree doublings in dynamic simulation and thus determine also the maximum number of steps taken. The default in most packages using Stan is setting adapt_delta=0.8. 1.14: What should happen when you increase adapt_delta, all else equal?max_treedepth controls the maximum number of doublings in the tree-building algorithm and thus controls the maximum number of steps in Hamiltonian simulation. This allows the sampler to explore further away in the distribution, which can be beneficial when dealing with complex posteriors. The default in Stan is max_treedepth = 10. 1.15: What is the main cost to increasing the max_treedepth?1.16: Despite the adaptated step-szie, you may encounter challenging posteriors, e.g. with highly varying curvature in log-density. What can happen if the step-size is too big compared to the curvature of the log-density? \n\n\nFrom 2018 to 2022, we have been keeping track of assignment submissions for the BDA course given the number of submissions for the 1st assignment. We will fit a simple linear model to answer two questions of interest:\n\n    What is the trend of student retention as measured by assignment submissions?\n    Given the submission rates for assignments 1–8, how many students will complete the final 9th assignment (and potentially pass the course)?\n\nBelow is broken Stan code for a linear model. In the following, we write the equations following the Stan distributional definitions. See Stan documentation for the definitions related to the normal distribution.\n\\(p(y | x, \\alpha, \\beta, \\sigma) = \\text{normal}(y | \\alpha + \\beta x, \\sigma) \\) (normal model)\\(p(\\alpha, \\beta, \\sigma) \\propto \\text{const.} \\) (improper flat prior)\nIn both the statistical model above and in the Stan model below, \\(x \\in \\mathbb{R}^N\\) and \\(y \\in \\mathbb{R}^N \\) are vectors of the covariates / predictors (the assignment number) and vectors of the observation (proportions of students who have handed in the respective assignment). \\(\\alpha \\in \\mathbb{R}\\) is the unknown scalar intercept, \\(\\beta \\in \\mathbb{R}\\) is the unknown scalar slope and \\(\\sigma \\in \\mathbb{R}_{&gt;0}\\) is the unknown scalar observation standard deviation. The statistical model further implies\\( p(y_{pred.} | x_{pred.}, \\alpha, \\beta, \\sigma) = \\text{normal}(y_{pred.} | \\alpha + \\beta x, \\sigma) \\)as the predictive distribution for a new observation \\(y_{pred.}\\) at a given new covariate value \\( x_{pred.} \\). The broken Stan model code: \n\ndata {\n    // number of data points\n    int&lt;lower=0&gt; N; \n    // covariate / predictor\n    vector[N] x; \n    // observations\n    vector[N] y; \n    // number of covariate values to make predictions at\n    int&lt;lower=0&gt; N_predictions;\n    // covariate values to make predictions at\n    vector[N_predictions] x_predictions; \n}\nparameters {\n    // intercept\n    real alpha; \n    // slope\n    real beta; \n    // the standard deviation should be constrained to be positive\n    real&lt;upper=0&gt; sigma; \n}\ntransformed parameters {\n    // deterministic transformation of parameters and data\n    vector[N] mu = alpha + beta * x // linear model\n}\nmodel {\n    // observation model\n    y ~ normal(mu, sigma); \n}\ngenerated quantities {\n    // compute the means for the covariate values at which to make predictions\n    vector[N_predictions] mu_pred = alpha + beta * x_predictions;\n    // sample from the predictive distribution normal(mu_pred, sigma).\n    array[N_predictions] real y_pred = normal_rng(to_array_1d(mu), sigma);\n}\nFirstly, let's review some Stan syntax.\n2.1: What is the function of the data block?\n2.2: Why should you be careful about declarations of constraints in the data block?\n2.3: What is the function of the parameters block?\n2.4: Can you use statements (e.g. real&lt;lower=0&gt; theta = lambda^2) in the parameters code block, where lambda is some other variable?\n2.5: What is the function of the transformed parameters block?\n2.6: What is the function of the model block?\n2.7: As you've learned in the lecture or from the Stan documentation, log densities can be added to the target density either via the distribution statement using ~ or via the log probability increment statement using target +=. Furthermore, we don't need to include constant terms. Assume the model block has a line   theta ~ normal(0,1);. How could you equivalently increment the log density to get the same increment (ignoring the possible difference in the constants)?\n2.8: Why can the normalising constant(s) be dropped when using MCMC (this holds for e.g. variational inference and optimisation too)?\n\n\nStan language allows writing the models as usually written in books and articles. As you've learned during the course, a collection of distribution statements define a joint distribution as a product of component distributions. \nSuppose your model is the following:\n\\( p(y|\\mu,\\sigma) = \\text{normal}(y|\\mu,\\sigma) \\)\\( p(\\mu) = \\text{normal}(\\mu|0,1) \\)\\( p(\\sigma) = \\text{normal}^+(\\sigma|0,10) \\)\n\n2.9: What are correct ways to write your model in the model block? In the below, assume that there is a line-break after each semi-colon, and that the variables have been appropriately declared in the parameter blocks.\nWhen writing complicated models, it may happen that some distribution statement is accidentally repeated twice. Check out this page in the Stan manual to see what happens in this case. \n2.10: What is the function of the generated quantities block?\n\nNow, let's return to the student retention model code, shown above.\n2.11: What are the three mistakes in the Stan model code above? \nYou may find some of the mistakes in the code using Stan syntax checker. If you copy the Stan code to a file ending .stan and open it in RStudio (you can also choose from RStudio menu File \\(\\rightarrow\\)New File\\(\\rightarrow\\)Stan file to create a new Stan file), the editor will show you some syntax errors. More syntax errors might be detected by clicking `Check’ in the bar just above the Stan file in the RStudio editor. Note that some of the errors in the presented Stan code may not be syntax errors.\nUse the code in the template to first compile your model using CmdStanR (which is a handy interface for CmdStan which transplies Stan model code to C++ which is compiled to executable program which can do the actual sampling), run the inference with the supplied data, and finally create the plot to answer the following questions. \n\nDefine the term of the model \\( \\mu = \\alpha + \\beta x \\) as the linear predictor.\n2.12: What is the solid red line plotting?\n2.13: What are the dashed red lines referring to?\n2.14: How and why are these different from the corresponding grey lines?\n2.15: What is the general trend of student retention as measured by the assignment submissions?\n2.16: Does it do a good job predicting the proportion of students who submit the final 9th assignment?\n2.17: What modeling choice could you make to improve the prediction for the given data set?\n\n\nAnother benefit of the HMC-NUTS algorithm over simpler non-gradient based MCMC algorithms, is that we have access to many diagnostic tools related to the posterior geometry that we would otherwise not have. This feedback is helpful for modeling and also for debugging code, and therefore an essential part of the Bayesian workflow. A model type which is likely to cause problems is logistic regression with complete separation in the data. This creates an unbounded likelihood.\n3.1: Why is it problematic to use flat, improper priors for this likelihood?\nUsing the data generated from the template, compile and run the following Stan model:\n\n// logistic regression\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; M;\n  array[N] int&lt;lower=0,upper=1&gt; y;\n  matrix[N,M] x;\n}\nparameters {\n  real alpha;\n  vector[M] beta;\n}\nmodel {\n  y ~ bernoulli_logit_glm(x, alpha, beta);\n}3.2: You can use the function [fit object name]$diagnostic summary() to check for the number of divergent transitions and max_treedepth exceedences. What do you see?3.3: You can examine the problematic behaviour of MCMC  by looking at parameter specific sampling diagnostics using the summarize_draws function discussed last week. To gain visual intuition use bayesplot's mcmc_pairs function to plot histograms and bivariate scatter plots of the posteriors. What do you observe?Before thinking about changing the model, a first step should be to check for any easy mistakes made in the Stan code. The Stan compiler has a pedantic mode to help spot such issues. By default the pedantic mode is not enabled, but we can use option pedantic = TRUE at compilation time, or after compilation with the check_syntax method.3.4: Use the function [Stan model]$check_syntax(pedantic = TRUE). What warning message(s) do you get?3.5: Set normal(0,10) priors for both parameters, what do you find from the MCMC output?As you've learned during the course, using proper priors, even when they are wide, are always preferable to stabilise inference and make the model generative. You'll learn next week more about priors. 3.6: Sometimes, when adjusting Stan model code you may have removed a variable from the model block, but left the declaration in the parameters block. Add such a variable to your model. What do you see from convergence diagnostics (it may also help you visualise the MCMC chains using mcmc_pairs and mcmc_trace)? 3.7: Use check_syntax(pedantic = TRUE), would you have been able to detect this problem from the output?A further important problem is that of high correlation between parameters in the posterior, which may lead to slow exploration and divergent transitions. Sometimes, we can address this by slightly re-writing the model. Check out the lecture's discussion the Kilpisjärvi case study. More on that in the weeks to follow.  \n\nNow, we re-investigate the Bioassay model from last week with Stan. If you need a reminder about the model and likelihood definition, check out section 3.7 in BDA3. To make the implementation simpler, we consider the following priors:\nalpha ~ normal(0,2)\nbeta ~ normal(10,10)\n4.1: What is the difference in the prior to last week?\n4.2: Which pre-built function can you use in Stan to compute the likelihood? \nUse the code hints in the template to complete your Stan model. For questions about declarations and functionality of Stan, your first point of reference should always be the Stan Manual (it's been recently revised, be sure to open at least version 2.35). Use 4 chains, with 1000 warmup and 2000 total draws each, use default options for adapt_delta and max_treedepth, and use seed = 4911.  Compile your model, define the data inputs needed, sample from the posterior, and answer the following. To make sure that your model is correct, compare a scatter plot of alpha and beta to last week (the priors are not exactly the same, but the posterior should be quite similar to last week as the priors are relatively weak):\n4.3: Did the Stan model produce any warnings about the sampling efficiency? \n4.4: Use the Rhat function from the posterior package, rhat(), (used in last week's assignment). The Rhat for for alpha  and beta  . \n4.5: The ESS mean for alpha= and for beta=.\n4.6: ESS q0.05 for alpha=, ESS q0.05 for beta=.\nPlot the autocorrelation function for the draws for the chains of alpha and beta using the mcmc_acf function from the bayesplot package and compare that to the autocorrelation function from the Metropolis-Hastings (MH) algorithm you developed last week. You don't need to re-do the model for the MH algorithm with the new priors, but for better comparison this is what you should do outside of this exercise. \n4.7: What do you see?",
    "crumbs": [
      "Assignments",
      "Assignment 6"
    ]
  },
  {
    "objectID": "assignment6.html#assignment-questions",
    "href": "assignment6.html#assignment-questions",
    "title": "Assignment 6",
    "section": "",
    "text": "For convenience the assignment questions are copied below. Answer the questions in MyCourses.\n\n\n\nThis week's assignment focuses on building the foundations for using frontier MCMC methods and the probabilistic programming language Stan. We will directly code in Stan, but later assignments will use the brms package for estimating Stan models. \n\n\n1.1: Based on the lecture slides for this week, what is the intuition behind HMC?\n\nIn order to generate good proposals, HMC uses the Hamiltonian function and it's partial derivatives to generate a path along the surface of the log posterior. As in the lecture, define \\( \\phi \\) as the momentum variable which is of the same dimension as the parameter vector \\( \\theta \\). The Hamiltonian function has two terms \\( U(\\theta) \\) and \\( K(\\phi) \\).  \n1.2: What is the intuition behind the term \\( U( \\theta ) \\)?  \n1.3: What is the intuition behind the term  \\( K( \\phi ) \\)?\n1.4: The partial derivatives of the Hamiltonian, also known as Hamilton's equations, determine how \\( \\theta \\) and \\( \\phi \\) change during MCMC. What problem occurs when implementing Hamilton's equations computationally?\n1.5: Therefore, all HMC implementations on the computer need to discretise the simulated trajectory dictated by Hamilton's equations. Which computational method does Stan (an most other HMC based algorithms) use for discretisation?\n1.6: It is not necessary in this course to know the computational details behind the leapfrog integrator, only that it is applied for L steps along the Hamiltonian trajectory with step size, \\(\\epsilon\\). The figure below by Neal (2012) shows dynamic simulation in the joint position-momentum space using leapfrog methd. Based on these figures, which of the following statements is false=\n\nFigure 1HMC has two general steps at each iteration of the MCMC chain. Denote the current state of the MCMC chain as t, then1) we draw a new momentum variable \\( \\phi \\) (often assumed to distributed marginally Gaussian) and2) perform a Metropolis update with \\( r=\\exp\\left(-H(\\theta^*,\\phi^*)+H(\\theta^{(t-1)},\\phi^{(t-1)})\\right) \\), where Hamiltonian dynamics are used to produce the proposal.The two parameters of the algorithm, number of steps L and step size \\(\\epsilon\\), need to be tuned. 1.7: What can happen eventually, when allowing the trajectory length, defined as step size times number of steps, \\(\\epsilon L\\), to be long enough? Check out this interactive demo (algorithm=HamiltonianMC, target=standard) and set Leapfrog steps equal to 75 for visual intuition (if the demo freezes, close the demo, restart and instead of sliders, make the control changes by editing the values in the numeric fields). Do not adjust anything else in the demo. To avoid this behaviour that static HMC with fixed integration time (number of steps times the step size) may have, the No-U-Turn (NUTS) algorithm by Hoffman and Gelman (2014) performs automatic tuning: neither the step size nor number of steps need be specified by the user. NUTS uses a tree-building algorithm (see the slides) to adaptively determine the number of steps, L,  while the step size is adapted during the warm-up phase according to a target average acceptance ratio. To gain some more intuition on the behaviour of the adaptivity of NUTS, open the interactive demo again with his link (algorithm=EfficientNUTS, target=banana). Set Autoplay delay to around 500, and set Leapfrog \\(\\delta\\) t to 0.03. This algorithm corresponds to Algorithm 3 in Hoffman and Gelman (2014), where for a fixed \\( \\epsilon \\), the algorithm adaptively determines the number of steps, L.1.8: What do you observe?1.9: On the other hand, set Leapfrog \\(\\delta\\) t to 0.6, all else equal. What do you observe?The user does not need to select the stepsize directly. Stan includes also adaptation of the step size in the warmup phase using stochastic optimization called dual averaging. The user specifies a target acceptance ratio (in Stan actually target for expected discretization error), with argument adapt_delta, and a number of iterations during which adaptation of \\( \\epsilon L \\) occurs (warm-up draws). Open, the interactive demo again with this link (algorithm=DualAveragingNUTS, target=banana), and adjust the Autoplay delay to around 70, but otherwise keep the default options, particularly, keep the target acceptance ratio (here \\(\\delta\\)) at 0.65. On the top left-hand corner m/M_adapt tells you how many draws have been taken compared to number of warm-up draws. Wait until m/M_adapt is at least 50/200.1.10: What do you observe with the first couple of iterations? 1.11: What do you observe with sufficient warm-up? 1.12: Now set the target acceptance ratio to 0.95. What do you observe?1.13: Now set the target acceptance ratio to 0.05. What do you observe?The current Stan HMC-NUTS implementation has some further enchantments, but we will not go to details of those. The main algorithm paramaters are adapt_delta and max_treedepth options.adapt_delta specifies the target expected discretization error (in the same scale as the average proposal acceptance ratio), during the warm-up phase and max_treedepth determines the maximum number of tree doublings in dynamic simulation and thus determine also the maximum number of steps taken. The default in most packages using Stan is setting adapt_delta=0.8. 1.14: What should happen when you increase adapt_delta, all else equal?max_treedepth controls the maximum number of doublings in the tree-building algorithm and thus controls the maximum number of steps in Hamiltonian simulation. This allows the sampler to explore further away in the distribution, which can be beneficial when dealing with complex posteriors. The default in Stan is max_treedepth = 10. 1.15: What is the main cost to increasing the max_treedepth?1.16: Despite the adaptated step-szie, you may encounter challenging posteriors, e.g. with highly varying curvature in log-density. What can happen if the step-size is too big compared to the curvature of the log-density? \n\n\nFrom 2018 to 2022, we have been keeping track of assignment submissions for the BDA course given the number of submissions for the 1st assignment. We will fit a simple linear model to answer two questions of interest:\n\n    What is the trend of student retention as measured by assignment submissions?\n    Given the submission rates for assignments 1–8, how many students will complete the final 9th assignment (and potentially pass the course)?\n\nBelow is broken Stan code for a linear model. In the following, we write the equations following the Stan distributional definitions. See Stan documentation for the definitions related to the normal distribution.\n\\(p(y | x, \\alpha, \\beta, \\sigma) = \\text{normal}(y | \\alpha + \\beta x, \\sigma) \\) (normal model)\\(p(\\alpha, \\beta, \\sigma) \\propto \\text{const.} \\) (improper flat prior)\nIn both the statistical model above and in the Stan model below, \\(x \\in \\mathbb{R}^N\\) and \\(y \\in \\mathbb{R}^N \\) are vectors of the covariates / predictors (the assignment number) and vectors of the observation (proportions of students who have handed in the respective assignment). \\(\\alpha \\in \\mathbb{R}\\) is the unknown scalar intercept, \\(\\beta \\in \\mathbb{R}\\) is the unknown scalar slope and \\(\\sigma \\in \\mathbb{R}_{&gt;0}\\) is the unknown scalar observation standard deviation. The statistical model further implies\\( p(y_{pred.} | x_{pred.}, \\alpha, \\beta, \\sigma) = \\text{normal}(y_{pred.} | \\alpha + \\beta x, \\sigma) \\)as the predictive distribution for a new observation \\(y_{pred.}\\) at a given new covariate value \\( x_{pred.} \\). The broken Stan model code: \n\ndata {\n    // number of data points\n    int&lt;lower=0&gt; N; \n    // covariate / predictor\n    vector[N] x; \n    // observations\n    vector[N] y; \n    // number of covariate values to make predictions at\n    int&lt;lower=0&gt; N_predictions;\n    // covariate values to make predictions at\n    vector[N_predictions] x_predictions; \n}\nparameters {\n    // intercept\n    real alpha; \n    // slope\n    real beta; \n    // the standard deviation should be constrained to be positive\n    real&lt;upper=0&gt; sigma; \n}\ntransformed parameters {\n    // deterministic transformation of parameters and data\n    vector[N] mu = alpha + beta * x // linear model\n}\nmodel {\n    // observation model\n    y ~ normal(mu, sigma); \n}\ngenerated quantities {\n    // compute the means for the covariate values at which to make predictions\n    vector[N_predictions] mu_pred = alpha + beta * x_predictions;\n    // sample from the predictive distribution normal(mu_pred, sigma).\n    array[N_predictions] real y_pred = normal_rng(to_array_1d(mu), sigma);\n}\nFirstly, let's review some Stan syntax.\n2.1: What is the function of the data block?\n2.2: Why should you be careful about declarations of constraints in the data block?\n2.3: What is the function of the parameters block?\n2.4: Can you use statements (e.g. real&lt;lower=0&gt; theta = lambda^2) in the parameters code block, where lambda is some other variable?\n2.5: What is the function of the transformed parameters block?\n2.6: What is the function of the model block?\n2.7: As you've learned in the lecture or from the Stan documentation, log densities can be added to the target density either via the distribution statement using ~ or via the log probability increment statement using target +=. Furthermore, we don't need to include constant terms. Assume the model block has a line   theta ~ normal(0,1);. How could you equivalently increment the log density to get the same increment (ignoring the possible difference in the constants)?\n2.8: Why can the normalising constant(s) be dropped when using MCMC (this holds for e.g. variational inference and optimisation too)?\n\n\nStan language allows writing the models as usually written in books and articles. As you've learned during the course, a collection of distribution statements define a joint distribution as a product of component distributions. \nSuppose your model is the following:\n\\( p(y|\\mu,\\sigma) = \\text{normal}(y|\\mu,\\sigma) \\)\\( p(\\mu) = \\text{normal}(\\mu|0,1) \\)\\( p(\\sigma) = \\text{normal}^+(\\sigma|0,10) \\)\n\n2.9: What are correct ways to write your model in the model block? In the below, assume that there is a line-break after each semi-colon, and that the variables have been appropriately declared in the parameter blocks.\nWhen writing complicated models, it may happen that some distribution statement is accidentally repeated twice. Check out this page in the Stan manual to see what happens in this case. \n2.10: What is the function of the generated quantities block?\n\nNow, let's return to the student retention model code, shown above.\n2.11: What are the three mistakes in the Stan model code above? \nYou may find some of the mistakes in the code using Stan syntax checker. If you copy the Stan code to a file ending .stan and open it in RStudio (you can also choose from RStudio menu File \\(\\rightarrow\\)New File\\(\\rightarrow\\)Stan file to create a new Stan file), the editor will show you some syntax errors. More syntax errors might be detected by clicking `Check’ in the bar just above the Stan file in the RStudio editor. Note that some of the errors in the presented Stan code may not be syntax errors.\nUse the code in the template to first compile your model using CmdStanR (which is a handy interface for CmdStan which transplies Stan model code to C++ which is compiled to executable program which can do the actual sampling), run the inference with the supplied data, and finally create the plot to answer the following questions. \n\nDefine the term of the model \\( \\mu = \\alpha + \\beta x \\) as the linear predictor.\n2.12: What is the solid red line plotting?\n2.13: What are the dashed red lines referring to?\n2.14: How and why are these different from the corresponding grey lines?\n2.15: What is the general trend of student retention as measured by the assignment submissions?\n2.16: Does it do a good job predicting the proportion of students who submit the final 9th assignment?\n2.17: What modeling choice could you make to improve the prediction for the given data set?\n\n\nAnother benefit of the HMC-NUTS algorithm over simpler non-gradient based MCMC algorithms, is that we have access to many diagnostic tools related to the posterior geometry that we would otherwise not have. This feedback is helpful for modeling and also for debugging code, and therefore an essential part of the Bayesian workflow. A model type which is likely to cause problems is logistic regression with complete separation in the data. This creates an unbounded likelihood.\n3.1: Why is it problematic to use flat, improper priors for this likelihood?\nUsing the data generated from the template, compile and run the following Stan model:\n\n// logistic regression\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; M;\n  array[N] int&lt;lower=0,upper=1&gt; y;\n  matrix[N,M] x;\n}\nparameters {\n  real alpha;\n  vector[M] beta;\n}\nmodel {\n  y ~ bernoulli_logit_glm(x, alpha, beta);\n}3.2: You can use the function [fit object name]$diagnostic summary() to check for the number of divergent transitions and max_treedepth exceedences. What do you see?3.3: You can examine the problematic behaviour of MCMC  by looking at parameter specific sampling diagnostics using the summarize_draws function discussed last week. To gain visual intuition use bayesplot's mcmc_pairs function to plot histograms and bivariate scatter plots of the posteriors. What do you observe?Before thinking about changing the model, a first step should be to check for any easy mistakes made in the Stan code. The Stan compiler has a pedantic mode to help spot such issues. By default the pedantic mode is not enabled, but we can use option pedantic = TRUE at compilation time, or after compilation with the check_syntax method.3.4: Use the function [Stan model]$check_syntax(pedantic = TRUE). What warning message(s) do you get?3.5: Set normal(0,10) priors for both parameters, what do you find from the MCMC output?As you've learned during the course, using proper priors, even when they are wide, are always preferable to stabilise inference and make the model generative. You'll learn next week more about priors. 3.6: Sometimes, when adjusting Stan model code you may have removed a variable from the model block, but left the declaration in the parameters block. Add such a variable to your model. What do you see from convergence diagnostics (it may also help you visualise the MCMC chains using mcmc_pairs and mcmc_trace)? 3.7: Use check_syntax(pedantic = TRUE), would you have been able to detect this problem from the output?A further important problem is that of high correlation between parameters in the posterior, which may lead to slow exploration and divergent transitions. Sometimes, we can address this by slightly re-writing the model. Check out the lecture's discussion the Kilpisjärvi case study. More on that in the weeks to follow.  \n\nNow, we re-investigate the Bioassay model from last week with Stan. If you need a reminder about the model and likelihood definition, check out section 3.7 in BDA3. To make the implementation simpler, we consider the following priors:\nalpha ~ normal(0,2)\nbeta ~ normal(10,10)\n4.1: What is the difference in the prior to last week?\n4.2: Which pre-built function can you use in Stan to compute the likelihood? \nUse the code hints in the template to complete your Stan model. For questions about declarations and functionality of Stan, your first point of reference should always be the Stan Manual (it's been recently revised, be sure to open at least version 2.35). Use 4 chains, with 1000 warmup and 2000 total draws each, use default options for adapt_delta and max_treedepth, and use seed = 4911.  Compile your model, define the data inputs needed, sample from the posterior, and answer the following. To make sure that your model is correct, compare a scatter plot of alpha and beta to last week (the priors are not exactly the same, but the posterior should be quite similar to last week as the priors are relatively weak):\n4.3: Did the Stan model produce any warnings about the sampling efficiency? \n4.4: Use the Rhat function from the posterior package, rhat(), (used in last week's assignment). The Rhat for for alpha  and beta  . \n4.5: The ESS mean for alpha= and for beta=.\n4.6: ESS q0.05 for alpha=, ESS q0.05 for beta=.\nPlot the autocorrelation function for the draws for the chains of alpha and beta using the mcmc_acf function from the bayesplot package and compare that to the autocorrelation function from the Metropolis-Hastings (MH) algorithm you developed last week. You don't need to re-do the model for the MH algorithm with the new priors, but for better comparison this is what you should do outside of this exercise. \n4.7: What do you see?",
    "crumbs": [
      "Assignments",
      "Assignment 6"
    ]
  },
  {
    "objectID": "template5.html",
    "href": "template5.html",
    "title": "Notebook for Assignment 5",
    "section": "",
    "text": "1 General information\nThis assignment is related to Lecture 5 and Chapters 10 and 11.\nIf you are not using JupyterHub (which has all the needed packages pre-installed), and want to make the assignment on your own computer, you may use a docker container that includes all the necessary software packages, too.\n\nReading instructions:\n\nThe reading instructions for BDA3 Chapter 10.\nThe reading instructions for BDA3 Chapter 11.\n\n\n\n\n\n\n\n\nGeneral Instructions for Answering the Assignment Questions\n\n\n\n\n\n\nQuestions below are exact copies of the text found in the MyCourses quiz and should serve as a notebook where you can store notes and code.\nWe recommend opening these notebooks in the Aalto JupyterHub, see how to use R and RStudio remotely.\nFor inspiration for code, have a look at the BDA R Demos and the specific Assignment code notebooks\nRecommended additional self study exercises for each chapter in BDA3 are listed in the course web page. These will help to gain deeper understanding of the topic.\nCommon questions and answers regarding installation and technical problems can be found in Frequently Asked Questions (FAQ).\nDeadlines for all assignments can be found on the course web page and in MyCourses.\nYou are allowed to discuss assignments with your friends, but it is not allowed to copy solutions directly from other students or from internet.\nDo not share your answers publicly.\nDo not copy answers from the internet or from previous years. We compare the answers to the answers from previous years and to the answers from other students this year.\nUse of AI is allowed on the course, but the most of the work needs to by the student, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching).\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof.\nIf you have any suggestions or improvements to the course material, please post in the course chat feedback channel, create an issue, or submit a pull request to the public repository!\n\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\n\nThe following will set-up markmyassignment to check your functions at the end of the notebook:\n\nif(!require(markmyassignment)){\n    install.packages(\"markmyassignment\")\n    library(markmyassignment)\n}\n\nLoading required package: markmyassignment\n\nassignment_path = paste(\"https://github.com/avehtari/BDA_course_Aalto/\",\n\"blob/master/tests/assignment5.yml\", sep=\"\")\nset_assignment(assignment_path)\n\nAssignment set:\nassignment5: Bayesian Data Analysis: Assignment 5\nThe assignment contain the following task:\n- density_ratio\n\n\nThe following installs and loads the aaltobda package:\n\nif(!require(aaltobda)){\n    install.packages(\"aaltobda\", repos = c(\"https://avehtari.github.io/BDA_course_Aalto/\", getOption(\"repos\")))\n    library(aaltobda)\n}\n\nLoading required package: aaltobda\n\n\nThe following installs and loads the latex2exp package, which allows us to use LaTeX in plots:\n\nif(!require(latex2exp)){\n    install.packages(\"latex2exp\")\n    library(latex2exp)\n}\n\nLoading required package: latex2exp\n\n\nThe following installs and loads the posterior package which imports the rhat_basic() function:\n\nif(!require(posterior)){\n    install.packages(\"posterior\")\n    library(posterior)\n}\n\nLoading required package: posterior\n\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following object is masked from 'package:aaltobda':\n\n    mcse_quantile\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nThe following installs and loads the ggplot2 package and the bayesplot package\n\nif(!require(ggplot2)){\n    install.packages(\"ggplot2\")\n    library(ggplot2)\n}\n\nLoading required package: ggplot2\n\nif(!require(bayesplot)){\n    install.packages(\"bayesplot\")\n    library(bayesplot)\n}\n\nLoading required package: bayesplot\n\n\nThis is bayesplot version 1.11.1\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\n\nAttaching package: 'bayesplot'\n\n\nThe following object is masked from 'package:posterior':\n\n    rhat\n\n\n\n\n\n\n\n2 Generalized linear model: Bioassay model with Metropolis algorithm\nMetropolis algorithm: Replicate the computations for the bioassay example of BDA3 Section 3.7 using the Metropolis algorithm. The Metropolis algorithm is described in BDA3 Chapter 11.2. More information on the bioassay data can be found in Section 3.7 in BDA3, and in Chapter 3 notes.\nBelow you are given a code that implements Metropolis algorithm. It contains two functions - density ratio that computes ratio of joint distributions using log densities and metropolis_bioassay that performs sampling. Reasons behind using log densities are explained on BDA3 page 261 and Lecture video 4.1. Remember that \\(p_1/p_0=\\exp(\\log(p_1)-\\log(p_0))\\). We use the Gaussian prior as in Assignment 4, that is \\[\n\\begin{aligned}\n    \\begin{bmatrix}\n    \\alpha \\\\ \\beta\n    \\end{bmatrix}\n    \\sim\n    \\text{N} \\left( \\mu_0,  \\Sigma_0 \\right), \\qquad\n    \\text{where} \\quad\n     \\mu_0 = \\begin{bmatrix} 0 \\\\ 10 \\end{bmatrix} \\quad \\text{and} \\quad\n     \\Sigma_0 = \\begin{bmatrix} 2^2 & 12 \\\\ 12 & 10^2 \\end{bmatrix}.\n\\end{aligned}\\]\nHowever, this code contains 3 errors. The lines with errors are marked as:\n\n### error x\n\n### error x\n\nYour task is to find these errors, correct them and answer corresponding questions in MyCourses. Note that bioassaylp() from aaltobda package evaluates the log-likelihood for given \\(\\alpha\\) and \\(\\beta\\). As for proposal distribution we used simple normal distributions \\(\\alpha^* \\sim N(\\alpha_{t-1}, \\sigma = \\alpha_\\sigma)\\) and \\(\\beta^* \\sim N(\\beta_{t-1}, \\sigma = \\beta_\\sigma)\\). Efficient proposals are discussed in BDA3 p. 295–297 (not part of the course). In real-life a pre-run could be made with an automatic adaptive control to adapt the proposal distribution.\n\ndata(\"bioassay\")\ndensity_ratio &lt;- function(alpha_propose, alpha_previous, beta_propose, beta_previous, x, y, n){\n        prior_mean = c(0, 10)\n    prior_sigma = cbind(c(4, 12), c(12, 100))\n    ### error 1\n        bioassaylp(alpha_propose, beta_propose, x, y, n)\n        - bioassaylp(alpha_previous, beta_previous, x, y, n)\n        + dmvnorm(c(alpha_propose, beta_propose), prior_mean, prior_sigma, TRUE)\n        - dmvnorm(c(alpha_previous, beta_previous), prior_mean, prior_sigma, TRUE)\n    ### error 1\n}\n\nmetropolis_bioassay &lt;- function(alpha_initial, beta_initial, alpha_sigma, beta_sigma, no_draws, warmup_len, x, y, n, chain_number){\n    data.frame(\n        alpha=c(alpha_initial, alpha_initial+alpha_sigma, alpha_initial-alpha_sigma),\n        beta=c(beta_initial, beta_initial+beta_sigma, beta_initial-beta_sigma)\n    )\n    alpha_previous = alpha_initial\n    beta_previous = beta_initial\n    alpha_rv = c()\n    beta_rv = c()\n    for(draw in 1:no_draws){\n        alpha_propose = rnorm(1, alpha_previous, alpha_sigma)\n        beta_propose = rnorm(1, beta_previous, beta_sigma)\n        ### error 2\n        if(runif(1) &gt; density_ratio(alpha_propose, alpha_previous, beta_propose, beta_previous, x, y, n))\n        ### error 2\n          {\n            alpha_previous = alpha_propose\n            beta_previous = beta_propose\n          }\n      ### error 3\n        alpha_rv = c(alpha_rv, alpha_propose)\n        beta_rv = c(beta_rv, beta_propose)\n      ### error 3\n    }\n    data.frame(alpha=tail(alpha_rv,warmup_len), beta=tail(beta_rv, warmup_len), Chain=rep(chain_number, each=no_draws - warmup_len))\n}\n\n\nset.seed(4911)\n\ndf_chain1 = metropolis_bioassay(0, 0, 1, 1, 3000, 1000, bioassay$x, bioassay$y, bioassay$n, 1)\ndf_chain2 = metropolis_bioassay(1, 1, 1, 1, 3000, 1000, bioassay$x, bioassay$y, bioassay$n, 2)\ndf_chain3 = metropolis_bioassay(1, 5, 1, 1, 3000, 1000, bioassay$x, bioassay$y, bioassay$n, 3)\ndf_chain4 = metropolis_bioassay(5, 3, 1, 1, 3000, 1000, bioassay$x, bioassay$y, bioassay$n, 4)\n\ndf_combined_samples &lt;- rbind(df_chain1, df_chain2, df_chain3, df_chain4)\n\nVisualize trace plots with the code below. Have a look at bayesplot trace plot examples and tune your plot if wanted.\n\n# Useful functions: mcmc_trace (from bayesplot)\nmcmc_trace(df_combined_samples, pars=c(\"alpha\", \"beta\")) + scale_colour_manual(values=c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\"))\n\nScale for colour is already present.\nAdding another scale for colour, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\nAfter looking at trace plots, compute Rhat values for both alpha and beta, along with the ess_mean as well as ess_quantile for the 25th quantile. If you’re unsure about the below, this demo provides more details on how to use the posterior and bayesplot package.\n\n# To compute this, first convert to a draws_df object\nnames(df_combined_samples)[names(df_combined_samples) == 'Chain'] &lt;- '.chain'\ndraws &lt;- as_draws_df(df_combined_samples)\n\n# You can get what you need from this summary\nsummarise_draws(draws, Rhat=rhat_basic, ESS= ess_mean, ~ess_quantile(.x, probs = 0.25))\n\n# A tibble: 2 × 4\n  variable  Rhat   ESS ess_q25\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n1 alpha     2.29  4.99   18.9 \n2 beta      2.28  5.01    9.69\n\n\nPlot the draws for \\(\\alpha\\) and \\(\\beta\\) (scatter plot) and include this plot in your report. You can compare the results to BDA3 Figure 3.3b to verify that your code gives sensible results. Notice though that the results in Figure 3.3b are generated from the posterior with a uniform prior, so even when if your algorithm works perfectly, the results will look slightly different (although fairly similar).\n\n# Useful functions: mcmc_scatter (from bayesplot)",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 5"
    ]
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "The exercises here refer to the lecture 1/BDA chapter 1 content, not the course infrastructure quiz. This assignment is meant to test whether or not you have sufficient knowledge to participate in the course. The first question checks that you remember basic terms of probability calculus. The second exercise checks you recognise the most important notation used throughout the course and used in BDA3. The third-fifth exercise you will solve some basic Bayes theorem questions to check your understanding on the basics of probability theory. The 6th exercise checks on whether you recall the three steps of Bayesian Data Ananlysis as mentioned in chapter 1 of BDA3. The last exercise walks you through an example of how we can use models to generate distributions for outcomes of interest, applied to a setting of a simplified Roulette table.\nThe exercises constitute 80% of the Quiz 1 grade.\nWe prepared a quarto notebook specific to this assignment to help you get started. You still need to fill in your answers on Mycourses! You can inspect this and future templates\n\nas a qmd file,\nas a rendered html file\n\n\n\n\n\n\n\nGeneral Instructions for Answering the Assignment Questions\n\n\n\n\n\n\nQuestions below are exact copies of the text found in the MyCourses quiz and should serve as a notebook where you can store notes and code.\nWe recommend opening these notebooks in the Aalto JupyterHub, see how to use R and RStudio remotely.\nFor inspiration for code, have a look at the BDA R Demos and the specific Assignment code notebooks\nRecommended additional self study exercises for each chapter in BDA3 are listed in the course web page. These will help to gain deeper understanding of the topic.\nCommon questions and answers regarding installation and technical problems can be found in Frequently Asked Questions (FAQ).\nDeadlines for all assignments can be found on the course web page and in MyCourses.\nYou are allowed to discuss assignments with your friends, but it is not allowed to copy solutions directly from other students or from internet.\nDo not share your answers publicly.\nDo not copy answers from the internet or from previous years. We compare the answers to the answers from previous years and to the answers from other students this year.\nUse of AI is allowed on the course, but the most of the work needs to by the student, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching).\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof.\nIf you have any suggestions or improvements to the course material, please post in the course chat feedback channel, create an issue, or submit a pull request to the public repository!\n\n\n\n\n\n\nFor convenience the assignment questions are copied below. Answer the questions in MyCourses.\n\n\n    \n        \n        \n        \n    \n    \n\n\n        Match the following terms with the correct definition:\nNote that the answers order and set of possible answers is the same for questions 1.1 - 1.8. Check the BDA chapter 1, the lecture slides, and Wikipedia if you are uncertain about the terms below. \n\n\n\n    \n        1.1 Probability:\n            \n        1.2 Probability mass (function): \n        1.3 Probability density (function):\n        1.4 Probability distribution: \n        1.5 Discrete probability distribution: \n        1.6 Continuous probability distribution: \n        1.7 Cumulative distribution function (cdf): \n        1.8 Likelihood:\n            \n    \n    \n    \n    \n        \n            \n        \n        \n    \n    \n            Match the following notation with the correct definition: \n    \n    \n        2.1 \\( \\sim \\):  \n        2.2 \\( \\propto \\):  \n        2.3 \\( \\text{E}[\\cdot] \\):  \n        2.4 \\( p(y | \\theta) \\): \n    \n\n    \n    \n    \n    A group of researchers has designed a new inexpensive and painless test\n        for detecting lung cancer. The test is intended to be an initial\n        screening test for the population in general. A positive result\n        (presence of lung cancer) from the test would be followed up immediately\n        with medication, surgery or more extensive and expensive test. \n    The\n        researchers know from their studies the following facts:\n    \n        Test gives a positive result in 98%  of the time when the test subject has lung cancer.\n        Test gives a negative result in  96% of the time when the test subject does not have lung cancer.\n        In general population approximately one person in 1000 has lung cancer.\n    \n    Here are some probability values that can help you figure out if you\n        copied the right conditional probabilities from the question:\n    \n        P(Test gives positive | Subject does not have lung cancer) = 4%\n        P(Test gives positive and Subject has lung cancer) = 0.098%\n            \n                \n                    this is also referred to as the joint probability of test being positive and the subject having lung cancer\n                \n            \n        \n    \n    Your goal is calculate the probability of having cancer given a positive test result: \\( P(\\text{cancer} | \\text{positive}) \\)\n    \n        3.1 Which quantity in Bayes' Theorem does this represent? \n        3.2 What is the probability of the test having a positive result, given that the test subject has cancer (P(B|A))? \n        3.3 What is the probability of having cancer (P(A))? \n        3.4 What is the probability of having a positive test (P(B))? \n        3.5 Using your previous answers, what is the probability of having cancer given a positive test?\n    \n    \n    \n    \n    We have three boxes, A, B, and C. There are\n    \n        2 red balls and 5 white balls in the box A\n        4 red balls and 1 white ball in the box B\n        1 red ball and 3 white balls in the box C.\n    \n    Consider a random experiment in which one of the boxes is randomly\n        selected and from that box, one ball is randomly picked up. After\n        observing the color of the ball it is replaced in the box it came from.\n        Suppose also that on average box A is selected 40% of the time and box B\n        10% of the time (i.e. P(A) = 0.4).\n    \n    \n        4.1 What is the probability of picking a red ball from box A? \n        4.2 What is the probability of picking a red ball from box B? \n        4.3 What is the probability of picking a red ball from box C? \n        4.4 Considering the probabilities of selecting each box, what is the probability of picking a red ball (enter as a number between 0 and 1 with 2 decimal digit accuracy)?\n        \n            4.5 If a red ball was picked, calculate the probability that it was picked from (enter as a number between 0 and 1 with 2 decimal digit accuracy):\n            \n                Box A:  \n                Box B:  \n                Box C:  \n            \n        \n    \n    \n    \n    \n    \n    \n    Assume that on average fraternal twins (two fertilized eggs and then\n        could be of different sex) occur once in 150 births and identical twins\n        (single egg divides into two separate embryos, so both have the same\n        sex) once in 400 births (Note! This is not the true\n        value, see Exercise 1.6, page 28, in BDA3). Assume\n        that an equal number of boys and girls are born on average.\n    American male singer-actor\n        Elvis Presley (1935 – 1977) had a twin brother who died in birth, your\n        goal is to compute the probability that Elvis was an identical twin.\n    \n    5.1 What is the probability of having a twin brother, given identical twins (enter as a number between 0 and 1 no decimal digits needed)?\n    \n    5.2 What is the total probability of having a twin brother (either fraternal or identical)? (enter as a number between 0 and 1 with 2 decimal digit accuracy)\n    5.3 What is the probability that Elvis was an identical twin, given that he had a twin brother? (enter as a number between 0 and 1 with 2 decimal digit accuracy)\n    \n    \n    6.1 Select the three steps of Bayesian data analysis (see BDA3 p. 3): \n\n\n\n    \n    In this course, models are used to explain social and physical data, and we will be able to generate data from our models which we can use for checking how well our model does. In this example, we show how to generate outcomes from a binomial model to explain outcomes of a roulette game (there is a connection to the history of statistics). Suppose a roulette table with only red and black colours. Roulette tables won't be perfect and it's likely that the probability of red vs black is not exactly 0.5 (the tables can have adjustments that are randomized each day to avoid long term bias). \n    Suppose your model for the tables' ratio of red/black is a Binomial which takes as inputs the number of trials and a probability parameter, theta. Set theta to 0.6 (this is much bigger than what we would expect in real roulette, but makes it easier as a teaching example) and generate a series (for a sequence of 100 equally spaced trial values between 10 and 1000) of red/black ratios. Generate 1000 random draws from your model for each trial value and save the data in a Data frame with columns Ratios, Nsims and Trials. Incomplete code can be found below.\n    \n    # load the tidyverse package for data manipulation and plottinglibrary(tidyverse)# Ratio of red/black\ntheta &lt;- # declare probability parameter for the binomial model\n\n# Sequence of trials\ntrials &lt;- seq(#start value of sequence,#end value of sequence,#value for spacing)\n\n# Number of simulation draws from the model\nnsims &lt;- # number of of simulations from the binomial model\n\n# Helper function for getting the ratios\nbinom_gen &lt;- function(trials,theta,nsims){\n    df &lt;-  as.data.frame(rbinom(nsims,trials,theta)/trials) |&gt; mutate(nsims = nsims,trials = trials)\n    colnames(df) &lt;- c(\"Ratios\",\"Nsims\",\"Trials\")\n  return(df)\n}\n\n# Create a data frame containing the draws for each number of trials\nratio_60 &lt;- do.call(rbind, lapply(trials, binom_gen, theta, nsims)) # lapply applies elements in trials column to binom_gen function, which is then rowbound via do.call\n    \n    \n        7.1 Suppose you are unsure whether the code to create the data frame worked. Which of the following functions should you use in order to check on the structure of the dataframe object (assuming df below stands for a generic dataframe object)?\n        7.2 The structure checks out, but now you want to print the first 5 rows of the dataframe to check whether the values are as expected. Which of the following functions should you use?\n        7.3 The quick peek checks also out, but you would be more at ease scrolling all data, perhaps you'll find some interesting patterns. Which of the following actions allows you to scroll through the data in a separate window (for the below, we assume that you have the code loaded in an RStudio session)?\n    \n    \n    \n    \n    Now, plot a histogram of the computed ratios for 10, 50 and 1000 trials, using the code below\n    # Plot the Distributions\nsubset_df &lt;- ratio_60[ratio_60$Trials %in% c(#trial values), ] # Subset your \n\nsubset_df60 |&gt; ggplot(aes(Ratios)) +\n  geom_histogram(position = \"identity\" ,bins = 40) +\n  facet_grid(cols = vars(Trials))  +\n  ggtitle(\"Ratios for specific trials\")\n    \n    \n    \n        7.4 Which histogram below is the correct one for theta = 0.6?\n        7.5 What do these distributions refer to?\n        7.6 Given these histograms, which number of trials gives you the most certainty about the likely red/black ratio for that table?\n        7.7 Given the draws from the model, give an estimate about the probability p(Ratio&lt;=0.5) for the model with 1000 trials (enter as a number between 0 and 1 with 2 decimal digit accuracy). \n    \n    Suppose you are now certain that theta = 0.6, plot the probability density given 1000 trials using the code below.\n\nsize =  # number of trials\nprob =  # probability of success\n\nbinom_data &lt;- data.frame(\n  Success = 0:size,\n  Probability = dbinom(0:size, size = size, prob = prob)\n)\n\nggplot(binom_data, aes(x = Success, y = Probability)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"PMF of Binomial Distribution\", x = \"Number of Successes\", y = \"PDF\")\n\n\n\n\n    \n        7.8 Which plot of the PMF is the correct one?\n        7.9 How does the PMF plot relate to the histogram of ratios plotted earlier?\n        7.10 Given the PMF for your model, calculate the probability for 1000 trials of observing less or equal to 500 red outcomes using theta = 0.6. Use the pbinom function in R. \n    \n    Click to next page",
    "crumbs": [
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignment1.html#assignment-questions",
    "href": "assignment1.html#assignment-questions",
    "title": "Assignment 1",
    "section": "",
    "text": "For convenience the assignment questions are copied below. Answer the questions in MyCourses.\n\n\n    \n        \n        \n        \n    \n    \n\n\n        Match the following terms with the correct definition:\nNote that the answers order and set of possible answers is the same for questions 1.1 - 1.8. Check the BDA chapter 1, the lecture slides, and Wikipedia if you are uncertain about the terms below. \n\n\n\n    \n        1.1 Probability:\n            \n        1.2 Probability mass (function): \n        1.3 Probability density (function):\n        1.4 Probability distribution: \n        1.5 Discrete probability distribution: \n        1.6 Continuous probability distribution: \n        1.7 Cumulative distribution function (cdf): \n        1.8 Likelihood:\n            \n    \n    \n    \n    \n        \n            \n        \n        \n    \n    \n            Match the following notation with the correct definition: \n    \n    \n        2.1 \\( \\sim \\):  \n        2.2 \\( \\propto \\):  \n        2.3 \\( \\text{E}[\\cdot] \\):  \n        2.4 \\( p(y | \\theta) \\): \n    \n\n    \n    \n    \n    A group of researchers has designed a new inexpensive and painless test\n        for detecting lung cancer. The test is intended to be an initial\n        screening test for the population in general. A positive result\n        (presence of lung cancer) from the test would be followed up immediately\n        with medication, surgery or more extensive and expensive test. \n    The\n        researchers know from their studies the following facts:\n    \n        Test gives a positive result in 98%  of the time when the test subject has lung cancer.\n        Test gives a negative result in  96% of the time when the test subject does not have lung cancer.\n        In general population approximately one person in 1000 has lung cancer.\n    \n    Here are some probability values that can help you figure out if you\n        copied the right conditional probabilities from the question:\n    \n        P(Test gives positive | Subject does not have lung cancer) = 4%\n        P(Test gives positive and Subject has lung cancer) = 0.098%\n            \n                \n                    this is also referred to as the joint probability of test being positive and the subject having lung cancer\n                \n            \n        \n    \n    Your goal is calculate the probability of having cancer given a positive test result: \\( P(\\text{cancer} | \\text{positive}) \\)\n    \n        3.1 Which quantity in Bayes' Theorem does this represent? \n        3.2 What is the probability of the test having a positive result, given that the test subject has cancer (P(B|A))? \n        3.3 What is the probability of having cancer (P(A))? \n        3.4 What is the probability of having a positive test (P(B))? \n        3.5 Using your previous answers, what is the probability of having cancer given a positive test?\n    \n    \n    \n    \n    We have three boxes, A, B, and C. There are\n    \n        2 red balls and 5 white balls in the box A\n        4 red balls and 1 white ball in the box B\n        1 red ball and 3 white balls in the box C.\n    \n    Consider a random experiment in which one of the boxes is randomly\n        selected and from that box, one ball is randomly picked up. After\n        observing the color of the ball it is replaced in the box it came from.\n        Suppose also that on average box A is selected 40% of the time and box B\n        10% of the time (i.e. P(A) = 0.4).\n    \n    \n        4.1 What is the probability of picking a red ball from box A? \n        4.2 What is the probability of picking a red ball from box B? \n        4.3 What is the probability of picking a red ball from box C? \n        4.4 Considering the probabilities of selecting each box, what is the probability of picking a red ball (enter as a number between 0 and 1 with 2 decimal digit accuracy)?\n        \n            4.5 If a red ball was picked, calculate the probability that it was picked from (enter as a number between 0 and 1 with 2 decimal digit accuracy):\n            \n                Box A:  \n                Box B:  \n                Box C:  \n            \n        \n    \n    \n    \n    \n    \n    \n    Assume that on average fraternal twins (two fertilized eggs and then\n        could be of different sex) occur once in 150 births and identical twins\n        (single egg divides into two separate embryos, so both have the same\n        sex) once in 400 births (Note! This is not the true\n        value, see Exercise 1.6, page 28, in BDA3). Assume\n        that an equal number of boys and girls are born on average.\n    American male singer-actor\n        Elvis Presley (1935 – 1977) had a twin brother who died in birth, your\n        goal is to compute the probability that Elvis was an identical twin.\n    \n    5.1 What is the probability of having a twin brother, given identical twins (enter as a number between 0 and 1 no decimal digits needed)?\n    \n    5.2 What is the total probability of having a twin brother (either fraternal or identical)? (enter as a number between 0 and 1 with 2 decimal digit accuracy)\n    5.3 What is the probability that Elvis was an identical twin, given that he had a twin brother? (enter as a number between 0 and 1 with 2 decimal digit accuracy)\n    \n    \n    6.1 Select the three steps of Bayesian data analysis (see BDA3 p. 3): \n\n\n\n    \n    In this course, models are used to explain social and physical data, and we will be able to generate data from our models which we can use for checking how well our model does. In this example, we show how to generate outcomes from a binomial model to explain outcomes of a roulette game (there is a connection to the history of statistics). Suppose a roulette table with only red and black colours. Roulette tables won't be perfect and it's likely that the probability of red vs black is not exactly 0.5 (the tables can have adjustments that are randomized each day to avoid long term bias). \n    Suppose your model for the tables' ratio of red/black is a Binomial which takes as inputs the number of trials and a probability parameter, theta. Set theta to 0.6 (this is much bigger than what we would expect in real roulette, but makes it easier as a teaching example) and generate a series (for a sequence of 100 equally spaced trial values between 10 and 1000) of red/black ratios. Generate 1000 random draws from your model for each trial value and save the data in a Data frame with columns Ratios, Nsims and Trials. Incomplete code can be found below.\n    \n    # load the tidyverse package for data manipulation and plottinglibrary(tidyverse)# Ratio of red/black\ntheta &lt;- # declare probability parameter for the binomial model\n\n# Sequence of trials\ntrials &lt;- seq(#start value of sequence,#end value of sequence,#value for spacing)\n\n# Number of simulation draws from the model\nnsims &lt;- # number of of simulations from the binomial model\n\n# Helper function for getting the ratios\nbinom_gen &lt;- function(trials,theta,nsims){\n    df &lt;-  as.data.frame(rbinom(nsims,trials,theta)/trials) |&gt; mutate(nsims = nsims,trials = trials)\n    colnames(df) &lt;- c(\"Ratios\",\"Nsims\",\"Trials\")\n  return(df)\n}\n\n# Create a data frame containing the draws for each number of trials\nratio_60 &lt;- do.call(rbind, lapply(trials, binom_gen, theta, nsims)) # lapply applies elements in trials column to binom_gen function, which is then rowbound via do.call\n    \n    \n        7.1 Suppose you are unsure whether the code to create the data frame worked. Which of the following functions should you use in order to check on the structure of the dataframe object (assuming df below stands for a generic dataframe object)?\n        7.2 The structure checks out, but now you want to print the first 5 rows of the dataframe to check whether the values are as expected. Which of the following functions should you use?\n        7.3 The quick peek checks also out, but you would be more at ease scrolling all data, perhaps you'll find some interesting patterns. Which of the following actions allows you to scroll through the data in a separate window (for the below, we assume that you have the code loaded in an RStudio session)?\n    \n    \n    \n    \n    Now, plot a histogram of the computed ratios for 10, 50 and 1000 trials, using the code below\n    # Plot the Distributions\nsubset_df &lt;- ratio_60[ratio_60$Trials %in% c(#trial values), ] # Subset your \n\nsubset_df60 |&gt; ggplot(aes(Ratios)) +\n  geom_histogram(position = \"identity\" ,bins = 40) +\n  facet_grid(cols = vars(Trials))  +\n  ggtitle(\"Ratios for specific trials\")\n    \n    \n    \n        7.4 Which histogram below is the correct one for theta = 0.6?\n        7.5 What do these distributions refer to?\n        7.6 Given these histograms, which number of trials gives you the most certainty about the likely red/black ratio for that table?\n        7.7 Given the draws from the model, give an estimate about the probability p(Ratio&lt;=0.5) for the model with 1000 trials (enter as a number between 0 and 1 with 2 decimal digit accuracy). \n    \n    Suppose you are now certain that theta = 0.6, plot the probability density given 1000 trials using the code below.\n\nsize =  # number of trials\nprob =  # probability of success\n\nbinom_data &lt;- data.frame(\n  Success = 0:size,\n  Probability = dbinom(0:size, size = size, prob = prob)\n)\n\nggplot(binom_data, aes(x = Success, y = Probability)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"PMF of Binomial Distribution\", x = \"Number of Successes\", y = \"PDF\")\n\n\n\n\n    \n        7.8 Which plot of the PMF is the correct one?\n        7.9 How does the PMF plot relate to the histogram of ratios plotted earlier?\n        7.10 Given the PMF for your model, calculate the probability for 1000 trials of observing less or equal to 500 red outcomes using theta = 0.6. Use the pbinom function in R. \n    \n    Click to next page",
    "crumbs": [
      "Assignments",
      "Assignment 1"
    ]
  },
  {
    "objectID": "template4.html",
    "href": "template4.html",
    "title": "Notebook for Assignment 4",
    "section": "",
    "text": "This assignment is related to Lecture 4 and BDA3 Chapters 3 and 10.\n\nReading instructions:\n\nThe reading instructions for BDA3 Chapter 3.\nThe reading instructions for BDA3 Chapter 10.\n\n\n\n\n\n\n\n\nGeneral Instructions for Answering the Assignment Questions\n\n\n\n\n\n\nQuestions below are exact copies of the text found in the MyCourses quiz and should serve as a notebook where you can store notes and code.\nWe recommend opening these notebooks in the Aalto JupyterHub, see how to use R and RStudio remotely.\nFor inspiration for code, have a look at the BDA R Demos and the specific Assignment code notebooks\nRecommended additional self study exercises for each chapter in BDA3 are listed in the course web page. These will help to gain deeper understanding of the topic.\nCommon questions and answers regarding installation and technical problems can be found in Frequently Asked Questions (FAQ).\nDeadlines for all assignments can be found on the course web page and in MyCourses.\nYou are allowed to discuss assignments with your friends, but it is not allowed to copy solutions directly from other students or from internet.\nDo not share your answers publicly.\nDo not copy answers from the internet or from previous years. We compare the answers to the answers from previous years and to the answers from other students this year.\nUse of AI is allowed on the course, but the most of the work needs to by the student, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching).\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof.\nIf you have any suggestions or improvements to the course material, please post in the course chat feedback channel, create an issue, or submit a pull request to the public repository!\n\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\n\nThe following will set-up markmyassignment to check your functions at the end of the notebook:\n\nif(!require(markmyassignment)){\n    install.packages(\"markmyassignment\")\n    library(markmyassignment)\n}\n\nLoading required package: markmyassignment\n\nassignment_path = paste(\"https://github.com/avehtari/BDA_course_Aalto/blob/master/tests/assignment4.yml\", sep=\"\")\nset_assignment(assignment_path)    \n\nAssignment set:\nassignment4: Bayesian Data Analysis: Assignment 4\nThe assignment contain the following (4) tasks:\n- log_importance_weights\n- normalized_importance_weights\n- S_eff\n- posterior_mean\n\n\nThe following installs and loads the aaltobda package:\n\nif(!require(aaltobda)){\n    install.packages(\"remotes\")\n    remotes::install_github(\"avehtari/BDA_course_Aalto\", subdir = \"rpackage\", upgrade=\"never\")\n    library(aaltobda)\n}\n\nLoading required package: aaltobda\n\n\nThe following installs and loads the latex2exp package, which allows us to use LaTeX in plots:\n\nif(!require(latex2exp)){\n    install.packages(\"latex2exp\")\n    library(latex2exp)\n}\n\nLoading required package: latex2exp\n\n\n\n\n\n\n\nThe following installs and loads the posterior package, which allows us to use its rvar Random Variable Datatype:\n\nif(!require(posterior)){\n    install.packages(\"posterior\")\n    library(posterior)\n}\n\nLoading required package: posterior\n\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following object is masked from 'package:aaltobda':\n\n    mcse_quantile\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nThe following installs and loads the ggdist package for advanced plotting functions:\n\nif(!require(ggplot2)){\n    install.packages(\"ggplot2\")\n    library(ggplot2)\n}\n\nLoading required package: ggplot2\n\nggplot2::theme_set(theme_minimal(base_size = 14))\nif(!require(ggdist)){\n    install.packages(\"ggdist\")\n    library(ggdist)\n}\n\nLoading required package: ggdist",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 4"
    ]
  },
  {
    "objectID": "template4.html#setting-up-advanced-packages-posterior-and-ggdist",
    "href": "template4.html#setting-up-advanced-packages-posterior-and-ggdist",
    "title": "Notebook for Assignment 4",
    "section": "",
    "text": "The following installs and loads the posterior package, which allows us to use its rvar Random Variable Datatype:\n\nif(!require(posterior)){\n    install.packages(\"posterior\")\n    library(posterior)\n}\n\nLoading required package: posterior\n\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following object is masked from 'package:aaltobda':\n\n    mcse_quantile\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nThe following installs and loads the ggdist package for advanced plotting functions:\n\nif(!require(ggplot2)){\n    install.packages(\"ggplot2\")\n    library(ggplot2)\n}\n\nLoading required package: ggplot2\n\nggplot2::theme_set(theme_minimal(base_size = 14))\nif(!require(ggdist)){\n    install.packages(\"ggdist\")\n    library(ggdist)\n}\n\nLoading required package: ggdist",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 4"
    ]
  },
  {
    "objectID": "template6.html",
    "href": "template6.html",
    "title": "Notebook for Assignment 6",
    "section": "",
    "text": "This assignment is related to Lecture 6 and Chapter 12.\nWe recommend using JupyterHub (which has all the needed packages pre-installed).\n\nReading instructions:\n\nThe reading instructions for BDA3 Chapter 12.\n\n\n\n\n\n\n\n\nGeneral Instructions for Answering the Assignment Questions\n\n\n\n\n\n\nQuestions below are exact copies of the text found in the MyCourses quiz and should serve as a notebook where you can store notes and code.\nWe recommend opening these notebooks in the Aalto JupyterHub, see how to use R and RStudio remotely.\nFor inspiration for code, have a look at the BDA R Demos and the specific Assignment code notebooks\nRecommended additional self study exercises for each chapter in BDA3 are listed in the course web page. These will help to gain deeper understanding of the topic.\nCommon questions and answers regarding installation and technical problems can be found in Frequently Asked Questions (FAQ).\nDeadlines for all assignments can be found on the course web page and in MyCourses.\nYou are allowed to discuss assignments with your friends, but it is not allowed to copy solutions directly from other students or from internet.\nDo not share your answers publicly.\nDo not copy answers from the internet or from previous years. We compare the answers to the answers from previous years and to the answers from other students this year.\nUse of AI is allowed on the course, but the most of the work needs to by the student, and you need to report whether you used AI and in which way you used them (See points 5 and 6 in Aalto guidelines for use of AI in teaching).\nAll suspected plagiarism will be reported and investigated. See more about the Aalto University Code of Academic Integrity and Handling Violations Thereof.\nIf you have any suggestions or improvements to the course material, please post in the course chat feedback channel, create an issue, or submit a pull request to the public repository!\n\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\n\nThe following installs and loads the aaltobda package:\n\nif(!require(aaltobda)){\n    install.packages(\"aaltobda\", repos = c(\"https://avehtari.github.io/BDA_course_Aalto/\", getOption(\"repos\")))\n    library(aaltobda)\n}\n\nLoading required package: aaltobda\n\n\nThe following installs and loads the latex2exp package, which allows us to use LaTeX in plots:\n\nif(!require(latex2exp)){\n    install.packages(\"latex2exp\")\n    library(latex2exp)\n}\n\nLoading required package: latex2exp\n\n\nThe following installs and loads the posterior package which imports the rhat_basic() function:\n\nif(!require(posterior)){\n    install.packages(\"posterior\")\n    library(posterior)\n}\n\nLoading required package: posterior\n\n\nThis is posterior version 1.6.0\n\n\n\nAttaching package: 'posterior'\n\n\nThe following object is masked from 'package:aaltobda':\n\n    mcse_quantile\n\n\nThe following objects are masked from 'package:stats':\n\n    mad, sd, var\n\n\nThe following objects are masked from 'package:base':\n\n    %in%, match\n\n\nThe following installs and loads the ggplot2 package and the bayesplot package\n\nif(!require(ggplot2)){\n    install.packages(\"ggplot2\")\n    library(ggplot2)\n}\n\nLoading required package: ggplot2\n\nif(!require(bayesplot)){\n    install.packages(\"bayesplot\")\n    library(bayesplot)\n}\n\nLoading required package: bayesplot\n\n\nThis is bayesplot version 1.11.1\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\n\n\nAttaching package: 'bayesplot'\n\n\nThe following object is masked from 'package:posterior':\n\n    rhat\n\nif(!require(dplyr)){\n    install.packages(\"dplyr\")\n    library(dplyr)\n}\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nif(!require(tidyr)){\n    install.packages(\"tidyr\")\n    library(tidyr)\n}\n\nLoading required package: tidyr\n\n# Some additional set-up to make plots legible\nggplot2::theme_set(theme_minimal(base_size = 14))\nbayesplot::bayesplot_theme_set(theme_minimal(base_size = 14))\n\nThe following installs and loads the cmdstanr package and tries to install cmdstan.\n\nif(!require(cmdstanr)){\n    install.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\n    library(cmdstanr)\n}\n\nLoading required package: cmdstanr\n\n\nThis is cmdstanr version 0.8.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/runner/.cmdstan/cmdstan-2.35.0\n\n\n- CmdStan version: 2.35.0\n\ncmdstan_installed &lt;- function(){\n  res &lt;- try(out &lt;- cmdstanr::cmdstan_path(), silent = TRUE)\n  !inherits(res, \"try-error\")\n}\nif(!cmdstan_installed()){\n    install_cmdstan()\n}",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 6"
    ]
  },
  {
    "objectID": "template6.html#data-preparation-and-sampling-from-the-posterior",
    "href": "template6.html#data-preparation-and-sampling-from-the-posterior",
    "title": "Notebook for Assignment 6",
    "section": "2.1 Data preparation and sampling from the posterior",
    "text": "2.1 Data preparation and sampling from the posterior\nData assembly happens here:\n\n# These are our observations y: the proportion of students handing in each assignment (1-8),\n# sorted by year (row-wise) and assignment (column-wise).\n# While the code suggest a matrix structure, \n# the result will actually be a vector of length N = no_years * no_assignments\npropstudents&lt;-c(c(176, 174, 158, 135, 138, 129, 126, 123)/176,\n                c(242, 212, 184, 177, 174, 172, 163, 156)/242,\n                c(332, 310, 278, 258, 243, 242, 226, 224)/332,\n                c(301, 269, 231, 232, 217, 208, 193, 191)/301,\n                c(245, 240, 228, 217, 206, 199, 191, 182)/245,\n                c(264, 249, 215, 221, 215, 206, 192, 186)/264)\n# These are our predictors x: for each observation, the corresponding assignment number.\nassignment &lt;- rep(1:8, 6)\n# These are in some sense our test data: the proportion of students handing in the last assignment (9),\n# sorted by year. \n# Usually, we would not want to split our data like that and instead\n# use e.g. Leave-One-Out Cross-Validation (LOO-CV, see e.g. http://mc-stan.org/loo/index.html)\n# to evaluate model performance.\npropstudents9 = c(121/176, 153/242, 218/332, 190/301, 175/245, 179/264)\n# The total number of assignments\nno_assignments = 9\n# The assignment numbers for which we want to generate predictions\nx_predictions = 1:no_assignments\n# (Cmd)Stan(R) expects the data to be passed in the below format:\nmodel_data = list(N=length(assignment),\n                 x=assignment,\n                 y=propstudents,\n                 no_predictions=no_assignments,\n                 x_predictions=x_predictions)\n\nSampling from the posterior distribution happens here:\n\n# This reads the file at the specified path and tries to compile it. \n# If it fails, an error is thrown.\nretention_model = cmdstan_model(\"assignment6_linear_model.stan\")\n# This \"out &lt;- capture.output(...)\" construction suppresses output from cmdstanr\n# See also https://github.com/stan-dev/cmdstanr/issues/646\nout &lt;- capture.output(\n    # Sampling from the posterior distribution happens here:\n    fit &lt;- retention_model$sample(data=model_data, refresh=0, show_messages=FALSE)\n)\n\nDraws postprocessing happens here:\n\n# This extracts the draws from the sampling result as a data.frame.\ndraws_df = fit$draws(format=\"draws_df\")\n\n# This does some data/draws wrangling to compute the 5, 50 and 95 percentiles of \n# the mean at the specified covariate values (x_predictions). \n# It can be instructive to play around with each of the data processing steps\n# to find out what each step does, e.g. by removing parts from the back like \"|&gt;  gather(pct,y,-x)\"\n# and printing the resulting data.frame.\nmu_quantiles_df = draws_df |&gt; \n      subset_draws(variable = c(\"mu_pred\")) |&gt; \n      summarise_draws(~quantile2(.x, probs = c(0.05, .5, 0.95))) |&gt; \n      mutate(x = 1:9) |&gt; \n      pivot_longer(c(q5, q50, q95), names_to = c(\"pct\"))\n# Same as above, but for the predictions.\ny_quantiles_df = draws_df |&gt; \n      subset_draws(variable = c(\"y_pred\")) |&gt; \n      summarise_draws(~quantile2(.x, probs = c(0.05, .5, 0.95))) |&gt; \n      mutate(x = 1:9) |&gt; \n      pivot_longer(c(q5, q50, q95), names_to = c(\"pct\"))\n\nPlotting happens here:\n\nggplot() +\n  # scatter plot of the training data:  \n  geom_point(\n    aes(x, y, color=assignment), \n    data=data.frame(x=assignment, y=propstudents, assignment=\"1-8\")\n) +\n  # scatter plot of the test data:\n  geom_point(\n    aes(x, y, color=assignment), \n    data=data.frame(x=no_assignments, y=propstudents9, assignment=\"9\")\n) +\n  # you have to tell us what this plots:\n  geom_line(aes(x,y=value,linetype=pct), data=mu_quantiles_df, color='grey', linewidth=1.5) +\n  # you have to tell us what this plots:\n  geom_line(aes(x,y=value,linetype=pct), data=y_quantiles_df, color='red') +\n  # adding xticks for each assignment:\n  scale_x_continuous(breaks=1:no_assignments) +\n  # adding labels to the plot:\n  labs(y=\"assignment submission %\", x=\"assignment number\") +\n  # specifying that line types repeat:\n  scale_linetype_manual(values=c(2,1,2)) +\n  # Specify colours of the observations:\n  scale_colour_manual(values = c(\"1-8\"=\"black\", \"9\"=\"blue\")) +\n  # remove the legend for the linetypes:\n  guides(linetype=\"none\")",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 6"
    ]
  },
  {
    "objectID": "template6.html#quick-check-for-sampling-convergence",
    "href": "template6.html#quick-check-for-sampling-convergence",
    "title": "Notebook for Assignment 6",
    "section": "2.2 Quick check for sampling convergence",
    "text": "2.2 Quick check for sampling convergence\nIf your model is correctly implemented, sampling from the posterior distribution should have been successful. You can check whether Stan thinks that sampling succeeded by inspecting the output of the below command, which you should be able to interpret with a little help from the CmdStan User’s Guide.\n\nfit$cmdstan_diagnose()",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 6"
    ]
  },
  {
    "objectID": "template6.html#improper-posterior",
    "href": "template6.html#improper-posterior",
    "title": "Notebook for Assignment 6",
    "section": "3.1 Improper Posterior",
    "text": "3.1 Improper Posterior\nAn unbounded likelihood without a proper prior can lead to an improper posterior. We recommend to always use proper priors (integral over a proper distribution is finite) to guarantee proper posteriors.\nA commonly used model that can have unbounded likelihood is logistic regression with complete separation in data.\n\n3.1.1 Data\nUnivariate continous predictor \\(x\\), binary target \\(y\\), and the two classes are completely separable, which leads to unbounded likelihood.\n\nset.seed(48927+4)\nM=1;\nN=10;\nx=matrix(sort(rnorm(N)),ncol=M)\ny=rep(c(0,1), each=N/2)\ndata_logit &lt;-list(M = M, N = N, x = x, y = y)\ndata.frame(data_logit) |&gt;\n  ggplot(aes(x, y)) +\n  geom_point(size = 3, shape=1, alpha=0.6) +\n  scale_y_continuous(breaks=c(0,1))\n\n\n\n\n\n\n\n\n\n\n3.1.2 Compile and Sample from the following model\n// logistic regression\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; M;\n  array[N] int&lt;lower=0,upper=1&gt; y;\n  matrix[N,M] x;\n}\nparameters {\n  real alpha;\n  vector[M] beta;\n}\nmodel {\n  y ~ bernoulli_logit_glm(x, alpha, beta);\n}\n\n\n3.1.3 Convergence diagnostics\n\n# Get the summary diagnostics using [fit object name]]$diagnostic_summary()\n\n# Get summary statistics from your chains using summarize_draws() (hint: look at how we used the function last week)\n\n# Plot histograms and bivariate scatter plots using mcmc_pairs(). See for guidance on how to use the function here: https://mc-stan.org/bayesplot/reference/MCMC-scatterplots.html\n\n# To plot the tracplot of individual chains, use the mcmc_trace() function. See for guidance on how to use the function here: https://mc-stan.org/bayesplot/reference/MCMC-traces.html",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 6"
    ]
  },
  {
    "objectID": "template6.html#run-model-and-analyse-output",
    "href": "template6.html#run-model-and-analyse-output",
    "title": "Notebook for Assignment 6",
    "section": "4.1 Run model and analyse output",
    "text": "4.1 Run model and analyse output\n\n# Load data\ndata =list(n = bioassay$n, x = bioassay$x, y = bioassay$y, N = nrow(bioassay))\n\n# Compile Model\nmod &lt;- cmdstan_model(# your stan model location)\n\n# Estimate model\nout &lt;- capture.output(\n  # Sampling from the posterior distribution happens here:\n  fit &lt;- mod$sample(data=data, refresh=0, show_messages=FALSE, seed = 4711, chains = 4, iter_warmup = 1000, \n                    iter_sampling = 3000)\n)\n\n# This extracts the draws from the sampling result as a data.frame.\ndraws_df = fit$draws(format=\"draws_df\")\n\n# This is what you'll need for convergence diagnostics\nsummarise_draws(draws_df, Rhat=rhat_basic, ESS= ess_mean, ~ess_quantile(.x, probs = 0.05))\n\n# Scatter plot of the draws\nmcmc_scatter(draws_df, pars=c(\"alpha\", \"beta\"))\n\n# Plot the autocorrelation function and compare to last week\nmcmc_acf(draws_df, pars = c(\"alpha\",\"beta\"))",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 6"
    ]
  },
  {
    "objectID": "template1.html",
    "href": "template1.html",
    "title": "Notebook for Assignment 1",
    "section": "",
    "text": "1 General information\nThe exercises here refer to the lecture 1/BDA chapter 1 content, not the course infrastructure quiz. This assignment is meant to test whether or not you have sufficient knowledge to participate in the course. The first question checks that you remember basic terms of probability calculus. The second exercise checks you recognise the most important notation used throughout the course and used in BDA3. The third-fifth exercise you will solve some basic Bayes theorem questions to check your understanding on the basics of probability theory. The 6th exercise checks on whether you recall the three steps of Bayesian Data Ananlysis as mentioned in chapter 1 of BDA3. The last exercise walks you through an example of how we can use models to generate distributions for outcomes of interest, applied to a setting of a simplified Roulette table.\nThis quarto document is not intended to be submitted, but to render the questions as they appear on Mycourses to be available also outside of it. The following will set-up markmyassignment to check your functions at the end of the notebook:\n\nlibrary(markmyassignment)\nassignment_path = paste(\"https://github.com/avehtari/BDA_course_Aalto/\",\n\"blob/master/tests/assignment1.yml\", sep=\"\")\nset_assignment(assignment_path)\n\nAssignment set:\nassignment1: Bayesian Data Analysis: Assignment 1\nThe assignment contain the following (3) tasks:\n- p_red\n- p_box\n- p_identical_twin\n\n\n\n\n2 1. Basic probability theory notation and terms\n\n\n3 2. Notation\n\n\n4 3. Bayes’ theorem 1\nIf you use pen and paper, it may help to draw pictures as follows (see also assignment_instructions#fig-workflow):\n\n\n\n\n\n\nFigure 1: Parts of Bayesian workflow\n\n\n\nSee Figure 1 for illustration of parts of Bayesian workflow.\n\n\n5 4. Bayes’ theorem 2\nThe following will help you implementing a function to calculate the required probabilities for this exercise. Keep the below name and format for the function to work with markmyassignment:\n\nboxes_test &lt;- matrix(c(2,2,1,5,5,1), ncol = 2,\n    dimnames = list(c(\"A\", \"B\", \"C\"), c(\"red\", \"white\")))\n\np_red &lt;- function(boxes) {\n    # Do computation here, and return as below.\n    # This is the correct return value for the test data provided above.\n    0.3928571\n}\n\np_box &lt;- function(boxes) {\n    # Do computation here, and return as below.\n    # This is the correct return value for the test data provided above.\n    c(0.29090909,0.07272727,0.63636364)\n}\n\n\n\n6 5. Bayes’ theorem 3\nThe R functions below might help you calculating the requited probabilities.\n\nfraternal_prob = 1/125\nidentical_prob = 1/300\n\nKeep the below name and format for the function to work with markmyassignment:\n\np_identical_twin &lt;- function(fraternal_prob, identical_prob) {\n    # Do computation here, and return as below.\n    # This is the correct return value for the test data provided above.\n    0.4545455\n}\n\n\n\n7 6. The three steps of Bayesian data analysis\n\n\n8 7. A Binomial Model for the Roulette Table\nIncomplete code can be found below.\n\n# Ratio of red/black\ntheta &lt;- # declare probability parameter for the binomial model\n\n# Sequence of trials\n\ntrials &lt;- seq(#start value of sequence,#end value of sequence,#value for spacing)\n\n# Number of simulation draws from the model\nnsims &lt;- # number of of simulations from the binomial model\n\n# Helper function for getting the ratios\nbinom_gen &lt;- function(trials,theta,nsims){\n    df &lt;-  as.data.frame(rbinom(nsims,trials,theta)/trials) |&gt; mutate(nsims = nsims,trials = trials)\n    colnames(df) &lt;- c(\"Ratios\",\"Nsims\",\"Trials\")\n  return(df)\n}\n\n# Create a data frame containing the draws for each number of trials\nratio_60 &lt;- do.call(rbind, lapply(trials, binom_gen, theta, nsims)) # lapply applies elements in trials column to binom_gen function, which is then rowbound via do.call\n\nNow plot a histogram of the computed ratios for 10, 50 and 1000 trials, using the code below\n\n# Plot the Distributions\nsubset_df60 &lt;- ratio_60[ratio_60$Trials %in% c(#trial values), ] # Subset your dataframe\n\nsubset_df60 |&gt; ggplot(aes(Ratios)) +\n  geom_histogram(position = \"identity\" ,bins = 40) +\n  facet_grid(cols = vars(Trials))  +\n  ggtitle(\"Ratios for specific trials\")\n\nSuppose you are now certain that theta = 0.6, plot the probability density given 1000 trials using the code below.\n\nsize =  # number of trials\nprob =  # probability of success\n\nbinom_data &lt;- data.frame(\n  Success = 0:size,\n  Probability = dbinom(0:size, size = size, prob = prob)\n)\n\nggplot(binom_data, aes(x = Success, y = Probability)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"PMF of Binomial Distribution\", x = \"Number of Successes\", y = \"PDF\")\n\n\n\n\n\n\n\nmarkmyassignment\n\n\n\n\n\nThe following will check the functions for which markmyassignment has been set up:\n\nmark_my_assignment()\n\n✔ | F W  S  OK | Context\n\n⠏ |          0 | task-1-subtask-1-tests                                         \n⠏ |          0 | p_red()                                                        \n✖ | 1        3 | p_red()\n────────────────────────────────────────────────────────────────────────────────\nFailure ('test-task-1-subtask-1-tests.R:21:3'): p_red()\np_red(boxes = boxes) not equivalent to 0.5.\n1/1 mismatches\n[1] 0.393 - 0.5 == -0.107\nError: Incorrect result for matrix(c(1,1,1,1,1,1), ncol = 2)\n────────────────────────────────────────────────────────────────────────────────\n\n⠏ |          0 | task-2-subtask-1-tests                                         \n⠏ |          0 | p_box()                                                        \n✖ | 1        3 | p_box()\n────────────────────────────────────────────────────────────────────────────────\nFailure ('test-task-2-subtask-1-tests.R:19:3'): p_box()\np_box(boxes = boxes) not equivalent to c(0.4, 0.1, 0.5).\n3/3 mismatches (average diff: 0.0909)\n[1] 0.2909 - 0.4 == -0.1091\n[2] 0.0727 - 0.1 == -0.0273\n[3] 0.6364 - 0.5 ==  0.1364\nError: Incorrect result for matrix(c(1,1,1,1,1,1), ncol = 2)\n────────────────────────────────────────────────────────────────────────────────\n\n⠏ |          0 | task-3-subtask-1-tests                                         \n⠏ |          0 | p_identical_twin()                                             \n✖ | 2        3 | p_identical_twin()\n────────────────────────────────────────────────────────────────────────────────\nFailure ('test-task-3-subtask-1-tests.R:16:3'): p_identical_twin()\np_identical_twin(fraternal_prob = 1/100, identical_prob = 1/500) not equivalent to 0.2857143.\n1/1 mismatches\n[1] 0.455 - 0.286 == 0.169\nError: Incorrect result for fraternal_prob = 1/100 and identical_prob = 1/500\n\nFailure ('test-task-3-subtask-1-tests.R:19:3'): p_identical_twin()\np_identical_twin(fraternal_prob = 1/10, identical_prob = 1/20) not equivalent to 0.5.\n1/1 mismatches\n[1] 0.455 - 0.5 == -0.0455\nError: Incorrect result for fraternal_prob = 1/10 and identical_prob = 1/20\n────────────────────────────────────────────────────────────────────────────────\n\n══ Results ═════════════════════════════════════════════════════════════════════\n── Failed tests ────────────────────────────────────────────────────────────────\nFailure ('test-task-1-subtask-1-tests.R:21:3'): p_red()\np_red(boxes = boxes) not equivalent to 0.5.\n1/1 mismatches\n[1] 0.393 - 0.5 == -0.107\nError: Incorrect result for matrix(c(1,1,1,1,1,1), ncol = 2)\n\nFailure ('test-task-2-subtask-1-tests.R:19:3'): p_box()\np_box(boxes = boxes) not equivalent to c(0.4, 0.1, 0.5).\n3/3 mismatches (average diff: 0.0909)\n[1] 0.2909 - 0.4 == -0.1091\n[2] 0.0727 - 0.1 == -0.0273\n[3] 0.6364 - 0.5 ==  0.1364\nError: Incorrect result for matrix(c(1,1,1,1,1,1), ncol = 2)\n\nFailure ('test-task-3-subtask-1-tests.R:16:3'): p_identical_twin()\np_identical_twin(fraternal_prob = 1/100, identical_prob = 1/500) not equivalent to 0.2857143.\n1/1 mismatches\n[1] 0.455 - 0.286 == 0.169\nError: Incorrect result for fraternal_prob = 1/100 and identical_prob = 1/500\n\nFailure ('test-task-3-subtask-1-tests.R:19:3'): p_identical_twin()\np_identical_twin(fraternal_prob = 1/10, identical_prob = 1/20) not equivalent to 0.5.\n1/1 mismatches\n[1] 0.455 - 0.5 == -0.0455\nError: Incorrect result for fraternal_prob = 1/10 and identical_prob = 1/20\n\n[ FAIL 4 | WARN 0 | SKIP 0 | PASS 9 ]",
    "crumbs": [
      "Templates",
      "Notebook for Assignment 1"
    ]
  },
  {
    "objectID": "assignment7.html",
    "href": "assignment7.html",
    "title": "Assignment 7",
    "section": "",
    "text": "1 General information\nThis assignment will be published later!",
    "crumbs": [
      "Assignments",
      "Assignment 7"
    ]
  }
]